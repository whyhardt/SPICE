{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf7jlYw4NA0v",
    "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/whyhardt/SPICE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oXIbg826NS5i",
    "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
   },
   "outputs": [],
   "source": [
    "# !pip install -e SPICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f0uVlABYznR5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For custom RNN\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data first with the `convert_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: torch.Size([277, 299, 11])\n",
      "Number of participants: 277\n",
      "Number of items in dataset: 6\n",
      "Number of actions in dataset: 2\n",
      "Number of additional inputs: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spice import SpiceDataset, convert_dataset\n",
    "\n",
    "# Load your data\n",
    "dataset = convert_dataset(\n",
    "    file = '../data/augustat2025/augustat2025.csv',\n",
    "    df_participant_id='participant_id',\n",
    "    df_choice='choice',\n",
    "    df_reward='reward',\n",
    "    additional_inputs=['shown_at_0', 'shown_at_1'],\n",
    "    timeshift_additional_inputs=False,\n",
    "    )\n",
    "\n",
    "n_actions = dataset.ys.shape[-1]\n",
    "n_items = dataset.xs[..., 2*n_actions+1].nan_to_num(-1).max().int() + 1\n",
    "# in order to set up the participant embedding we have to compute the number of unique participants in our data \n",
    "# to get the number of participants n_participants we do:\n",
    "n_participants = len(dataset.xs[..., -1].unique())\n",
    "\n",
    "# add shown_at_i_next (alongside shown_at_i) because both are necessary in the SPICE model\n",
    "shown_at_i_next = dataset.xs[:, 1:, n_actions*2:n_actions*2+2]\n",
    "xs = torch.concat((dataset.xs[:, :-1, :n_actions*2+2], shown_at_i_next, dataset.xs[:, :-1, -3:]), dim=-1)\n",
    "ys = dataset.ys[:, :-1]\n",
    "dataset = SpiceDataset(xs, ys)\n",
    "\n",
    "# TODO: split into training and test data\n",
    "dataset_train, dataset_test = dataset, dataset\n",
    "\n",
    "# instead of timeshift add the predictor states shown_at_0 and shown_at_1 of the next trial to the inputs\n",
    "# xs = dataset.xs[:, :-1]\n",
    "# ys = dataset.ys[:, :-1]\n",
    "# shown_next = dataset.xs[:, 1:, 2*2:-3]\n",
    "# xs = torch.concat((xs[..., :-3], shown_next, xs[..., -3:]), dim=-1)\n",
    "# dataset = SpiceDataset(xs, ys)\n",
    "\n",
    "# structure of dataset:\n",
    "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
    "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
    "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
    "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
    "print(f\"Number of participants: {n_participants}\")\n",
    "print(f\"Number of items in dataset: {n_items}\")\n",
    "print(f\"Number of actions in dataset: {n_actions}\")\n",
    "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPICE setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
    "\n",
    "The `SpiceConfig` takes as arguments \n",
    "1. `library_setup (dict)`: Defining the variable names of each module.\n",
    "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
    "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice import SpiceConfig\n",
    "\n",
    "spice_config = SpiceConfig(\n",
    "    library_setup={\n",
    "        'value_reward_chosen': ['reward'],\n",
    "        'value_reward_not_chosen': [],\n",
    "        'value_reward_not_displayed': [],\n",
    "        # 'value_choice': ['choice'],\n",
    "    },\n",
    "    \n",
    "    memory_state={\n",
    "        'value_reward': 0.,\n",
    "        # 'value_choice': 0.,\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
    "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
    "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
    "3. `n_participants (int)`: number of participants in your dataset.\n",
    "\n",
    "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
    "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
    "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
    "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice import BaseRNN\n",
    "\n",
    "\n",
    "class SPICERNN(BaseRNN):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.participant_embedding = self.setup_embedding(num_embeddings=n_participants, embedding_size=self.embedding_size)\n",
    "\n",
    "        self.setup_module(key_module='value_reward_chosen', input_size=1+self.embedding_size)\n",
    "        self.setup_module(key_module='value_reward_not_chosen', input_size=self.embedding_size)\n",
    "        self.setup_module(key_module='value_reward_not_displayed', input_size=self.embedding_size)\n",
    "        # self.setup_module(key_module='value_choice', input_size=1+self.embedding_size)\n",
    "        \n",
    "    def forward(self, inputs, prev_state, batch_first=False):\n",
    "\n",
    "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "\n",
    "        # Get shown items (raw indices) - these are time-shifted, so they refer to the NEXT trial\n",
    "        shown_at_0_current = spice_signals.additional_inputs[..., 0].long()\n",
    "        shown_at_1_current = spice_signals.additional_inputs[..., 1].long()\n",
    "        shown_at_0_next = spice_signals.additional_inputs[..., 2].long()\n",
    "        shown_at_1_next = spice_signals.additional_inputs[..., 3].long()\n",
    "        \n",
    "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
    "\n",
    "        for timestep in spice_signals.timesteps:\n",
    "\n",
    "            # Transform input data from action space to item space\n",
    "\n",
    "            # Determine which action was chosen\n",
    "            action_idx = spice_signals.actions[timestep].argmax(dim=-1)\n",
    "\n",
    "            # Map to item indices using current trial's shown items\n",
    "            item_chosen_idx = torch.where(action_idx == 0, shown_at_0_current[timestep], shown_at_1_current[timestep])\n",
    "            item_not_chosen_idx = torch.where(action_idx == 1, shown_at_0_current[timestep], shown_at_1_current[timestep])\n",
    "\n",
    "            # Create one-hot masks\n",
    "            item_chosen_onehot = torch.nn.functional.one_hot(item_chosen_idx, num_classes=self.n_items).float()\n",
    "            item_not_chosen_onehot = torch.nn.functional.one_hot(item_not_chosen_idx, num_classes=self.n_items).float()\n",
    "            item_not_displayed_onehot = 1 - (item_chosen_onehot + item_not_chosen_onehot)\n",
    "\n",
    "            # Map rewards from action space to item space\n",
    "            reward_action = spice_signals.rewards[timestep, :]  # shape: (batch, n_actions)\n",
    "\n",
    "            # Create reward tensor in item space (batch, n_items)\n",
    "            reward_item = torch.zeros(reward_action.shape[0], self.n_items, device=reward_action.device)\n",
    "            \n",
    "            # Scatter rewards to the corresponding items:\n",
    "            # Item at shown_at_0_current gets reward for action 0\n",
    "            # Item at shown_at_1_current gets reward for action 1\n",
    "            reward_item.scatter_(1, shown_at_0_current[timestep].unsqueeze(-1), reward_action[:, 0].unsqueeze(-1))\n",
    "            reward_item.scatter_(1, shown_at_1_current[timestep].unsqueeze(-1), reward_action[:, 1].unsqueeze(-1))\n",
    "            \n",
    "            # Update chosen\n",
    "            self.call_module(\n",
    "                key_module='value_reward_chosen',\n",
    "                key_state='value_reward',\n",
    "                action_mask=item_chosen_onehot,\n",
    "                inputs=(\n",
    "                    reward_item,\n",
    "                    # ...\n",
    "                    ),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embeddings,\n",
    "            )\n",
    "\n",
    "            # Update not chosen\n",
    "            self.call_module(\n",
    "                key_module='value_reward_not_chosen',\n",
    "                key_state='value_reward',\n",
    "                action_mask=item_not_chosen_onehot,\n",
    "                inputs=None,\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embeddings,\n",
    "            )\n",
    "\n",
    "            # Update not displayed\n",
    "            self.call_module(\n",
    "                key_module='value_reward_not_displayed',\n",
    "                key_state='value_reward',\n",
    "                action_mask=item_not_displayed_onehot,\n",
    "                inputs=None,\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embeddings,\n",
    "            )\n",
    "\n",
    "            # Transform values from item space to action space for NEXT trial (for prediction)\n",
    "            # Use the time-shifted items (next trial's items)\n",
    "            value_at_0 = torch.gather(self.state['value_reward'], 1, shown_at_0_next[timestep].unsqueeze(-1))\n",
    "            value_at_1 = torch.gather(self.state['value_reward'], 1, shown_at_1_next[timestep].unsqueeze(-1))\n",
    "\n",
    "            # log action values\n",
    "            spice_signals.logits[timestep] = torch.concat([value_at_0, value_at_1], dim=-1)\n",
    "\n",
    "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
    "\n",
    "        return spice_signals.logits, self.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup now the `SpiceEstimator` object and fit it to the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice import SpiceEstimator\n",
    "\n",
    "path_spice = '../params/augustat2025/spice_augustat2025.pkl'\n",
    "\n",
    "estimator = SpiceEstimator(\n",
    "        # model paramaeters\n",
    "        rnn_class=SPICERNN,\n",
    "        spice_config=spice_config,\n",
    "        n_actions=n_actions,\n",
    "        n_items=n_items,\n",
    "        n_participants=n_participants,\n",
    "        \n",
    "        # rnn training parameters\n",
    "        epochs=1000,\n",
    "        warmup_steps=250,\n",
    "        learning_rate=0.01,\n",
    "        \n",
    "        # sindy fitting parameters\n",
    "        sindy_weight=0.1,\n",
    "        sindy_threshold=0.05,\n",
    "        sindy_threshold_frequency=1,\n",
    "        sindy_threshold_terms=1,\n",
    "        sindy_cutoff_patience=100,\n",
    "        sindy_epochs=1000,\n",
    "        sindy_alpha=0.0001,\n",
    "        sindy_library_polynomial_degree=2,\n",
    "        sindy_ensemble_size=1,\n",
    "        \n",
    "        # additional generalization parameters\n",
    "        bagging=True,\n",
    "        scheduler=True,\n",
    "        \n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        verbose=True,\n",
    "        save_path_spice=path_spice,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "3EnmDiUMWq6e",
    "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on cuda...\n",
      "================================================================================\n",
      "\n",
      "Training the RNN...\n",
      "================================================================================\n",
      "Epoch 1/1000 --- L(Train): 0.7024866 --- L(Val, RNN): 0.6814768 --- L(Val, SINDy): 0.7145708 --- Time: 1.76s; --- Convergence: 6.59e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.01 1 + 0.989 value_reward_chosen[t] + 0.011 reward + 0.01 value_reward_chosen^2 + -0.009 value_reward_chosen*reward + 0.01 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.008 1 + 0.99 value_reward_not_chosen[t] + -0.01 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.01 1 + 0.99 value_reward_not_displayed[t] + 0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 2/1000 --- L(Train): 0.6851085 --- L(Val, RNN): 0.6656446 --- L(Val, SINDy): 0.8247462 --- Time: 1.95s; --- Convergence: 3.38e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.016 1 + 0.983 value_reward_chosen[t] + 0.018 reward + 0.017 value_reward_chosen^2 + -0.015 value_reward_chosen*reward + 0.017 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.015 1 + 0.984 value_reward_not_chosen[t] + -0.017 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.016 1 + 0.984 value_reward_not_displayed[t] + 0.018 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 3/1000 --- L(Train): 0.6774237 --- L(Val, RNN): 0.6543282 --- L(Val, SINDy): 0.8736541 --- Time: 1.81s; --- Convergence: 1.74e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.021 1 + 0.977 value_reward_chosen[t] + 0.023 reward + 0.022 value_reward_chosen^2 + -0.02 value_reward_chosen*reward + 0.022 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.02 1 + 0.978 value_reward_not_chosen[t] + -0.022 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.021 1 + 0.979 value_reward_not_displayed[t] + 0.023 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 4/1000 --- L(Train): 0.6622434 --- L(Val, RNN): 0.6460653 --- L(Val, SINDy): 0.8758246 --- Time: 1.78s; --- Convergence: 9.13e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.026 1 + 0.973 value_reward_chosen[t] + 0.027 reward + 0.026 value_reward_chosen^2 + -0.024 value_reward_chosen*reward + 0.026 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.024 1 + 0.974 value_reward_not_chosen[t] + -0.026 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.025 1 + 0.974 value_reward_not_displayed[t] + 0.027 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 5/1000 --- L(Train): 0.6569582 --- L(Val, RNN): 0.6398179 --- L(Val, SINDy): 0.8526426 --- Time: 1.76s; --- Convergence: 4.88e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.967 value_reward_chosen[t] + 0.033 reward + 0.032 value_reward_chosen^2 + -0.03 value_reward_chosen*reward + 0.033 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.03 1 + 0.968 value_reward_not_chosen[t] + -0.032 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.032 1 + 0.968 value_reward_not_displayed[t] + 0.033 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 6/1000 --- L(Train): 0.6513974 --- L(Val, RNN): 0.6349866 --- L(Val, SINDy): 0.8274162 --- Time: 1.74s; --- Convergence: 2.68e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.962 value_reward_chosen[t] + 0.039 reward + 0.038 value_reward_chosen^2 + -0.035 value_reward_chosen*reward + 0.038 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.036 1 + 0.963 value_reward_not_chosen[t] + -0.037 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.037 1 + 0.963 value_reward_not_displayed[t] + 0.038 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 7/1000 --- L(Train): 0.6508761 --- L(Val, RNN): 0.6310269 --- L(Val, SINDy): 0.7572564 --- Time: 1.78s; --- Convergence: 1.54e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.042 1 + 0.957 value_reward_chosen[t] + 0.043 reward + 0.042 value_reward_chosen^2 + -0.04 value_reward_chosen*reward + 0.043 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.04 1 + 0.959 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.042 1 + 0.958 value_reward_not_displayed[t] + 0.043 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 8/1000 --- L(Train): 0.6508499 --- L(Val, RNN): 0.6277301 --- L(Val, SINDy): 0.7151951 --- Time: 2.07s; --- Convergence: 9.34e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.951 value_reward_chosen[t] + 0.049 reward + 0.048 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.049 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.046 1 + 0.953 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.04 1 + 0.953 value_reward_not_displayed[t] + 0.049 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 9/1000 --- L(Train): 0.6544579 --- L(Val, RNN): 0.6249103 --- L(Val, SINDy): 0.6715025 --- Time: 2.49s; --- Convergence: 6.08e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.946 value_reward_chosen[t] + 0.055 reward + 0.053 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.054 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.052 1 + 0.948 value_reward_not_chosen[t] + -0.052 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.038 1 + 0.947 value_reward_not_displayed[t] + 0.054 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 10/1000 --- L(Train): 0.6500167 --- L(Val, RNN): 0.6224757 --- L(Val, SINDy): 0.6460022 --- Time: 2.42s; --- Convergence: 4.26e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.06 1 + 0.94 value_reward_chosen[t] + 0.061 reward + 0.06 value_reward_chosen^2 + -0.057 value_reward_chosen*reward + 0.061 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.058 1 + 0.941 value_reward_not_chosen[t] + -0.059 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.037 1 + 0.941 value_reward_not_displayed[t] + 0.06 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 11/1000 --- L(Train): 0.6536608 --- L(Val, RNN): 0.6203373 --- L(Val, SINDy): 0.6271142 --- Time: 2.42s; --- Convergence: 3.20e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.066 1 + 0.934 value_reward_chosen[t] + 0.067 reward + 0.065 value_reward_chosen^2 + -0.063 value_reward_chosen*reward + 0.066 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.064 1 + 0.935 value_reward_not_chosen[t] + -0.065 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.035 1 + 0.935 value_reward_not_displayed[t] + 0.066 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 12/1000 --- L(Train): 0.6492076 --- L(Val, RNN): 0.6185634 --- L(Val, SINDy): 0.6146959 --- Time: 2.34s; --- Convergence: 2.49e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.071 1 + 0.928 value_reward_chosen[t] + 0.072 reward + 0.071 value_reward_chosen^2 + -0.068 value_reward_chosen*reward + 0.072 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.069 1 + 0.93 value_reward_not_chosen[t] + -0.07 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.034 1 + 0.93 value_reward_not_displayed[t] + 0.071 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 13/1000 --- L(Train): 0.6525556 --- L(Val, RNN): 0.6171963 --- L(Val, SINDy): 0.6047730 --- Time: 2.39s; --- Convergence: 1.93e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.077 1 + 0.922 value_reward_chosen[t] + 0.079 reward + 0.077 value_reward_chosen^2 + -0.075 value_reward_chosen*reward + 0.078 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.076 1 + 0.923 value_reward_not_chosen[t] + -0.076 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.038 1 + 0.923 value_reward_not_displayed[t] + 0.078 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 14/1000 --- L(Train): 0.6524199 --- L(Val, RNN): 0.6161145 --- L(Val, SINDy): 0.5981737 --- Time: 2.73s; --- Convergence: 1.50e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.083 1 + 0.916 value_reward_chosen[t] + 0.085 reward + 0.083 value_reward_chosen^2 + -0.08 value_reward_chosen*reward + 0.084 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.081 1 + 0.918 value_reward_not_chosen[t] + -0.082 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.042 1 + 0.918 value_reward_not_displayed[t] + 0.084 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 15/1000 --- L(Train): 0.6459054 --- L(Val, RNN): 0.6151809 --- L(Val, SINDy): 0.5904203 --- Time: 2.71s; --- Convergence: 1.22e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.088 1 + 0.911 value_reward_chosen[t] + 0.091 reward + 0.088 value_reward_chosen^2 + -0.086 value_reward_chosen*reward + 0.09 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.087 1 + 0.912 value_reward_not_chosen[t] + -0.088 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.045 1 + 0.912 value_reward_not_displayed[t] + 0.089 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 16/1000 --- L(Train): 0.6446572 --- L(Val, RNN): 0.6143895 --- L(Val, SINDy): 0.5866902 --- Time: 2.68s; --- Convergence: 1.01e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.094 1 + 0.905 value_reward_chosen[t] + 0.096 reward + 0.094 value_reward_chosen^2 + -0.092 value_reward_chosen*reward + 0.096 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.093 1 + 0.907 value_reward_not_chosen[t] + -0.093 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.046 1 + 0.906 value_reward_not_displayed[t] + 0.095 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 17/1000 --- L(Train): 0.6472359 --- L(Val, RNN): 0.6137020 --- L(Val, SINDy): 0.5860174 --- Time: 2.90s; --- Convergence: 8.46e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.1 1 + 0.899 value_reward_chosen[t] + 0.101 reward + 0.1 value_reward_chosen^2 + -0.097 value_reward_chosen*reward + 0.101 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.098 1 + 0.901 value_reward_not_chosen[t] + -0.098 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.048 1 + 0.901 value_reward_not_displayed[t] + 0.101 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 18/1000 --- L(Train): 0.6372381 --- L(Val, RNN): 0.6126326 --- L(Val, SINDy): 0.5797304 --- Time: 2.74s; --- Convergence: 9.58e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.106 1 + 0.893 value_reward_chosen[t] + 0.106 reward + 0.106 value_reward_chosen^2 + -0.104 value_reward_chosen*reward + 0.105 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.105 1 + 0.896 value_reward_not_chosen[t] + -0.102 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.047 1 + 0.894 value_reward_not_displayed[t] + 0.107 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 19/1000 --- L(Train): 0.6342480 --- L(Val, RNN): 0.6111106 --- L(Val, SINDy): 0.5756076 --- Time: 2.65s; --- Convergence: 1.24e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.112 1 + 0.887 value_reward_chosen[t] + 0.11 reward + 0.112 value_reward_chosen^2 + -0.111 value_reward_chosen*reward + 0.11 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.112 1 + 0.891 value_reward_not_chosen[t] + -0.107 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.044 1 + 0.887 value_reward_not_displayed[t] + 0.114 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 20/1000 --- L(Train): 0.6340639 --- L(Val, RNN): 0.6093261 --- L(Val, SINDy): 0.5709773 --- Time: 2.49s; --- Convergence: 1.51e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.118 1 + 0.881 value_reward_chosen[t] + 0.114 reward + 0.118 value_reward_chosen^2 + -0.118 value_reward_chosen*reward + 0.113 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.118 1 + 0.887 value_reward_not_chosen[t] + -0.11 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.042 1 + 0.881 value_reward_not_displayed[t] + 0.12 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 21/1000 --- L(Train): 0.6371914 --- L(Val, RNN): 0.6074715 --- L(Val, SINDy): 0.5645096 --- Time: 2.51s; --- Convergence: 1.68e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.125 1 + 0.875 value_reward_chosen[t] + 0.117 reward + 0.124 value_reward_chosen^2 + -0.125 value_reward_chosen*reward + 0.116 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.125 1 + 0.882 value_reward_not_chosen[t] + -0.114 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.039 1 + 0.874 value_reward_not_displayed[t] + 0.127 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 22/1000 --- L(Train): 0.6335034 --- L(Val, RNN): 0.6057950 --- L(Val, SINDy): 0.5603588 --- Time: 2.50s; --- Convergence: 1.68e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.131 1 + 0.868 value_reward_chosen[t] + 0.118 reward + 0.131 value_reward_chosen^2 + -0.133 value_reward_chosen*reward + 0.118 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.133 1 + 0.878 value_reward_not_chosen[t] + -0.117 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.037 1 + 0.866 value_reward_not_displayed[t] + 0.134 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 23/1000 --- L(Train): 0.6257751 --- L(Val, RNN): 0.6041242 --- L(Val, SINDy): 0.5646061 --- Time: 2.64s; --- Convergence: 1.68e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.138 1 + 0.861 value_reward_chosen[t] + 0.12 reward + 0.137 value_reward_chosen^2 + -0.141 value_reward_chosen*reward + 0.119 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.14 1 + 0.874 value_reward_not_chosen[t] + -0.119 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.035 1 + 0.859 value_reward_not_displayed[t] + 0.141 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 24/1000 --- L(Train): 0.6229929 --- L(Val, RNN): 0.6025002 --- L(Val, SINDy): 0.5696821 --- Time: 2.47s; --- Convergence: 1.65e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.143 1 + 0.856 value_reward_chosen[t] + 0.121 reward + 0.142 value_reward_chosen^2 + -0.148 value_reward_chosen*reward + 0.12 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.146 1 + 0.871 value_reward_not_chosen[t] + -0.122 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.033 1 + 0.853 value_reward_not_displayed[t] + 0.147 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 25/1000 --- L(Train): 0.6262198 --- L(Val, RNN): 0.6008629 --- L(Val, SINDy): 0.5757454 --- Time: 2.58s; --- Convergence: 1.64e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.15 1 + 0.849 value_reward_chosen[t] + 0.119 reward + 0.149 value_reward_chosen^2 + -0.156 value_reward_chosen*reward + 0.119 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.154 1 + 0.868 value_reward_not_chosen[t] + -0.124 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.034 1 + 0.845 value_reward_not_displayed[t] + 0.155 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 26/1000 --- L(Train): 0.6206028 --- L(Val, RNN): 0.5991258 --- L(Val, SINDy): 0.5797076 --- Time: 2.61s; --- Convergence: 1.69e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.156 1 + 0.843 value_reward_chosen[t] + 0.118 reward + 0.155 value_reward_chosen^2 + -0.163 value_reward_chosen*reward + 0.117 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.161 1 + 0.866 value_reward_not_chosen[t] + -0.126 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.035 1 + 0.838 value_reward_not_displayed[t] + 0.162 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 27/1000 --- L(Train): 0.6160787 --- L(Val, RNN): 0.5973529 --- L(Val, SINDy): 0.5896847 --- Time: 2.48s; --- Convergence: 1.73e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.163 1 + 0.835 value_reward_chosen[t] + 0.113 reward + 0.162 value_reward_chosen^2 + -0.172 value_reward_chosen*reward + 0.113 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.169 1 + 0.865 value_reward_not_chosen[t] + -0.128 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.035 1 + 0.83 value_reward_not_displayed[t] + 0.17 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 28/1000 --- L(Train): 0.6124343 --- L(Val, RNN): 0.5954250 --- L(Val, SINDy): 0.5932353 --- Time: 2.64s; --- Convergence: 1.83e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.17 1 + 0.828 value_reward_chosen[t] + 0.108 reward + 0.169 value_reward_chosen^2 + -0.182 value_reward_chosen*reward + 0.107 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.178 1 + 0.866 value_reward_not_chosen[t] + -0.131 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.033 1 + 0.821 value_reward_not_displayed[t] + 0.179 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 29/1000 --- L(Train): 0.6098703 --- L(Val, RNN): 0.5931714 --- L(Val, SINDy): 0.5945089 --- Time: 2.68s; --- Convergence: 2.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.177 1 + 0.821 value_reward_chosen[t] + 0.103 reward + 0.176 value_reward_chosen^2 + -0.19 value_reward_chosen*reward + 0.102 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.187 1 + 0.866 value_reward_not_chosen[t] + -0.133 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.032 1 + 0.812 value_reward_not_displayed[t] + 0.187 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 30/1000 --- L(Train): 0.6157539 --- L(Val, RNN): 0.5902788 --- L(Val, SINDy): 0.5965179 --- Time: 2.63s; --- Convergence: 2.47e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.183 1 + 0.814 value_reward_chosen[t] + 0.098 reward + 0.182 value_reward_chosen^2 + -0.198 value_reward_chosen*reward + 0.097 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.194 1 + 0.867 value_reward_not_chosen[t] + -0.135 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.03 1 + 0.805 value_reward_not_displayed[t] + 0.194 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 31/1000 --- L(Train): 0.6053516 --- L(Val, RNN): 0.5871326 --- L(Val, SINDy): 0.5957773 --- Time: 2.66s; --- Convergence: 2.81e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.189 1 + 0.808 value_reward_chosen[t] + 0.093 reward + 0.188 value_reward_chosen^2 + -0.206 value_reward_chosen*reward + 0.093 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.202 1 + 0.868 value_reward_not_chosen[t] + -0.138 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.028 1 + 0.797 value_reward_not_displayed[t] + 0.202 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 32/1000 --- L(Train): 0.6039108 --- L(Val, RNN): 0.5836929 --- L(Val, SINDy): 0.5997428 --- Time: 2.72s; --- Convergence: 3.12e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.194 1 + 0.802 value_reward_chosen[t] + 0.089 reward + 0.193 value_reward_chosen^2 + -0.213 value_reward_chosen*reward + 0.089 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.21 1 + 0.87 value_reward_not_chosen[t] + -0.14 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.025 1 + 0.79 value_reward_not_displayed[t] + 0.208 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 33/1000 --- L(Train): 0.5906181 --- L(Val, RNN): 0.5799375 --- L(Val, SINDy): 0.6002243 --- Time: 2.53s; --- Convergence: 3.44e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.199 1 + 0.795 value_reward_chosen[t] + 0.089 reward + 0.199 value_reward_chosen^2 + -0.222 value_reward_chosen*reward + 0.088 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.218 1 + 0.874 value_reward_not_chosen[t] + -0.143 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.019 1 + 0.781 value_reward_not_displayed[t] + 0.217 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 34/1000 --- L(Train): 0.6008819 --- L(Val, RNN): 0.5763913 --- L(Val, SINDy): 0.5948750 --- Time: 2.56s; --- Convergence: 3.49e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.205 1 + 0.789 value_reward_chosen[t] + 0.089 reward + 0.205 value_reward_chosen^2 + -0.23 value_reward_chosen*reward + 0.088 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.226 1 + 0.878 value_reward_not_chosen[t] + -0.146 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.013 1 + 0.773 value_reward_not_displayed[t] + 0.224 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 35/1000 --- L(Train): 0.5850655 --- L(Val, RNN): 0.5727443 --- L(Val, SINDy): 0.5887967 --- Time: 2.59s; --- Convergence: 3.57e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.21 1 + 0.783 value_reward_chosen[t] + 0.092 reward + 0.21 value_reward_chosen^2 + -0.239 value_reward_chosen*reward + 0.091 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.235 1 + 0.883 value_reward_not_chosen[t] + -0.149 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.007 1 + 0.765 value_reward_not_displayed[t] + 0.232 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 36/1000 --- L(Train): 0.5820802 --- L(Val, RNN): 0.5687199 --- L(Val, SINDy): 0.5853457 --- Time: 2.52s; --- Convergence: 3.80e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.216 1 + 0.776 value_reward_chosen[t] + 0.096 reward + 0.216 value_reward_chosen^2 + -0.249 value_reward_chosen*reward + 0.096 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.245 1 + 0.889 value_reward_not_chosen[t] + -0.152 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 0.756 value_reward_not_displayed[t] + 0.239 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 37/1000 --- L(Train): 0.5865379 --- L(Val, RNN): 0.5647106 --- L(Val, SINDy): 0.5875652 --- Time: 2.55s; --- Convergence: 3.90e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.221 1 + 0.77 value_reward_chosen[t] + 0.103 reward + 0.221 value_reward_chosen^2 + -0.259 value_reward_chosen*reward + 0.102 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.255 1 + 0.896 value_reward_not_chosen[t] + -0.155 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.746 value_reward_not_displayed[t] + 0.247 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 38/1000 --- L(Train): 0.5765434 --- L(Val, RNN): 0.5607247 --- L(Val, SINDy): 0.5825311 --- Time: 2.53s; --- Convergence: 3.94e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.225 1 + 0.764 value_reward_chosen[t] + 0.108 reward + 0.226 value_reward_chosen^2 + -0.269 value_reward_chosen*reward + 0.107 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.264 1 + 0.902 value_reward_not_chosen[t] + -0.158 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.738 value_reward_not_displayed[t] + 0.253 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 39/1000 --- L(Train): 0.5736980 --- L(Val, RNN): 0.5561621 --- L(Val, SINDy): 0.5915490 --- Time: 2.82s; --- Convergence: 4.25e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.229 1 + 0.759 value_reward_chosen[t] + 0.115 reward + 0.23 value_reward_chosen^2 + -0.279 value_reward_chosen*reward + 0.114 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.273 1 + 0.908 value_reward_not_chosen[t] + -0.16 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.729 value_reward_not_displayed[t] + 0.259 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 40/1000 --- L(Train): 0.5611258 --- L(Val, RNN): 0.5513577 --- L(Val, SINDy): 0.5808557 --- Time: 2.57s; --- Convergence: 4.53e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.233 1 + 0.754 value_reward_chosen[t] + 0.122 reward + 0.234 value_reward_chosen^2 + -0.288 value_reward_chosen*reward + 0.122 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.283 1 + 0.915 value_reward_not_chosen[t] + -0.162 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.721 value_reward_not_displayed[t] + 0.265 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 41/1000 --- L(Train): 0.5712345 --- L(Val, RNN): 0.5460332 --- L(Val, SINDy): 0.5836583 --- Time: 2.56s; --- Convergence: 4.93e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.235 1 + 0.75 value_reward_chosen[t] + 0.13 reward + 0.237 value_reward_chosen^2 + -0.298 value_reward_chosen*reward + 0.129 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.292 1 + 0.921 value_reward_not_chosen[t] + -0.164 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.713 value_reward_not_displayed[t] + 0.269 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 42/1000 --- L(Train): 0.5590940 --- L(Val, RNN): 0.5411500 --- L(Val, SINDy): 0.5843334 --- Time: 2.54s; --- Convergence: 4.90e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.237 1 + 0.746 value_reward_chosen[t] + 0.137 reward + 0.239 value_reward_chosen^2 + -0.308 value_reward_chosen*reward + 0.136 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.3 1 + 0.927 value_reward_not_chosen[t] + -0.165 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.705 value_reward_not_displayed[t] + 0.273 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 43/1000 --- L(Train): 0.5498526 --- L(Val, RNN): 0.5354708 --- L(Val, SINDy): 0.5806063 --- Time: 2.62s; --- Convergence: 5.29e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.24 1 + 0.743 value_reward_chosen[t] + 0.143 reward + 0.242 value_reward_chosen^2 + -0.316 value_reward_chosen*reward + 0.143 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.307 1 + 0.932 value_reward_not_chosen[t] + -0.166 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.01 1 + 0.699 value_reward_not_displayed[t] + 0.277 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 44/1000 --- L(Train): 0.5498921 --- L(Val, RNN): 0.5299221 --- L(Val, SINDy): 0.5767152 --- Time: 2.57s; --- Convergence: 5.42e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.242 1 + 0.74 value_reward_chosen[t] + 0.149 reward + 0.244 value_reward_chosen^2 + -0.323 value_reward_chosen*reward + 0.148 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.314 1 + 0.937 value_reward_not_chosen[t] + -0.168 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.011 1 + 0.693 value_reward_not_displayed[t] + 0.281 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 45/1000 --- L(Train): 0.5526555 --- L(Val, RNN): 0.5244681 --- L(Val, SINDy): 0.5710797 --- Time: 2.53s; --- Convergence: 5.44e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.243 1 + 0.737 value_reward_chosen[t] + 0.154 reward + 0.246 value_reward_chosen^2 + -0.33 value_reward_chosen*reward + 0.153 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.32 1 + 0.941 value_reward_not_chosen[t] + -0.169 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.012 1 + 0.687 value_reward_not_displayed[t] + 0.284 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 46/1000 --- L(Train): 0.5448274 --- L(Val, RNN): 0.5184830 --- L(Val, SINDy): 0.5707963 --- Time: 2.68s; --- Convergence: 5.71e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.245 1 + 0.734 value_reward_chosen[t] + 0.159 reward + 0.248 value_reward_chosen^2 + -0.336 value_reward_chosen*reward + 0.158 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.326 1 + 0.945 value_reward_not_chosen[t] + -0.17 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.013 1 + 0.682 value_reward_not_displayed[t] + 0.287 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 47/1000 --- L(Train): 0.5265366 --- L(Val, RNN): 0.5114700 --- L(Val, SINDy): 0.5582808 --- Time: 2.89s; --- Convergence: 6.36e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.245 1 + 0.733 value_reward_chosen[t] + 0.164 reward + 0.249 value_reward_chosen^2 + -0.343 value_reward_chosen*reward + 0.164 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.332 1 + 0.949 value_reward_not_chosen[t] + -0.17 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.017 1 + 0.677 value_reward_not_displayed[t] + 0.288 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 48/1000 --- L(Train): 0.5310180 --- L(Val, RNN): 0.5069196 --- L(Val, SINDy): 0.5567334 --- Time: 2.95s; --- Convergence: 5.46e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.245 1 + 0.732 value_reward_chosen[t] + 0.171 reward + 0.249 value_reward_chosen^2 + -0.351 value_reward_chosen*reward + 0.17 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.338 1 + 0.952 value_reward_not_chosen[t] + -0.169 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.023 1 + 0.673 value_reward_not_displayed[t] + 0.287 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 49/1000 --- L(Train): 0.5310737 --- L(Val, RNN): 0.4999041 --- L(Val, SINDy): 0.5476434 --- Time: 2.70s; --- Convergence: 6.24e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.243 1 + 0.731 value_reward_chosen[t] + 0.178 reward + 0.248 value_reward_chosen^2 + -0.359 value_reward_chosen*reward + 0.177 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.343 1 + 0.955 value_reward_not_chosen[t] + -0.167 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.028 1 + 0.669 value_reward_not_displayed[t] + 0.286 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 50/1000 --- L(Train): 0.5106095 --- L(Val, RNN): 0.4965909 --- L(Val, SINDy): 0.5377619 --- Time: 2.78s; --- Convergence: 4.77e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.24 1 + 0.732 value_reward_chosen[t] + 0.185 reward + 0.246 value_reward_chosen^2 + -0.368 value_reward_chosen*reward + 0.185 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.349 1 + 0.958 value_reward_not_chosen[t] + -0.163 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.032 1 + 0.665 value_reward_not_displayed[t] + 0.283 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 51/1000 --- L(Train): 0.5122008 --- L(Val, RNN): 0.4874291 --- L(Val, SINDy): 0.5336049 --- Time: 2.82s; --- Convergence: 6.97e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.233 1 + 0.735 value_reward_chosen[t] + 0.195 reward + 0.241 value_reward_chosen^2 + -0.379 value_reward_chosen*reward + 0.194 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.356 1 + 0.959 value_reward_not_chosen[t] + -0.157 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.028 1 + 0.662 value_reward_not_displayed[t] + 0.276 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 52/1000 --- L(Train): 0.5078347 --- L(Val, RNN): 0.4830944 --- L(Val, SINDy): 0.5313956 --- Time: 2.68s; --- Convergence: 5.65e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.227 1 + 0.738 value_reward_chosen[t] + 0.204 reward + 0.236 value_reward_chosen^2 + -0.389 value_reward_chosen*reward + 0.203 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.362 1 + 0.96 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.025 1 + 0.659 value_reward_not_displayed[t] + 0.269 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 53/1000 --- L(Train): 0.5025281 --- L(Val, RNN): 0.4761046 --- L(Val, SINDy): 0.5277274 --- Time: 2.50s; --- Convergence: 6.32e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.222 1 + 0.741 value_reward_chosen[t] + 0.212 reward + 0.231 value_reward_chosen^2 + -0.398 value_reward_chosen*reward + 0.211 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.368 1 + 0.961 value_reward_not_chosen[t] + -0.145 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.023 1 + 0.656 value_reward_not_displayed[t] + 0.264 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 54/1000 --- L(Train): 0.4954197 --- L(Val, RNN): 0.4712514 --- L(Val, SINDy): 0.5221187 --- Time: 2.67s; --- Convergence: 5.59e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.217 1 + 0.744 value_reward_chosen[t] + 0.221 reward + 0.227 value_reward_chosen^2 + -0.407 value_reward_chosen*reward + 0.22 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.374 1 + 0.961 value_reward_not_chosen[t] + -0.139 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.02 1 + 0.654 value_reward_not_displayed[t] + 0.257 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 55/1000 --- L(Train): 0.4939046 --- L(Val, RNN): 0.4656801 --- L(Val, SINDy): 0.5154382 --- Time: 2.65s; --- Convergence: 5.58e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.211 1 + 0.748 value_reward_chosen[t] + 0.229 reward + 0.222 value_reward_chosen^2 + -0.417 value_reward_chosen*reward + 0.228 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.379 1 + 0.961 value_reward_not_chosen[t] + -0.132 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.018 1 + 0.653 value_reward_not_displayed[t] + 0.249 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 56/1000 --- L(Train): 0.4872390 --- L(Val, RNN): 0.4612382 --- L(Val, SINDy): 0.5116233 --- Time: 2.52s; --- Convergence: 5.01e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.203 1 + 0.752 value_reward_chosen[t] + 0.239 reward + 0.215 value_reward_chosen^2 + -0.428 value_reward_chosen*reward + 0.238 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.385 1 + 0.959 value_reward_not_chosen[t] + -0.123 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.016 1 + 0.653 value_reward_not_displayed[t] + 0.24 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 57/1000 --- L(Train): 0.4849140 --- L(Val, RNN): 0.4558113 --- L(Val, SINDy): 0.5111225 --- Time: 2.52s; --- Convergence: 5.22e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.196 1 + 0.757 value_reward_chosen[t] + 0.248 reward + 0.208 value_reward_chosen^2 + -0.438 value_reward_chosen*reward + 0.248 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.391 1 + 0.956 value_reward_not_chosen[t] + -0.114 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.013 1 + 0.654 value_reward_not_displayed[t] + 0.23 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 58/1000 --- L(Train): 0.4671157 --- L(Val, RNN): 0.4506938 --- L(Val, SINDy): 0.5071942 --- Time: 2.52s; --- Convergence: 5.17e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.188 1 + 0.762 value_reward_chosen[t] + 0.258 reward + 0.201 value_reward_chosen^2 + -0.449 value_reward_chosen*reward + 0.257 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.396 1 + 0.952 value_reward_not_chosen[t] + -0.104 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.01 1 + 0.656 value_reward_not_displayed[t] + 0.22 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 59/1000 --- L(Train): 0.4701861 --- L(Val, RNN): 0.4459711 --- L(Val, SINDy): 0.4995942 --- Time: 2.63s; --- Convergence: 4.95e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.18 1 + 0.766 value_reward_chosen[t] + 0.267 reward + 0.195 value_reward_chosen^2 + -0.459 value_reward_chosen*reward + 0.266 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.401 1 + 0.949 value_reward_not_chosen[t] + -0.095 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.657 value_reward_not_displayed[t] + 0.211 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 60/1000 --- L(Train): 0.4719980 --- L(Val, RNN): 0.4408174 --- L(Val, SINDy): 0.4952070 --- Time: 2.48s; --- Convergence: 5.05e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.173 1 + 0.77 value_reward_chosen[t] + 0.275 reward + 0.189 value_reward_chosen^2 + -0.469 value_reward_chosen*reward + 0.275 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.406 1 + 0.945 value_reward_not_chosen[t] + -0.086 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.66 value_reward_not_displayed[t] + 0.201 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 61/1000 --- L(Train): 0.4789466 --- L(Val, RNN): 0.4367761 --- L(Val, SINDy): 0.4890957 --- Time: 2.57s; --- Convergence: 4.55e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.167 1 + 0.774 value_reward_chosen[t] + 0.284 reward + 0.182 value_reward_chosen^2 + -0.479 value_reward_chosen*reward + 0.284 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.411 1 + 0.94 value_reward_not_chosen[t] + -0.076 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.664 value_reward_not_displayed[t] + 0.19 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 62/1000 --- L(Train): 0.4720545 --- L(Val, RNN): 0.4342308 --- L(Val, SINDy): 0.4876328 --- Time: 2.63s; --- Convergence: 3.55e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.16 1 + 0.779 value_reward_chosen[t] + 0.294 reward + 0.176 value_reward_chosen^2 + -0.49 value_reward_chosen*reward + 0.293 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.416 1 + 0.933 value_reward_not_chosen[t] + -0.066 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.67 value_reward_not_displayed[t] + 0.179 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 63/1000 --- L(Train): 0.4558991 --- L(Val, RNN): 0.4313716 --- L(Val, SINDy): 0.4895850 --- Time: 2.72s; --- Convergence: 3.20e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.153 1 + 0.783 value_reward_chosen[t] + 0.303 reward + 0.169 value_reward_chosen^2 + -0.501 value_reward_chosen*reward + 0.302 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.421 1 + 0.926 value_reward_not_chosen[t] + -0.055 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.01 1 + 0.677 value_reward_not_displayed[t] + 0.167 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 64/1000 --- L(Train): 0.4550039 --- L(Val, RNN): 0.4292634 --- L(Val, SINDy): 0.4845450 --- Time: 2.58s; --- Convergence: 2.66e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.147 1 + 0.786 value_reward_chosen[t] + 0.311 reward + 0.163 value_reward_chosen^2 + -0.511 value_reward_chosen*reward + 0.311 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.426 1 + 0.919 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.016 1 + 0.684 value_reward_not_displayed[t] + 0.156 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 65/1000 --- L(Train): 0.4562552 --- L(Val, RNN): 0.4267712 --- L(Val, SINDy): 0.4873105 --- Time: 2.54s; --- Convergence: 2.57e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.141 1 + 0.789 value_reward_chosen[t] + 0.319 reward + 0.158 value_reward_chosen^2 + -0.52 value_reward_chosen*reward + 0.318 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.43 1 + 0.913 value_reward_not_chosen[t] + -0.036 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.022 1 + 0.689 value_reward_not_displayed[t] + 0.146 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 66/1000 --- L(Train): 0.4595583 --- L(Val, RNN): 0.4226950 --- L(Val, SINDy): 0.4895196 --- Time: 2.57s; --- Convergence: 3.32e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.136 1 + 0.792 value_reward_chosen[t] + 0.326 reward + 0.153 value_reward_chosen^2 + -0.528 value_reward_chosen*reward + 0.325 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.434 1 + 0.907 value_reward_not_chosen[t] + -0.028 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.027 1 + 0.695 value_reward_not_displayed[t] + 0.137 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 67/1000 --- L(Train): 0.4447939 --- L(Val, RNN): 0.4198469 --- L(Val, SINDy): 0.4924987 --- Time: 2.64s; --- Convergence: 3.09e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.132 1 + 0.795 value_reward_chosen[t] + 0.332 reward + 0.148 value_reward_chosen^2 + -0.535 value_reward_chosen*reward + 0.331 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.437 1 + 0.902 value_reward_not_chosen[t] + -0.021 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.032 1 + 0.7 value_reward_not_displayed[t] + 0.129 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 68/1000 --- L(Train): 0.4452820 --- L(Val, RNN): 0.4159547 --- L(Val, SINDy): 0.4957943 --- Time: 2.53s; --- Convergence: 3.49e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.128 1 + 0.797 value_reward_chosen[t] + 0.337 reward + 0.144 value_reward_chosen^2 + -0.542 value_reward_chosen*reward + 0.337 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.44 1 + 0.897 value_reward_not_chosen[t] + -0.014 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.036 1 + 0.704 value_reward_not_displayed[t] + 0.122 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 69/1000 --- L(Train): 0.4496787 --- L(Val, RNN): 0.4126519 --- L(Val, SINDy): 0.4978451 --- Time: 2.62s; --- Convergence: 3.40e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.123 1 + 0.8 value_reward_chosen[t] + 0.34 reward + 0.14 value_reward_chosen^2 + -0.546 value_reward_chosen*reward + 0.339 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.443 1 + 0.891 value_reward_not_chosen[t] + -0.007 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.044 1 + 0.711 value_reward_not_displayed[t] + 0.114 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 70/1000 --- L(Train): 0.4416543 --- L(Val, RNN): 0.4091305 --- L(Val, SINDy): 0.5074055 --- Time: 2.64s; --- Convergence: 3.46e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.118 1 + 0.802 value_reward_chosen[t] + 0.341 reward + 0.135 value_reward_chosen^2 + -0.549 value_reward_chosen*reward + 0.34 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.446 1 + 0.884 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.055 1 + 0.72 value_reward_not_displayed[t] + 0.106 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 71/1000 --- L(Train): 0.4475545 --- L(Val, RNN): 0.4061921 --- L(Val, SINDy): 0.5113497 --- Time: 2.66s; --- Convergence: 3.20e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.113 1 + 0.804 value_reward_chosen[t] + 0.34 reward + 0.13 value_reward_chosen^2 + -0.549 value_reward_chosen*reward + 0.339 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.449 1 + 0.875 value_reward_not_chosen[t] + 0.011 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.066 1 + 0.732 value_reward_not_displayed[t] + 0.098 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 72/1000 --- L(Train): 0.4405909 --- L(Val, RNN): 0.4030837 --- L(Val, SINDy): 0.5088111 --- Time: 2.61s; --- Convergence: 3.15e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.107 1 + 0.805 value_reward_chosen[t] + 0.338 reward + 0.125 value_reward_chosen^2 + -0.547 value_reward_chosen*reward + 0.337 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.452 1 + 0.865 value_reward_not_chosen[t] + 0.021 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.075 1 + 0.744 value_reward_not_displayed[t] + 0.089 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 73/1000 --- L(Train): 0.4288295 --- L(Val, RNN): 0.4008557 --- L(Val, SINDy): 0.4991099 --- Time: 2.69s; --- Convergence: 2.69e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.1 1 + 0.807 value_reward_chosen[t] + 0.334 reward + 0.119 value_reward_chosen^2 + -0.54 value_reward_chosen*reward + 0.333 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.455 1 + 0.852 value_reward_not_chosen[t] + 0.032 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.075 1 + 0.758 value_reward_not_displayed[t] + 0.079 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 74/1000 --- L(Train): 0.4205653 --- L(Val, RNN): 0.3987667 --- L(Val, SINDy): 0.5129345 --- Time: 3.09s; --- Convergence: 2.39e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.093 1 + 0.809 value_reward_chosen[t] + 0.329 reward + 0.113 value_reward_chosen^2 + -0.532 value_reward_chosen*reward + 0.329 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.458 1 + 0.839 value_reward_not_chosen[t] + 0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.073 1 + 0.772 value_reward_not_displayed[t] + 0.068 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 75/1000 --- L(Train): 0.4298264 --- L(Val, RNN): 0.3967695 --- L(Val, SINDy): 0.5212253 --- Time: 2.67s; --- Convergence: 2.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.085 1 + 0.81 value_reward_chosen[t] + 0.323 reward + 0.107 value_reward_chosen^2 + -0.522 value_reward_chosen*reward + 0.323 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.46 1 + 0.827 value_reward_not_chosen[t] + 0.056 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.072 1 + 0.787 value_reward_not_displayed[t] + 0.059 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 76/1000 --- L(Train): 0.4295553 --- L(Val, RNN): 0.3954964 --- L(Val, SINDy): 0.5303524 --- Time: 2.59s; --- Convergence: 1.73e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.079 1 + 0.811 value_reward_chosen[t] + 0.318 reward + 0.102 value_reward_chosen^2 + -0.513 value_reward_chosen*reward + 0.317 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.462 1 + 0.815 value_reward_not_chosen[t] + 0.066 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.071 1 + 0.8 value_reward_not_displayed[t] + 0.05 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 77/1000 --- L(Train): 0.4094890 --- L(Val, RNN): 0.3939956 --- L(Val, SINDy): 0.5401227 --- Time: 2.51s; --- Convergence: 1.62e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.072 1 + 0.812 value_reward_chosen[t] + 0.313 reward + 0.096 value_reward_chosen^2 + -0.503 value_reward_chosen*reward + 0.313 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.464 1 + 0.803 value_reward_not_chosen[t] + 0.077 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.073 1 + 0.815 value_reward_not_displayed[t] + 0.043 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 78/1000 --- L(Train): 0.4231553 --- L(Val, RNN): 0.3924806 --- L(Val, SINDy): 0.5380768 --- Time: 2.58s; --- Convergence: 1.57e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.065 1 + 0.813 value_reward_chosen[t] + 0.311 reward + 0.091 value_reward_chosen^2 + -0.491 value_reward_chosen*reward + 0.31 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.466 1 + 0.791 value_reward_not_chosen[t] + 0.087 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.077 1 + 0.829 value_reward_not_displayed[t] + 0.039 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 79/1000 --- L(Train): 0.4235158 --- L(Val, RNN): 0.3906022 --- L(Val, SINDy): 0.5344962 --- Time: 2.55s; --- Convergence: 1.72e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.059 1 + 0.814 value_reward_chosen[t] + 0.312 reward + 0.086 value_reward_chosen^2 + -0.478 value_reward_chosen*reward + 0.311 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.468 1 + 0.778 value_reward_not_chosen[t] + 0.095 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.083 1 + 0.845 value_reward_not_displayed[t] + 0.036 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 80/1000 --- L(Train): 0.4142204 --- L(Val, RNN): 0.3885297 --- L(Val, SINDy): 0.5412979 --- Time: 2.52s; --- Convergence: 1.90e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.814 value_reward_chosen[t] + 0.312 reward + 0.081 value_reward_chosen^2 + -0.466 value_reward_chosen*reward + 0.311 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.47 1 + 0.767 value_reward_not_chosen[t] + 0.103 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.089 1 + 0.858 value_reward_not_displayed[t] + 0.034 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 81/1000 --- L(Train): 0.4212206 --- L(Val, RNN): 0.3866644 --- L(Val, SINDy): 0.5460641 --- Time: 2.52s; --- Convergence: 1.88e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.815 value_reward_chosen[t] + 0.313 reward + 0.077 value_reward_chosen^2 + -0.455 value_reward_chosen*reward + 0.312 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.471 1 + 0.756 value_reward_not_chosen[t] + 0.111 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.094 1 + 0.871 value_reward_not_displayed[t] + 0.032 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 82/1000 --- L(Train): 0.4128239 --- L(Val, RNN): 0.3845198 --- L(Val, SINDy): 0.5335302 --- Time: 2.60s; --- Convergence: 2.01e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.815 value_reward_chosen[t] + 0.313 reward + 0.073 value_reward_chosen^2 + -0.446 value_reward_chosen*reward + 0.312 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.472 1 + 0.747 value_reward_not_chosen[t] + 0.117 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.098 1 + 0.882 value_reward_not_displayed[t] + 0.03 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 83/1000 --- L(Train): 0.4195918 --- L(Val, RNN): 0.3949001 --- L(Val, SINDy): 0.5344067 --- Time: 2.91s; --- Convergence: 6.20e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.812 value_reward_chosen[t] + 0.319 reward + 0.067 value_reward_chosen^2 + -0.434 value_reward_chosen*reward + 0.318 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.475 1 + 0.734 value_reward_not_chosen[t] + 0.118 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.09 1 + 0.896 value_reward_not_displayed[t] + 0.022 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 84/1000 --- L(Train): 0.4142688 --- L(Val, RNN): 0.3969689 --- L(Val, SINDy): 0.5219933 --- Time: 2.79s; --- Convergence: 4.13e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.031 1 + 0.81 value_reward_chosen[t] + 0.325 reward + 0.062 value_reward_chosen^2 + -0.423 value_reward_chosen*reward + 0.324 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.477 1 + 0.723 value_reward_not_chosen[t] + 0.119 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.083 1 + 0.908 value_reward_not_displayed[t] + 0.015 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 85/1000 --- L(Train): 0.4368387 --- L(Val, RNN): 0.3893492 --- L(Val, SINDy): 0.5048047 --- Time: 2.80s; --- Convergence: 5.88e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.027 1 + 0.806 value_reward_chosen[t] + 0.332 reward + 0.058 value_reward_chosen^2 + -0.412 value_reward_chosen*reward + 0.332 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.479 1 + 0.711 value_reward_not_chosen[t] + 0.115 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.077 1 + 0.922 value_reward_not_displayed[t] + 0.015 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 86/1000 --- L(Train): 0.4172353 --- L(Val, RNN): 0.3881594 --- L(Val, SINDy): 0.4924093 --- Time: 2.99s; --- Convergence: 3.53e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.024 1 + 0.802 value_reward_chosen[t] + 0.34 reward + 0.055 value_reward_chosen^2 + -0.402 value_reward_chosen*reward + 0.34 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.482 1 + 0.699 value_reward_not_chosen[t] + 0.108 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.071 1 + 0.937 value_reward_not_displayed[t] + 0.018 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 87/1000 --- L(Train): 0.4281299 --- L(Val, RNN): 0.3908452 --- L(Val, SINDy): 0.4908536 --- Time: 2.57s; --- Convergence: 3.11e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.023 1 + 0.797 value_reward_chosen[t] + 0.349 reward + 0.052 value_reward_chosen^2 + -0.394 value_reward_chosen*reward + 0.348 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.485 1 + 0.686 value_reward_not_chosen[t] + 0.099 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.064 1 + 0.95 value_reward_not_displayed[t] + 0.018 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 88/1000 --- L(Train): 0.4260196 --- L(Val, RNN): 0.3922848 --- L(Val, SINDy): 0.5024399 --- Time: 2.62s; --- Convergence: 2.27e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.022 1 + 0.793 value_reward_chosen[t] + 0.357 reward + 0.05 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.356 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.487 1 + 0.674 value_reward_not_chosen[t] + 0.09 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.058 1 + 0.961 value_reward_not_displayed[t] + 0.017 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 89/1000 --- L(Train): 0.4351025 --- L(Val, RNN): 0.3912519 --- L(Val, SINDy): 0.5013823 --- Time: 2.54s; --- Convergence: 1.65e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.021 1 + 0.789 value_reward_chosen[t] + 0.364 reward + 0.048 value_reward_chosen^2 + -0.381 value_reward_chosen*reward + 0.364 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.49 1 + 0.664 value_reward_not_chosen[t] + 0.082 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.053 1 + 0.971 value_reward_not_displayed[t] + 0.016 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 90/1000 --- L(Train): 0.4309278 --- L(Val, RNN): 0.3896110 --- L(Val, SINDy): 0.5137050 --- Time: 2.54s; --- Convergence: 1.65e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.021 1 + 0.785 value_reward_chosen[t] + 0.372 reward + 0.047 value_reward_chosen^2 + -0.376 value_reward_chosen*reward + 0.371 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.492 1 + 0.653 value_reward_not_chosen[t] + 0.073 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.045 1 + 0.979 value_reward_not_displayed[t] + 0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 91/1000 --- L(Train): 0.4130887 --- L(Val, RNN): 0.3895588 --- L(Val, SINDy): 0.5147189 --- Time: 2.49s; --- Convergence: 8.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.02 1 + 0.781 value_reward_chosen[t] + 0.377 reward + 0.045 value_reward_chosen^2 + -0.373 value_reward_chosen*reward + 0.377 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.494 1 + 0.641 value_reward_not_chosen[t] + 0.065 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.034 1 + 0.983 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 92/1000 --- L(Train): 0.4101562 --- L(Val, RNN): 0.3886623 --- L(Val, SINDy): 0.5019165 --- Time: 2.62s; --- Convergence: 8.73e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.019 1 + 0.777 value_reward_chosen[t] + 0.381 reward + 0.042 value_reward_chosen^2 + -0.37 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.496 1 + 0.628 value_reward_not_chosen[t] + 0.057 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.023 1 + 0.986 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 93/1000 --- L(Train): 0.4166474 --- L(Val, RNN): 0.3871133 --- L(Val, SINDy): 0.4981938 --- Time: 2.55s; --- Convergence: 1.21e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.015 1 + 0.774 value_reward_chosen[t] + 0.38 reward + 0.038 value_reward_chosen^2 + -0.368 value_reward_chosen*reward + 0.379 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.496 1 + 0.614 value_reward_not_chosen[t] + 0.052 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.011 1 + 0.989 value_reward_not_displayed[t] + -0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 94/1000 --- L(Train): 0.4153141 --- L(Val, RNN): 0.3860501 --- L(Val, SINDy): 0.4951169 --- Time: 2.59s; --- Convergence: 1.14e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.011 1 + 0.773 value_reward_chosen[t] + 0.377 reward + 0.033 value_reward_chosen^2 + -0.367 value_reward_chosen*reward + 0.377 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.496 1 + 0.598 value_reward_not_chosen[t] + 0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.99 value_reward_not_displayed[t] + -0.018 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 95/1000 --- L(Train): 0.3994464 --- L(Val, RNN): 0.3850437 --- L(Val, SINDy): 0.4901440 --- Time: 2.54s; --- Convergence: 1.07e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.006 1 + 0.772 value_reward_chosen[t] + 0.375 reward + 0.028 value_reward_chosen^2 + -0.367 value_reward_chosen*reward + 0.374 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.495 1 + 0.583 value_reward_not_chosen[t] + 0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.01 1 + 0.992 value_reward_not_displayed[t] + -0.024 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 96/1000 --- L(Train): 0.3890548 --- L(Val, RNN): 0.3841722 --- L(Val, SINDy): 0.4801833 --- Time: 2.53s; --- Convergence: 9.72e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.002 1 + 0.772 value_reward_chosen[t] + 0.373 reward + 0.024 value_reward_chosen^2 + -0.367 value_reward_chosen*reward + 0.372 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.495 1 + 0.568 value_reward_not_chosen[t] + 0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.017 1 + 0.994 value_reward_not_displayed[t] + -0.028 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 97/1000 --- L(Train): 0.4180999 --- L(Val, RNN): 0.3826993 --- L(Val, SINDy): 0.4718928 --- Time: 2.61s; --- Convergence: 1.22e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.772 value_reward_chosen[t] + 0.374 reward + 0.021 value_reward_chosen^2 + -0.368 value_reward_chosen*reward + 0.373 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.495 1 + 0.552 value_reward_not_chosen[t] + 0.039 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.019 1 + 0.997 value_reward_not_displayed[t] + -0.028 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 98/1000 --- L(Train): 0.3903897 --- L(Val, RNN): 0.3818681 --- L(Val, SINDy): 0.4603309 --- Time: 2.52s; --- Convergence: 1.03e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.003 1 + 0.773 value_reward_chosen[t] + 0.375 reward + 0.018 value_reward_chosen^2 + -0.369 value_reward_chosen*reward + 0.375 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.495 1 + 0.538 value_reward_not_chosen[t] + 0.035 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.02 1 + 0.999 value_reward_not_displayed[t] + -0.028 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 99/1000 --- L(Train): 0.4222071 --- L(Val, RNN): 0.3831265 --- L(Val, SINDy): 0.4591168 --- Time: 2.61s; --- Convergence: 1.14e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.004 1 + 0.773 value_reward_chosen[t] + 0.378 reward + 0.017 value_reward_chosen^2 + -0.371 value_reward_chosen*reward + 0.377 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.496 1 + 0.524 value_reward_not_chosen[t] + 0.028 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.017 1 + 1.003 value_reward_not_displayed[t] + -0.024 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 100/1000 --- L(Train): 0.4182971 --- L(Val, RNN): 0.3931984 --- L(Val, SINDy): 0.4596675 --- Time: 2.46s; --- Convergence: 5.61e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.004 1 + 0.774 value_reward_chosen[t] + 0.381 reward + 0.016 value_reward_chosen^2 + -0.372 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.497 1 + 0.511 value_reward_not_chosen[t] + 0.019 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.012 1 + 1.007 value_reward_not_displayed[t] + -0.019 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 101/1000 --- L(Train): 0.4236592 --- L(Val, RNN): 0.3993745 --- L(Val, SINDy): 0.4512446 --- Time: 2.55s; --- Convergence: 5.89e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.004 1 + 0.774 value_reward_chosen[t] + 0.383 reward + 0.015 value_reward_chosen^2 + -0.374 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.498 1 + 0.498 value_reward_not_chosen[t] + 0.011 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 1.01 value_reward_not_displayed[t] + -0.014 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 102/1000 --- L(Train): 0.4246546 --- L(Val, RNN): 0.3968214 --- L(Val, SINDy): 0.4594301 --- Time: 2.56s; --- Convergence: 4.22e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.004 1 + 0.775 value_reward_chosen[t] + 0.383 reward + 0.015 value_reward_chosen^2 + -0.375 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.5 1 + 0.487 value_reward_not_chosen[t] + 0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.013 value_reward_not_displayed[t] + -0.009 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 103/1000 --- L(Train): 0.4296912 --- L(Val, RNN): 0.3913643 --- L(Val, SINDy): 0.4563904 --- Time: 2.52s; --- Convergence: 4.84e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.004 1 + 0.776 value_reward_chosen[t] + 0.382 reward + 0.014 value_reward_chosen^2 + -0.376 value_reward_chosen*reward + 0.381 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.503 1 + 0.476 value_reward_not_chosen[t] + -0.008 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 1.014 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 104/1000 --- L(Train): 0.4134921 --- L(Val, RNN): 0.3873206 --- L(Val, SINDy): 0.4457767 --- Time: 2.49s; --- Convergence: 4.44e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.004 1 + 0.777 value_reward_chosen[t] + 0.381 reward + 0.014 value_reward_chosen^2 + -0.378 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.505 1 + 0.467 value_reward_not_chosen[t] + -0.017 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.007 1 + 1.016 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 105/1000 --- L(Train): 0.4260474 --- L(Val, RNN): 0.3892115 --- L(Val, SINDy): 0.4502907 --- Time: 2.54s; --- Convergence: 3.17e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.002 1 + 0.776 value_reward_chosen[t] + 0.382 reward + 0.015 value_reward_chosen^2 + -0.382 value_reward_chosen*reward + 0.381 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.509 1 + 0.456 value_reward_not_chosen[t] + -0.029 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 1.012 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 106/1000 --- L(Train): 0.4294436 --- L(Val, RNN): 0.3927132 --- L(Val, SINDy): 0.4478145 --- Time: 2.62s; --- Convergence: 3.33e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.776 value_reward_chosen[t] + 0.383 reward + 0.016 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.512 1 + 0.447 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.011 1 + 1.008 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 107/1000 --- L(Train): 0.4332858 --- L(Val, RNN): 0.3956151 --- L(Val, SINDy): 0.4481395 --- Time: 2.62s; --- Convergence: 3.12e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.004 1 + 0.775 value_reward_chosen[t] + 0.387 reward + 0.018 value_reward_chosen^2 + -0.393 value_reward_chosen*reward + 0.386 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.516 1 + 0.438 value_reward_not_chosen[t] + -0.053 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.013 1 + 1.003 value_reward_not_displayed[t] + -0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 108/1000 --- L(Train): 0.4313911 --- L(Val, RNN): 0.3957931 --- L(Val, SINDy): 0.4436982 --- Time: 2.50s; --- Convergence: 1.65e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.007 1 + 0.774 value_reward_chosen[t] + 0.39 reward + 0.02 value_reward_chosen^2 + -0.398 value_reward_chosen*reward + 0.389 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.519 1 + 0.429 value_reward_not_chosen[t] + -0.064 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.014 1 + 0.999 value_reward_not_displayed[t] + -0.016 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 109/1000 --- L(Train): 0.4301530 --- L(Val, RNN): 0.3944790 --- L(Val, SINDy): 0.4348744 --- Time: 2.58s; --- Convergence: 1.48e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.01 1 + 0.776 value_reward_chosen[t] + 0.397 reward + 0.022 value_reward_chosen^2 + -0.406 value_reward_chosen*reward + 0.396 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.521 1 + 0.419 value_reward_not_chosen[t] + -0.075 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.015 1 + 0.994 value_reward_not_displayed[t] + -0.02 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 110/1000 --- L(Train): 0.4228823 --- L(Val, RNN): 0.3937689 --- L(Val, SINDy): 0.4357065 --- Time: 2.58s; --- Convergence: 1.10e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.013 1 + 0.777 value_reward_chosen[t] + 0.403 reward + 0.023 value_reward_chosen^2 + -0.414 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.524 1 + 0.409 value_reward_not_chosen[t] + -0.085 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.015 1 + 0.99 value_reward_not_displayed[t] + -0.023 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 111/1000 --- L(Train): 0.4259640 --- L(Val, RNN): 0.3917183 --- L(Val, SINDy): 0.4498553 --- Time: 2.52s; --- Convergence: 1.57e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.012 1 + 0.779 value_reward_chosen[t] + 0.408 reward + 0.022 value_reward_chosen^2 + -0.423 value_reward_chosen*reward + 0.407 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.525 1 + 0.399 value_reward_not_chosen[t] + -0.092 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.015 1 + 0.986 value_reward_not_displayed[t] + -0.026 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 112/1000 --- L(Train): 0.4165800 --- L(Val, RNN): 0.3896607 --- L(Val, SINDy): 0.4513990 --- Time: 2.56s; --- Convergence: 1.82e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.008 1 + 0.783 value_reward_chosen[t] + 0.41 reward + 0.019 value_reward_chosen^2 + -0.431 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.526 1 + 0.388 value_reward_not_chosen[t] + -0.097 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.015 1 + 0.981 value_reward_not_displayed[t] + -0.028 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 113/1000 --- L(Train): 0.4165839 --- L(Val, RNN): 0.3872389 --- L(Val, SINDy): 0.4509074 --- Time: 2.54s; --- Convergence: 2.12e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.002 1 + 0.788 value_reward_chosen[t] + 0.409 reward + 0.013 value_reward_chosen^2 + -0.439 value_reward_chosen*reward + 0.408 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.526 1 + 0.377 value_reward_not_chosen[t] + -0.1 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.014 1 + 0.978 value_reward_not_displayed[t] + -0.03 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 114/1000 --- L(Train): 0.4006147 --- L(Val, RNN): 0.3851644 --- L(Val, SINDy): 0.4527675 --- Time: 2.55s; --- Convergence: 2.10e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.007 1 + 0.793 value_reward_chosen[t] + 0.404 reward + 0.006 value_reward_chosen^2 + -0.445 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.526 1 + 0.366 value_reward_not_chosen[t] + -0.103 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.013 1 + 0.975 value_reward_not_displayed[t] + -0.032 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 115/1000 --- L(Train): 0.4053185 --- L(Val, RNN): 0.3834713 --- L(Val, SINDy): 0.4485596 --- Time: 2.59s; --- Convergence: 1.89e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.016 1 + 0.798 value_reward_chosen[t] + 0.399 reward + -0.001 value_reward_chosen^2 + -0.45 value_reward_chosen*reward + 0.399 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.526 1 + 0.357 value_reward_not_chosen[t] + -0.105 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.012 1 + 0.972 value_reward_not_displayed[t] + -0.034 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 116/1000 --- L(Train): 0.4103063 --- L(Val, RNN): 0.3816682 --- L(Val, SINDy): 0.4400181 --- Time: 2.52s; --- Convergence: 1.85e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.023 1 + 0.802 value_reward_chosen[t] + 0.395 reward + -0.008 value_reward_chosen^2 + -0.455 value_reward_chosen*reward + 0.395 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.526 1 + 0.348 value_reward_not_chosen[t] + -0.107 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.011 1 + 0.969 value_reward_not_displayed[t] + -0.035 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 117/1000 --- L(Train): 0.4122964 --- L(Val, RNN): 0.3805504 --- L(Val, SINDy): 0.4386939 --- Time: 2.68s; --- Convergence: 1.48e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.031 1 + 0.804 value_reward_chosen[t] + 0.387 reward + -0.014 value_reward_chosen^2 + -0.455 value_reward_chosen*reward + 0.387 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.529 1 + 0.342 value_reward_not_chosen[t] + -0.113 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.012 1 + 0.969 value_reward_not_displayed[t] + -0.035 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 118/1000 --- L(Train): 0.4019507 --- L(Val, RNN): 0.3799340 --- L(Val, SINDy): 0.4520992 --- Time: 2.61s; --- Convergence: 1.05e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.039 1 + 0.806 value_reward_chosen[t] + 0.38 reward + -0.02 value_reward_chosen^2 + -0.454 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.531 1 + 0.336 value_reward_not_chosen[t] + -0.119 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.012 1 + 0.968 value_reward_not_displayed[t] + -0.034 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 119/1000 --- L(Train): 0.4030375 --- L(Val, RNN): 0.3794545 --- L(Val, SINDy): 0.4463149 --- Time: 2.51s; --- Convergence: 7.65e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.044 1 + 0.806 value_reward_chosen[t] + 0.375 reward + -0.024 value_reward_chosen^2 + -0.451 value_reward_chosen*reward + 0.374 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.535 1 + 0.331 value_reward_not_chosen[t] + -0.126 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.015 1 + 0.971 value_reward_not_displayed[t] + -0.032 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 120/1000 --- L(Train): 0.4046640 --- L(Val, RNN): 0.3785601 --- L(Val, SINDy): 0.4358027 --- Time: 2.50s; --- Convergence: 8.30e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.049 1 + 0.807 value_reward_chosen[t] + 0.37 reward + -0.028 value_reward_chosen^2 + -0.448 value_reward_chosen*reward + 0.37 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.538 1 + 0.327 value_reward_not_chosen[t] + -0.133 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.018 1 + 0.974 value_reward_not_displayed[t] + -0.029 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 121/1000 --- L(Train): 0.3951274 --- L(Val, RNN): 0.3775573 --- L(Val, SINDy): 0.4275413 --- Time: 2.55s; --- Convergence: 9.16e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.054 1 + 0.807 value_reward_chosen[t] + 0.366 reward + -0.031 value_reward_chosen^2 + -0.445 value_reward_chosen*reward + 0.365 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.541 1 + 0.324 value_reward_not_chosen[t] + -0.14 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.02 1 + 0.976 value_reward_not_displayed[t] + -0.027 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 122/1000 --- L(Train): 0.4232074 --- L(Val, RNN): 0.3766524 --- L(Val, SINDy): 0.4135519 --- Time: 2.72s; --- Convergence: 9.11e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.058 1 + 0.808 value_reward_chosen[t] + 0.362 reward + -0.034 value_reward_chosen^2 + -0.443 value_reward_chosen*reward + 0.361 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.544 1 + 0.32 value_reward_not_chosen[t] + -0.145 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.023 1 + 0.978 value_reward_not_displayed[t] + -0.025 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 123/1000 --- L(Train): 0.3915671 --- L(Val, RNN): 0.3763033 --- L(Val, SINDy): 0.4102093 --- Time: 2.50s; --- Convergence: 6.30e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.058 1 + 0.807 value_reward_chosen[t] + 0.364 reward + -0.034 value_reward_chosen^2 + -0.438 value_reward_chosen*reward + 0.363 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.547 1 + 0.317 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.024 1 + 0.981 value_reward_not_displayed[t] + -0.023 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 124/1000 --- L(Train): 0.3988999 --- L(Val, RNN): 0.3754930 --- L(Val, SINDy): 0.4176452 --- Time: 2.48s; --- Convergence: 7.20e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.059 1 + 0.807 value_reward_chosen[t] + 0.365 reward + -0.034 value_reward_chosen^2 + -0.433 value_reward_chosen*reward + 0.364 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.55 1 + 0.314 value_reward_not_chosen[t] + -0.157 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.025 1 + 0.984 value_reward_not_displayed[t] + -0.022 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 125/1000 --- L(Train): 0.3987561 --- L(Val, RNN): 0.3752628 --- L(Val, SINDy): 0.4111934 --- Time: 2.60s; --- Convergence: 4.75e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.059 1 + 0.807 value_reward_chosen[t] + 0.366 reward + -0.033 value_reward_chosen^2 + -0.428 value_reward_chosen*reward + 0.365 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.552 1 + 0.311 value_reward_not_chosen[t] + -0.162 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.025 1 + 0.987 value_reward_not_displayed[t] + -0.021 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 126/1000 --- L(Train): 0.3924809 --- L(Val, RNN): 0.3753177 --- L(Val, SINDy): 0.4174532 --- Time: 2.55s; --- Convergence: 2.65e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.059 1 + 0.807 value_reward_chosen[t] + 0.367 reward + -0.033 value_reward_chosen^2 + -0.424 value_reward_chosen*reward + 0.366 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.554 1 + 0.309 value_reward_not_chosen[t] + -0.166 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.026 1 + 0.989 value_reward_not_displayed[t] + -0.019 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 127/1000 --- L(Train): 0.3879158 --- L(Val, RNN): 0.3752043 --- L(Val, SINDy): 0.4102339 --- Time: 2.61s; --- Convergence: 1.89e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.06 1 + 0.807 value_reward_chosen[t] + 0.368 reward + -0.033 value_reward_chosen^2 + -0.421 value_reward_chosen*reward + 0.367 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.556 1 + 0.307 value_reward_not_chosen[t] + -0.17 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.027 1 + 0.991 value_reward_not_displayed[t] + -0.018 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 128/1000 --- L(Train): 0.3841269 --- L(Val, RNN): 0.3749550 --- L(Val, SINDy): 0.4128864 --- Time: 2.48s; --- Convergence: 2.19e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.052 1 + 0.804 value_reward_chosen[t] + 0.376 reward + -0.025 value_reward_chosen^2 + -0.412 value_reward_chosen*reward + 0.376 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.561 1 + 0.304 value_reward_not_chosen[t] + -0.177 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.026 1 + 0.995 value_reward_not_displayed[t] + -0.016 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 129/1000 --- L(Train): 0.3963375 --- L(Val, RNN): 0.3748387 --- L(Val, SINDy): 0.4130394 --- Time: 2.51s; --- Convergence: 1.68e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.046 1 + 0.8 value_reward_chosen[t] + 0.384 reward + -0.018 value_reward_chosen^2 + -0.403 value_reward_chosen*reward + 0.383 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.564 1 + 0.301 value_reward_not_chosen[t] + -0.183 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.026 1 + 0.999 value_reward_not_displayed[t] + -0.015 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 130/1000 --- L(Train): 0.3991216 --- L(Val, RNN): 0.3746355 --- L(Val, SINDy): 0.4147315 --- Time: 2.61s; --- Convergence: 1.86e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.039 1 + 0.797 value_reward_chosen[t] + 0.391 reward + -0.011 value_reward_chosen^2 + -0.395 value_reward_chosen*reward + 0.391 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.568 1 + 0.298 value_reward_not_chosen[t] + -0.189 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.024 1 + 1.002 value_reward_not_displayed[t] + -0.014 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 131/1000 --- L(Train): 0.3846467 --- L(Val, RNN): 0.3742265 --- L(Val, SINDy): 0.4099015 --- Time: 2.48s; --- Convergence: 2.97e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.033 1 + 0.793 value_reward_chosen[t] + 0.398 reward + -0.004 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.397 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.572 1 + 0.296 value_reward_not_chosen[t] + -0.195 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.022 1 + 1.004 value_reward_not_displayed[t] + -0.013 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 132/1000 --- L(Train): 0.3970858 --- L(Val, RNN): 0.3736091 --- L(Val, SINDy): 0.3989312 --- Time: 2.48s; --- Convergence: 4.57e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.027 1 + 0.79 value_reward_chosen[t] + 0.404 reward + 0.002 value_reward_chosen^2 + -0.381 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.575 1 + 0.293 value_reward_not_chosen[t] + -0.2 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.021 1 + 1.006 value_reward_not_displayed[t] + -0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 133/1000 --- L(Train): 0.3872336 --- L(Val, RNN): 0.3726732 --- L(Val, SINDy): 0.3960453 --- Time: 2.52s; --- Convergence: 6.97e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.788 value_reward_chosen[t] + 0.405 reward + 0.006 value_reward_chosen^2 + -0.376 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.578 1 + 0.289 value_reward_not_chosen[t] + -0.202 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.018 1 + 1.007 value_reward_not_displayed[t] + -0.013 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 134/1000 --- L(Train): 0.3750404 --- L(Val, RNN): 0.3718589 --- L(Val, SINDy): 0.3971803 --- Time: 2.59s; --- Convergence: 7.55e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.021 1 + 0.787 value_reward_chosen[t] + 0.406 reward + 0.009 value_reward_chosen^2 + -0.372 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.58 1 + 0.285 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.015 1 + 1.008 value_reward_not_displayed[t] + -0.013 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 135/1000 --- L(Train): 0.3878372 --- L(Val, RNN): 0.3714576 --- L(Val, SINDy): 0.4057893 --- Time: 2.50s; --- Convergence: 5.78e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.021 1 + 0.787 value_reward_chosen[t] + 0.402 reward + 0.009 value_reward_chosen^2 + -0.368 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.581 1 + 0.28 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.013 1 + 1.008 value_reward_not_displayed[t] + -0.014 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 136/1000 --- L(Train): 0.3853461 --- L(Val, RNN): 0.3716211 --- L(Val, SINDy): 0.4124742 --- Time: 2.50s; --- Convergence: 3.71e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.025 1 + 0.789 value_reward_chosen[t] + 0.395 reward + 0.006 value_reward_chosen^2 + -0.366 value_reward_chosen*reward + 0.394 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.582 1 + 0.273 value_reward_not_chosen[t] + -0.2 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.01 1 + 1.007 value_reward_not_displayed[t] + -0.015 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 137/1000 --- L(Train): 0.3937965 --- L(Val, RNN): 0.3718851 --- L(Val, SINDy): 0.4133090 --- Time: 2.55s; --- Convergence: 3.17e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.034 1 + 0.796 value_reward_chosen[t] + 0.382 reward + -0.003 value_reward_chosen^2 + -0.366 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.581 1 + 0.262 value_reward_not_chosen[t] + -0.192 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.01 1 + 1.006 value_reward_not_displayed[t] + -0.013 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 138/1000 --- L(Train): 0.3857864 --- L(Val, RNN): 0.3722339 --- L(Val, SINDy): 0.4118302 --- Time: 2.63s; --- Convergence: 3.33e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.042 1 + 0.802 value_reward_chosen[t] + 0.371 reward + -0.011 value_reward_chosen^2 + -0.366 value_reward_chosen*reward + 0.37 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.581 1 + 0.253 value_reward_not_chosen[t] + -0.185 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 1.004 value_reward_not_displayed[t] + -0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 139/1000 --- L(Train): 0.3883578 --- L(Val, RNN): 0.3723769 --- L(Val, SINDy): 0.4027542 --- Time: 2.59s; --- Convergence: 2.38e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.05 1 + 0.809 value_reward_chosen[t] + 0.36 reward + -0.019 value_reward_chosen^2 + -0.366 value_reward_chosen*reward + 0.36 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.581 1 + 0.244 value_reward_not_chosen[t] + -0.179 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 1.002 value_reward_not_displayed[t] + -0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 140/1000 --- L(Train): 0.4009251 --- L(Val, RNN): 0.3719413 --- L(Val, SINDy): 0.4007063 --- Time: 2.49s; --- Convergence: 3.37e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.057 1 + 0.815 value_reward_chosen[t] + 0.353 reward + -0.026 value_reward_chosen^2 + -0.364 value_reward_chosen*reward + 0.352 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.583 1 + 0.235 value_reward_not_chosen[t] + -0.177 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 1.001 value_reward_not_displayed[t] + -0.01 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 141/1000 --- L(Train): 0.3824789 --- L(Val, RNN): 0.3741344 --- L(Val, SINDy): 0.3946677 --- Time: 2.64s; --- Convergence: 1.26e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.063 1 + 0.821 value_reward_chosen[t] + 0.347 reward + -0.032 value_reward_chosen^2 + -0.362 value_reward_chosen*reward + 0.346 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.585 1 + 0.228 value_reward_not_chosen[t] + -0.175 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 0.999 value_reward_not_displayed[t] + -0.009 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 142/1000 --- L(Train): 0.3919950 --- L(Val, RNN): 0.3762601 --- L(Val, SINDy): 0.3854851 --- Time: 2.62s; --- Convergence: 1.70e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.067 1 + 0.826 value_reward_chosen[t] + 0.344 reward + -0.036 value_reward_chosen^2 + -0.356 value_reward_chosen*reward + 0.344 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.59 1 + 0.221 value_reward_not_chosen[t] + -0.177 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 0.998 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 143/1000 --- L(Train): 0.4030307 --- L(Val, RNN): 0.3773265 --- L(Val, SINDy): 0.3762436 --- Time: 2.53s; --- Convergence: 1.38e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.07 1 + 0.831 value_reward_chosen[t] + 0.344 reward + -0.039 value_reward_chosen^2 + -0.349 value_reward_chosen*reward + 0.343 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.594 1 + 0.216 value_reward_not_chosen[t] + -0.181 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 0.999 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 144/1000 --- L(Train): 0.4114271 --- L(Val, RNN): 0.3775139 --- L(Val, SINDy): 0.3752685 --- Time: 2.56s; --- Convergence: 7.84e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.07 1 + 0.834 value_reward_chosen[t] + 0.346 reward + -0.04 value_reward_chosen^2 + -0.341 value_reward_chosen*reward + 0.345 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.6 1 + 0.211 value_reward_not_chosen[t] + -0.186 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 1.0 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 145/1000 --- L(Train): 0.3884969 --- L(Val, RNN): 0.3772129 --- L(Val, SINDy): 0.3734625 --- Time: 2.75s; --- Convergence: 5.43e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.067 1 + 0.833 value_reward_chosen[t] + 0.352 reward + -0.037 value_reward_chosen^2 + -0.333 value_reward_chosen*reward + 0.351 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.607 1 + 0.206 value_reward_not_chosen[t] + -0.192 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.008 1 + 1.001 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 146/1000 --- L(Train): 0.4283125 --- L(Val, RNN): 0.3771322 --- L(Val, SINDy): 0.3739769 --- Time: 2.53s; --- Convergence: 3.12e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.06 1 + 0.83 value_reward_chosen[t] + 0.362 reward + -0.031 value_reward_chosen^2 + -0.329 value_reward_chosen*reward + 0.361 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.614 1 + 0.2 value_reward_not_chosen[t] + -0.199 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.005 1 + 1.0 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 147/1000 --- L(Train): 0.3953548 --- L(Val, RNN): 0.3767596 --- L(Val, SINDy): 0.3775415 --- Time: 2.48s; --- Convergence: 3.42e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.054 1 + 0.826 value_reward_chosen[t] + 0.371 reward + -0.025 value_reward_chosen^2 + -0.326 value_reward_chosen*reward + 0.371 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.62 1 + 0.193 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 0.999 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 148/1000 --- L(Train): 0.3786175 --- L(Val, RNN): 0.3764139 --- L(Val, SINDy): 0.3800537 --- Time: 2.54s; --- Convergence: 3.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.047 1 + 0.822 value_reward_chosen[t] + 0.381 reward + -0.019 value_reward_chosen^2 + -0.329 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.625 1 + 0.184 value_reward_not_chosen[t] + -0.205 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 149/1000 --- L(Train): 0.3916447 --- L(Val, RNN): 0.3760087 --- L(Val, SINDy): 0.3812445 --- Time: 3.01s; --- Convergence: 3.75e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.042 1 + 0.819 value_reward_chosen[t] + 0.389 reward + -0.015 value_reward_chosen^2 + -0.338 value_reward_chosen*reward + 0.388 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.628 1 + 0.171 value_reward_not_chosen[t] + -0.202 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 150/1000 --- L(Train): 0.3934814 --- L(Val, RNN): 0.3754304 --- L(Val, SINDy): 0.3841430 --- Time: 2.87s; --- Convergence: 4.76e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.038 1 + 0.815 value_reward_chosen[t] + 0.396 reward + -0.012 value_reward_chosen^2 + -0.347 value_reward_chosen*reward + 0.395 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.631 1 + 0.16 value_reward_not_chosen[t] + -0.199 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.003 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 151/1000 --- L(Train): 0.3812289 --- L(Val, RNN): 0.3748449 --- L(Val, SINDy): 0.3806874 --- Time: 2.62s; --- Convergence: 5.31e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.038 1 + 0.817 value_reward_chosen[t] + 0.399 reward + -0.013 value_reward_chosen^2 + -0.358 value_reward_chosen*reward + 0.398 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.634 1 + 0.148 value_reward_not_chosen[t] + -0.197 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.003 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 152/1000 --- L(Train): 0.4018001 --- L(Val, RNN): 0.3740868 --- L(Val, SINDy): 0.3799831 --- Time: 2.75s; --- Convergence: 6.45e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.037 1 + 0.818 value_reward_chosen[t] + 0.401 reward + -0.014 value_reward_chosen^2 + -0.368 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.637 1 + 0.138 value_reward_not_chosen[t] + -0.194 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.004 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 153/1000 --- L(Train): 0.4045275 --- L(Val, RNN): 0.3733661 --- L(Val, SINDy): 0.3752435 --- Time: 2.85s; --- Convergence: 6.83e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.039 1 + 0.821 value_reward_chosen[t] + 0.402 reward + -0.016 value_reward_chosen^2 + -0.379 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.641 1 + 0.128 value_reward_not_chosen[t] + -0.193 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.003 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 154/1000 --- L(Train): 0.3879692 --- L(Val, RNN): 0.3729610 --- L(Val, SINDy): 0.3748254 --- Time: 2.62s; --- Convergence: 5.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.04 1 + 0.824 value_reward_chosen[t] + 0.403 reward + -0.018 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.644 1 + 0.12 value_reward_not_chosen[t] + -0.192 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.002 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 155/1000 --- L(Train): 0.3813656 --- L(Val, RNN): 0.3727046 --- L(Val, SINDy): 0.3729308 --- Time: 2.53s; --- Convergence: 4.00e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.042 1 + 0.829 value_reward_chosen[t] + 0.403 reward + -0.021 value_reward_chosen^2 + -0.399 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.649 1 + 0.113 value_reward_not_chosen[t] + -0.194 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 1.0 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 156/1000 --- L(Train): 0.3861326 --- L(Val, RNN): 0.3724549 --- L(Val, SINDy): 0.3711778 --- Time: 2.86s; --- Convergence: 3.25e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.046 1 + 0.837 value_reward_chosen[t] + 0.402 reward + -0.025 value_reward_chosen^2 + -0.411 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.656 1 + 0.108 value_reward_not_chosen[t] + -0.199 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.998 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 157/1000 --- L(Train): 0.3828572 --- L(Val, RNN): 0.3721466 --- L(Val, SINDy): 0.3729700 --- Time: 2.82s; --- Convergence: 3.17e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.048 1 + 0.844 value_reward_chosen[t] + 0.401 reward + -0.028 value_reward_chosen^2 + -0.422 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.662 1 + 0.103 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.995 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 158/1000 --- L(Train): 0.4002450 --- L(Val, RNN): 0.3718689 --- L(Val, SINDy): 0.3763955 --- Time: 2.53s; --- Convergence: 2.97e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.051 1 + 0.851 value_reward_chosen[t] + 0.401 reward + -0.031 value_reward_chosen^2 + -0.431 value_reward_chosen*reward + 0.4 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.667 1 + 0.098 value_reward_not_chosen[t] + -0.208 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.993 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 159/1000 --- L(Train): 0.3835495 --- L(Val, RNN): 0.3716534 --- L(Val, SINDy): 0.3772628 --- Time: 2.52s; --- Convergence: 2.56e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.052 1 + 0.858 value_reward_chosen[t] + 0.401 reward + -0.033 value_reward_chosen^2 + -0.441 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.672 1 + 0.093 value_reward_not_chosen[t] + -0.211 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.994 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 160/1000 --- L(Train): 0.3803749 --- L(Val, RNN): 0.3715011 --- L(Val, SINDy): 0.3842060 --- Time: 2.70s; --- Convergence: 2.04e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.052 1 + 0.865 value_reward_chosen[t] + 0.403 reward + -0.033 value_reward_chosen^2 + -0.451 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.676 1 + 0.087 value_reward_not_chosen[t] + -0.212 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.996 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 161/1000 --- L(Train): 0.3978060 --- L(Val, RNN): 0.3712545 --- L(Val, SINDy): 0.3865079 --- Time: 2.69s; --- Convergence: 2.25e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.051 1 + 0.872 value_reward_chosen[t] + 0.404 reward + -0.034 value_reward_chosen^2 + -0.459 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.68 1 + 0.082 value_reward_not_chosen[t] + -0.214 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.998 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 162/1000 --- L(Train): 0.3872254 --- L(Val, RNN): 0.3708532 --- L(Val, SINDy): 0.3819069 --- Time: 2.59s; --- Convergence: 3.13e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.051 1 + 0.878 value_reward_chosen[t] + 0.405 reward + -0.034 value_reward_chosen^2 + -0.466 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.684 1 + 0.077 value_reward_not_chosen[t] + -0.215 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.999 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 163/1000 --- L(Train): 0.3953741 --- L(Val, RNN): 0.3703414 --- L(Val, SINDy): 0.3839710 --- Time: 2.54s; --- Convergence: 4.13e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.051 1 + 0.883 value_reward_chosen[t] + 0.406 reward + -0.034 value_reward_chosen^2 + -0.473 value_reward_chosen*reward + 0.405 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.687 1 + 0.072 value_reward_not_chosen[t] + -0.216 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 1.0 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 164/1000 --- L(Train): 0.3927118 --- L(Val, RNN): 0.3699203 --- L(Val, SINDy): 0.3832093 --- Time: 2.71s; --- Convergence: 4.17e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.051 1 + 0.888 value_reward_chosen[t] + 0.406 reward + -0.035 value_reward_chosen^2 + -0.479 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.69 1 + 0.068 value_reward_not_chosen[t] + -0.217 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 1.002 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 165/1000 --- L(Train): 0.3987445 --- L(Val, RNN): 0.3696343 --- L(Val, SINDy): 0.3880130 --- Time: 2.62s; --- Convergence: 3.51e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.05 1 + 0.891 value_reward_chosen[t] + 0.405 reward + -0.034 value_reward_chosen^2 + -0.482 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.691 1 + 0.063 value_reward_not_chosen[t] + -0.214 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 1.001 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 166/1000 --- L(Train): 0.4036603 --- L(Val, RNN): 0.3697018 --- L(Val, SINDy): 0.3878113 --- Time: 2.57s; --- Convergence: 2.09e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.049 1 + 0.892 value_reward_chosen[t] + 0.402 reward + -0.033 value_reward_chosen^2 + -0.481 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.692 1 + 0.056 value_reward_not_chosen[t] + -0.21 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.999 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 167/1000 --- L(Train): 0.3961892 --- L(Val, RNN): 0.3697043 --- L(Val, SINDy): 0.3836810 --- Time: 2.61s; --- Convergence: 1.06e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.047 1 + 0.892 value_reward_chosen[t] + 0.398 reward + -0.032 value_reward_chosen^2 + -0.48 value_reward_chosen*reward + 0.398 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.693 1 + 0.049 value_reward_not_chosen[t] + -0.205 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.997 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 168/1000 --- L(Train): 0.3964389 --- L(Val, RNN): 0.3697714 --- L(Val, SINDy): 0.3827212 --- Time: 2.68s; --- Convergence: 8.65e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.045 1 + 0.892 value_reward_chosen[t] + 0.395 reward + -0.031 value_reward_chosen^2 + -0.479 value_reward_chosen*reward + 0.395 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.693 1 + 0.043 value_reward_not_chosen[t] + -0.201 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.996 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 169/1000 --- L(Train): 0.3880989 --- L(Val, RNN): 0.3695647 --- L(Val, SINDy): 0.3744685 --- Time: 2.65s; --- Convergence: 1.47e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.043 1 + 0.891 value_reward_chosen[t] + 0.393 reward + -0.029 value_reward_chosen^2 + -0.477 value_reward_chosen*reward + 0.393 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.695 1 + 0.038 value_reward_not_chosen[t] + -0.198 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.995 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 170/1000 --- L(Train): 0.3793310 --- L(Val, RNN): 0.3692050 --- L(Val, SINDy): 0.3797071 --- Time: 2.53s; --- Convergence: 2.53e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.04 1 + 0.89 value_reward_chosen[t] + 0.392 reward + -0.027 value_reward_chosen^2 + -0.475 value_reward_chosen*reward + 0.391 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.696 1 + 0.033 value_reward_not_chosen[t] + -0.195 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.995 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 171/1000 --- L(Train): 0.4062706 --- L(Val, RNN): 0.3688886 --- L(Val, SINDy): 0.3787920 --- Time: 2.54s; --- Convergence: 2.85e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.038 1 + 0.89 value_reward_chosen[t] + 0.39 reward + -0.025 value_reward_chosen^2 + -0.473 value_reward_chosen*reward + 0.389 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.698 1 + 0.029 value_reward_not_chosen[t] + -0.193 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.995 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 172/1000 --- L(Train): 0.3915660 --- L(Val, RNN): 0.3687881 --- L(Val, SINDy): 0.3816755 --- Time: 2.75s; --- Convergence: 1.93e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.035 1 + 0.889 value_reward_chosen[t] + 0.39 reward + -0.023 value_reward_chosen^2 + -0.471 value_reward_chosen*reward + 0.389 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.702 1 + 0.027 value_reward_not_chosen[t] + -0.195 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.996 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 173/1000 --- L(Train): 0.3921744 --- L(Val, RNN): 0.3688528 --- L(Val, SINDy): 0.3766946 --- Time: 2.51s; --- Convergence: 1.29e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.034 1 + 0.891 value_reward_chosen[t] + 0.392 reward + -0.022 value_reward_chosen^2 + -0.469 value_reward_chosen*reward + 0.391 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.709 1 + 0.029 value_reward_not_chosen[t] + -0.2 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.998 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 174/1000 --- L(Train): 0.3943197 --- L(Val, RNN): 0.3688366 --- L(Val, SINDy): 0.3748761 --- Time: 2.50s; --- Convergence: 7.24e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.033 1 + 0.893 value_reward_chosen[t] + 0.393 reward + -0.022 value_reward_chosen^2 + -0.467 value_reward_chosen*reward + 0.393 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.715 1 + 0.029 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.0 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 175/1000 --- L(Train): 0.3660030 --- L(Val, RNN): 0.3686053 --- L(Val, SINDy): 0.3718330 --- Time: 2.49s; --- Convergence: 1.52e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.033 1 + 0.896 value_reward_chosen[t] + 0.395 reward + -0.022 value_reward_chosen^2 + -0.466 value_reward_chosen*reward + 0.394 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.72 1 + 0.028 value_reward_not_chosen[t] + -0.205 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.001 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 176/1000 --- L(Train): 0.3961572 --- L(Val, RNN): 0.3681971 --- L(Val, SINDy): 0.3724576 --- Time: 2.64s; --- Convergence: 2.80e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.033 1 + 0.899 value_reward_chosen[t] + 0.396 reward + -0.022 value_reward_chosen^2 + -0.465 value_reward_chosen*reward + 0.395 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.724 1 + 0.027 value_reward_not_chosen[t] + -0.206 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.002 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 177/1000 --- L(Train): 0.3909412 --- L(Val, RNN): 0.3678978 --- L(Val, SINDy): 0.3711440 --- Time: 2.62s; --- Convergence: 2.90e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.033 1 + 0.901 value_reward_chosen[t] + 0.397 reward + -0.023 value_reward_chosen^2 + -0.465 value_reward_chosen*reward + 0.396 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.727 1 + 0.027 value_reward_not_chosen[t] + -0.206 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.003 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 178/1000 --- L(Train): 0.3942048 --- L(Val, RNN): 0.3681636 --- L(Val, SINDy): 0.3705351 --- Time: 2.69s; --- Convergence: 2.78e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.033 1 + 0.904 value_reward_chosen[t] + 0.397 reward + -0.024 value_reward_chosen^2 + -0.465 value_reward_chosen*reward + 0.397 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.728 1 + 0.023 value_reward_not_chosen[t] + -0.203 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 179/1000 --- L(Train): 0.3834141 --- L(Val, RNN): 0.3683023 --- L(Val, SINDy): 0.3709900 --- Time: 2.62s; --- Convergence: 2.08e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.036 1 + 0.905 value_reward_chosen[t] + 0.396 reward + -0.028 value_reward_chosen^2 + -0.468 value_reward_chosen*reward + 0.395 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.724 1 + 0.015 value_reward_not_chosen[t] + -0.192 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 180/1000 --- L(Train): 0.3881890 --- L(Val, RNN): 0.3683784 --- L(Val, SINDy): 0.3696649 --- Time: 2.59s; --- Convergence: 1.42e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.038 1 + 0.906 value_reward_chosen[t] + 0.395 reward + -0.031 value_reward_chosen^2 + -0.471 value_reward_chosen*reward + 0.394 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.72 1 + 0.007 value_reward_not_chosen[t] + -0.183 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.995 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 181/1000 --- L(Train): 0.3772127 --- L(Val, RNN): 0.3681133 --- L(Val, SINDy): 0.3698914 --- Time: 2.64s; --- Convergence: 2.04e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.038 1 + 0.906 value_reward_chosen[t] + 0.397 reward + -0.032 value_reward_chosen^2 + -0.474 value_reward_chosen*reward + 0.396 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.718 1 + 0.001 value_reward_not_chosen[t] + -0.175 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.993 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 182/1000 --- L(Train): 0.3919487 --- L(Val, RNN): 0.3676168 --- L(Val, SINDy): 0.3705797 --- Time: 2.55s; --- Convergence: 3.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.037 1 + 0.906 value_reward_chosen[t] + 0.399 reward + -0.032 value_reward_chosen^2 + -0.478 value_reward_chosen*reward + 0.399 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.718 1 + -0.002 value_reward_not_chosen[t] + -0.171 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.992 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 183/1000 --- L(Train): 0.3972991 --- L(Val, RNN): 0.3673219 --- L(Val, SINDy): 0.3698942 --- Time: 2.55s; --- Convergence: 3.22e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.036 1 + 0.905 value_reward_chosen[t] + 0.402 reward + -0.033 value_reward_chosen^2 + -0.481 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.719 1 + -0.004 value_reward_not_chosen[t] + -0.167 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.992 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 184/1000 --- L(Train): 0.3743680 --- L(Val, RNN): 0.3674960 --- L(Val, SINDy): 0.3703447 --- Time: 2.71s; --- Convergence: 2.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.034 1 + 0.904 value_reward_chosen[t] + 0.405 reward + -0.032 value_reward_chosen^2 + -0.485 value_reward_chosen*reward + 0.405 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.722 1 + -0.004 value_reward_not_chosen[t] + -0.167 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.993 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 185/1000 --- L(Train): 0.3812136 --- L(Val, RNN): 0.3678640 --- L(Val, SINDy): 0.3708981 --- Time: 2.52s; --- Convergence: 3.08e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.031 1 + 0.903 value_reward_chosen[t] + 0.41 reward + -0.031 value_reward_chosen^2 + -0.489 value_reward_chosen*reward + 0.409 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.727 1 + -0.001 value_reward_not_chosen[t] + -0.169 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.995 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 186/1000 --- L(Train): 0.3845664 --- L(Val, RNN): 0.3681130 --- L(Val, SINDy): 0.3741784 --- Time: 2.62s; --- Convergence: 2.79e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.028 1 + 0.902 value_reward_chosen[t] + 0.414 reward + -0.03 value_reward_chosen^2 + -0.492 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.732 1 + 0.002 value_reward_not_chosen[t] + -0.172 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.997 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 187/1000 --- L(Train): 0.3904957 --- L(Val, RNN): 0.3679062 --- L(Val, SINDy): 0.3739481 --- Time: 2.63s; --- Convergence: 2.43e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.026 1 + 0.901 value_reward_chosen[t] + 0.417 reward + -0.028 value_reward_chosen^2 + -0.495 value_reward_chosen*reward + 0.416 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.737 1 + 0.004 value_reward_not_chosen[t] + -0.174 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.999 value_reward_not_displayed[t] + -0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 188/1000 --- L(Train): 0.4037164 --- L(Val, RNN): 0.3673750 --- L(Val, SINDy): 0.3738360 --- Time: 2.66s; --- Convergence: 3.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.025 1 + 0.902 value_reward_chosen[t] + 0.421 reward + -0.029 value_reward_chosen^2 + -0.501 value_reward_chosen*reward + 0.42 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.74 1 + 0.006 value_reward_not_chosen[t] + -0.174 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 189/1000 --- L(Train): 0.3895762 --- L(Val, RNN): 0.3670900 --- L(Val, SINDy): 0.3734003 --- Time: 2.57s; --- Convergence: 3.36e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.903 value_reward_chosen[t] + 0.424 reward + -0.029 value_reward_chosen^2 + -0.506 value_reward_chosen*reward + 0.424 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.743 1 + 0.007 value_reward_not_chosen[t] + -0.175 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 190/1000 --- L(Train): 0.3771129 --- L(Val, RNN): 0.3671414 --- L(Val, SINDy): 0.3748534 --- Time: 2.53s; --- Convergence: 1.94e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.905 value_reward_chosen[t] + 0.428 reward + -0.03 value_reward_chosen^2 + -0.511 value_reward_chosen*reward + 0.427 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.746 1 + 0.008 value_reward_not_chosen[t] + -0.175 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.002 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 191/1000 --- L(Train): 0.3799326 --- L(Val, RNN): 0.3673214 --- L(Val, SINDy): 0.3834505 --- Time: 2.69s; --- Convergence: 1.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.907 value_reward_chosen[t] + 0.43 reward + -0.032 value_reward_chosen^2 + -0.516 value_reward_chosen*reward + 0.429 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.747 1 + 0.008 value_reward_not_chosen[t] + -0.173 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 192/1000 --- L(Train): 0.3804818 --- L(Val, RNN): 0.3675462 --- L(Val, SINDy): 0.3892980 --- Time: 2.55s; --- Convergence: 2.06e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.909 value_reward_chosen[t] + 0.432 reward + -0.033 value_reward_chosen^2 + -0.521 value_reward_chosen*reward + 0.431 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.748 1 + 0.007 value_reward_not_chosen[t] + -0.17 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 193/1000 --- L(Train): 0.3953887 --- L(Val, RNN): 0.3672512 --- L(Val, SINDy): 0.3782016 --- Time: 2.50s; --- Convergence: 2.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.911 value_reward_chosen[t] + 0.434 reward + -0.035 value_reward_chosen^2 + -0.53 value_reward_chosen*reward + 0.434 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.745 1 + 0.003 value_reward_not_chosen[t] + -0.163 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.996 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 194/1000 --- L(Train): 0.3862573 --- L(Val, RNN): 0.3667546 --- L(Val, SINDy): 0.3801627 --- Time: 2.56s; --- Convergence: 3.74e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.024 1 + 0.913 value_reward_chosen[t] + 0.437 reward + -0.037 value_reward_chosen^2 + -0.538 value_reward_chosen*reward + 0.436 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.742 1 + -0.001 value_reward_not_chosen[t] + -0.156 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.993 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 195/1000 --- L(Train): 0.3727956 --- L(Val, RNN): 0.3665317 --- L(Val, SINDy): 0.3797618 --- Time: 2.61s; --- Convergence: 2.98e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.023 1 + 0.916 value_reward_chosen[t] + 0.44 reward + -0.038 value_reward_chosen^2 + -0.547 value_reward_chosen*reward + 0.44 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.741 1 + -0.003 value_reward_not_chosen[t] + -0.152 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.993 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 196/1000 --- L(Train): 0.3837907 --- L(Val, RNN): 0.3668250 --- L(Val, SINDy): 0.3808447 --- Time: 2.67s; --- Convergence: 2.96e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.018 1 + 0.919 value_reward_chosen[t] + 0.446 reward + -0.036 value_reward_chosen^2 + -0.556 value_reward_chosen*reward + 0.445 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.745 1 + -0.001 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 197/1000 --- L(Train): 0.3763804 --- L(Val, RNN): 0.3673436 --- L(Val, SINDy): 0.3726907 --- Time: 2.55s; --- Convergence: 4.07e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.013 1 + 0.922 value_reward_chosen[t] + 0.451 reward + -0.035 value_reward_chosen^2 + -0.566 value_reward_chosen*reward + 0.451 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.748 1 + 0.002 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 198/1000 --- L(Train): 0.3800798 --- L(Val, RNN): 0.3675108 --- L(Val, SINDy): 0.3690102 --- Time: 2.50s; --- Convergence: 2.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.009 1 + 0.926 value_reward_chosen[t] + 0.456 reward + -0.034 value_reward_chosen^2 + -0.576 value_reward_chosen*reward + 0.455 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.752 1 + 0.005 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.006 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 199/1000 --- L(Train): 0.3801182 --- L(Val, RNN): 0.3670888 --- L(Val, SINDy): 0.3694683 --- Time: 2.65s; --- Convergence: 3.55e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.005 1 + 0.928 value_reward_chosen[t] + 0.461 reward + -0.033 value_reward_chosen^2 + -0.584 value_reward_chosen*reward + 0.46 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.755 1 + 0.007 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.01 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 200/1000 --- L(Train): 0.3895285 --- L(Val, RNN): 0.3664994 --- L(Val, SINDy): 0.3694781 --- Time: 2.59s; --- Convergence: 4.72e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.931 value_reward_chosen[t] + 0.465 reward + -0.032 value_reward_chosen^2 + -0.591 value_reward_chosen*reward + 0.464 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.758 1 + 0.009 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.013 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 201/1000 --- L(Train): 0.3985822 --- L(Val, RNN): 0.3666524 --- L(Val, SINDy): 0.3694222 --- Time: 2.51s; --- Convergence: 3.13e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.934 value_reward_chosen[t] + 0.465 reward + -0.034 value_reward_chosen^2 + -0.6 value_reward_chosen*reward + 0.464 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.758 1 + 0.009 value_reward_not_chosen[t] + -0.148 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.009 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 202/1000 --- L(Train): 0.3993929 --- L(Val, RNN): 0.3674518 --- L(Val, SINDy): 0.3694641 --- Time: 2.64s; --- Convergence: 5.56e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.003 1 + 0.938 value_reward_chosen[t] + 0.46 reward + -0.04 value_reward_chosen^2 + -0.611 value_reward_chosen*reward + 0.46 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.755 1 + 0.005 value_reward_not_chosen[t] + -0.141 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.999 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 203/1000 --- L(Train): 0.3679457 --- L(Val, RNN): 0.3678279 --- L(Val, SINDy): 0.3781063 --- Time: 2.59s; --- Convergence: 4.66e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.005 1 + 0.942 value_reward_chosen[t] + 0.457 reward + -0.046 value_reward_chosen^2 + -0.622 value_reward_chosen*reward + 0.456 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.752 1 + 0.001 value_reward_not_chosen[t] + -0.134 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.99 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 204/1000 --- L(Train): 0.3822774 --- L(Val, RNN): 0.3674324 --- L(Val, SINDy): 0.3857390 --- Time: 2.66s; --- Convergence: 4.31e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.946 value_reward_chosen[t] + 0.458 reward + -0.046 value_reward_chosen^2 + -0.631 value_reward_chosen*reward + 0.457 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.75 1 + -0.001 value_reward_not_chosen[t] + -0.127 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.986 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 205/1000 --- L(Train): 0.3841059 --- L(Val, RNN): 0.3669284 --- L(Val, SINDy): 0.3891540 --- Time: 2.54s; --- Convergence: 4.67e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.006 1 + 0.949 value_reward_chosen[t] + 0.463 reward + -0.043 value_reward_chosen^2 + -0.637 value_reward_chosen*reward + 0.462 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.751 1 + -0.001 value_reward_not_chosen[t] + -0.124 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.004 1 + 0.989 value_reward_not_displayed[t] + 0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 206/1000 --- L(Train): 0.3829572 --- L(Val, RNN): 0.3671433 --- L(Val, SINDy): 0.3892171 --- Time: 2.49s; --- Convergence: 3.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.014 1 + 0.951 value_reward_chosen[t] + 0.467 reward + -0.039 value_reward_chosen^2 + -0.642 value_reward_chosen*reward + 0.467 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.755 1 + 0.004 value_reward_not_chosen[t] + -0.125 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 0.994 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 207/1000 --- L(Train): 0.3812857 --- L(Val, RNN): 0.3676381 --- L(Val, SINDy): 0.3857487 --- Time: 2.76s; --- Convergence: 4.18e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.019 1 + 0.953 value_reward_chosen[t] + 0.468 reward + -0.039 value_reward_chosen^2 + -0.647 value_reward_chosen*reward + 0.468 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.761 1 + 0.01 value_reward_not_chosen[t] + -0.127 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 208/1000 --- L(Train): 0.4007198 --- L(Val, RNN): 0.3678321 --- L(Val, SINDy): 0.3735165 --- Time: 2.54s; --- Convergence: 3.06e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.022 1 + 0.956 value_reward_chosen[t] + 0.468 reward + -0.041 value_reward_chosen^2 + -0.652 value_reward_chosen*reward + 0.467 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + 0.014 value_reward_not_chosen[t] + -0.128 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.011 1 + 1.003 value_reward_not_displayed[t] + -0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 209/1000 --- L(Train): 0.3827563 --- L(Val, RNN): 0.3678506 --- L(Val, SINDy): 0.3697158 --- Time: 2.53s; --- Convergence: 1.62e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.023 1 + 0.961 value_reward_chosen[t] + 0.466 reward + -0.044 value_reward_chosen^2 + -0.658 value_reward_chosen*reward + 0.465 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.767 1 + 0.015 value_reward_not_chosen[t] + -0.125 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.012 1 + 1.005 value_reward_not_displayed[t] + -0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 210/1000 --- L(Train): 0.3780316 --- L(Val, RNN): 0.3676097 --- L(Val, SINDy): 0.3721480 --- Time: 2.53s; --- Convergence: 2.02e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.023 1 + 0.966 value_reward_chosen[t] + 0.464 reward + -0.048 value_reward_chosen^2 + -0.662 value_reward_chosen*reward + 0.464 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.015 value_reward_not_chosen[t] + -0.123 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.013 1 + 1.006 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 211/1000 --- L(Train): 0.3924185 --- L(Val, RNN): 0.3675887 --- L(Val, SINDy): 0.3713386 --- Time: 2.67s; --- Convergence: 1.11e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.024 1 + 0.97 value_reward_chosen[t] + 0.463 reward + -0.051 value_reward_chosen^2 + -0.666 value_reward_chosen*reward + 0.462 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.016 value_reward_not_chosen[t] + -0.121 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.015 1 + 1.007 value_reward_not_displayed[t] + -0.01 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 212/1000 --- L(Train): 0.3814746 --- L(Val, RNN): 0.3677555 --- L(Val, SINDy): 0.3698434 --- Time: 2.70s; --- Convergence: 1.39e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.024 1 + 0.974 value_reward_chosen[t] + 0.462 reward + -0.054 value_reward_chosen^2 + -0.67 value_reward_chosen*reward + 0.461 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.77 1 + 0.017 value_reward_not_chosen[t] + -0.119 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.016 1 + 1.008 value_reward_not_displayed[t] + -0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 213/1000 --- L(Train): 0.3864333 --- L(Val, RNN): 0.3680003 --- L(Val, SINDy): 0.3700653 --- Time: 2.53s; --- Convergence: 1.92e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.026 1 + 0.977 value_reward_chosen[t] + 0.463 reward + -0.056 value_reward_chosen^2 + -0.672 value_reward_chosen*reward + 0.462 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.015 value_reward_not_chosen[t] + -0.114 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.011 1 + 1.006 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 214/1000 --- L(Train): 0.3819144 --- L(Val, RNN): 0.3680702 --- L(Val, SINDy): 0.3702303 --- Time: 2.61s; --- Convergence: 1.31e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.03 1 + 0.978 value_reward_chosen[t] + 0.465 reward + -0.056 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.464 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + 0.012 value_reward_not_chosen[t] + -0.108 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.002 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 215/1000 --- L(Train): 0.3970824 --- L(Val, RNN): 0.3678229 --- L(Val, SINDy): 0.3704070 --- Time: 2.62s; --- Convergence: 1.89e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 0.976 value_reward_chosen[t] + 0.469 reward + -0.055 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.468 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + 0.009 value_reward_not_chosen[t] + -0.102 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.005 1 + 0.996 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 216/1000 --- L(Train): 0.3777871 --- L(Val, RNN): 0.3680590 --- L(Val, SINDy): 0.3704700 --- Time: 2.54s; --- Convergence: 2.13e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 0.974 value_reward_chosen[t] + 0.472 reward + -0.054 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.471 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.761 1 + 0.006 value_reward_not_chosen[t] + -0.097 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.011 1 + 0.991 value_reward_not_displayed[t] + 0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 217/1000 --- L(Train): 0.3868945 --- L(Val, RNN): 0.3688224 --- L(Val, SINDy): 0.3703162 --- Time: 2.61s; --- Convergence: 4.88e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.973 value_reward_chosen[t] + 0.473 reward + -0.056 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.472 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.76 1 + 0.004 value_reward_not_chosen[t] + -0.093 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.011 1 + 0.986 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 218/1000 --- L(Train): 0.3848753 --- L(Val, RNN): 0.3691357 --- L(Val, SINDy): 0.3701762 --- Time: 2.57s; --- Convergence: 4.01e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.042 1 + 0.971 value_reward_chosen[t] + 0.474 reward + -0.057 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.474 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.759 1 + 0.003 value_reward_not_chosen[t] + -0.09 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.011 1 + 0.982 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 219/1000 --- L(Train): 0.3970010 --- L(Val, RNN): 0.3687818 --- L(Val, SINDy): 0.3703890 --- Time: 2.67s; --- Convergence: 3.77e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.97 value_reward_chosen[t] + 0.475 reward + -0.06 value_reward_chosen^2 + -0.674 value_reward_chosen*reward + 0.474 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.76 1 + 0.004 value_reward_not_chosen[t] + -0.089 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.006 1 + 0.981 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 220/1000 --- L(Train): 0.3878480 --- L(Val, RNN): 0.3682940 --- L(Val, SINDy): 0.3707192 --- Time: 2.58s; --- Convergence: 4.33e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.971 value_reward_chosen[t] + 0.474 reward + -0.067 value_reward_chosen^2 + -0.674 value_reward_chosen*reward + 0.473 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.767 1 + 0.01 value_reward_not_chosen[t] + -0.095 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.987 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 221/1000 --- L(Train): 0.3776319 --- L(Val, RNN): 0.3684721 --- L(Val, SINDy): 0.3707388 --- Time: 2.58s; --- Convergence: 3.05e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 0.971 value_reward_chosen[t] + 0.473 reward + -0.073 value_reward_chosen^2 + -0.674 value_reward_chosen*reward + 0.472 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.016 value_reward_not_chosen[t] + -0.099 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.014 1 + 0.992 value_reward_not_displayed[t] + -0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 222/1000 --- L(Train): 0.3912606 --- L(Val, RNN): 0.3689837 --- L(Val, SINDy): 0.3707677 --- Time: 2.62s; --- Convergence: 4.08e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 0.971 value_reward_chosen[t] + 0.472 reward + -0.078 value_reward_chosen^2 + -0.674 value_reward_chosen*reward + 0.471 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.021 value_reward_not_chosen[t] + -0.104 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.022 1 + 0.997 value_reward_not_displayed[t] + -0.017 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 223/1000 --- L(Train): 0.3899527 --- L(Val, RNN): 0.3686448 --- L(Val, SINDy): 0.3706978 --- Time: 2.55s; --- Convergence: 3.74e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 0.972 value_reward_chosen[t] + 0.471 reward + -0.083 value_reward_chosen^2 + -0.674 value_reward_chosen*reward + 0.471 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.026 value_reward_not_chosen[t] + -0.108 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.029 1 + 1.002 value_reward_not_displayed[t] + -0.022 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 224/1000 --- L(Train): 0.3895295 --- L(Val, RNN): 0.3681107 --- L(Val, SINDy): 0.3705913 --- Time: 2.56s; --- Convergence: 4.54e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.972 value_reward_chosen[t] + 0.476 reward + -0.084 value_reward_chosen^2 + -0.67 value_reward_chosen*reward + 0.475 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.784 1 + 0.025 value_reward_not_chosen[t] + -0.105 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.025 1 + 1.006 value_reward_not_displayed[t] + -0.017 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 225/1000 --- L(Train): 0.3954909 --- L(Val, RNN): 0.3689451 --- L(Val, SINDy): 0.3722885 --- Time: 2.52s; --- Convergence: 6.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 0.972 value_reward_chosen[t] + 0.48 reward + -0.084 value_reward_chosen^2 + -0.667 value_reward_chosen*reward + 0.479 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.023 value_reward_not_chosen[t] + -0.102 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.022 1 + 1.01 value_reward_not_displayed[t] + -0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 226/1000 --- L(Train): 0.3825510 --- L(Val, RNN): 0.3688080 --- L(Val, SINDy): 0.3744576 --- Time: 2.60s; --- Convergence: 3.91e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.969 value_reward_chosen[t] + 0.487 reward + -0.082 value_reward_chosen^2 + -0.663 value_reward_chosen*reward + 0.486 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.777 1 + 0.017 value_reward_not_chosen[t] + -0.095 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.013 1 + 1.009 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 227/1000 --- L(Train): 0.4055158 --- L(Val, RNN): 0.3686388 --- L(Val, SINDy): 0.3708173 --- Time: 2.60s; --- Convergence: 2.80e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.965 value_reward_chosen[t] + 0.493 reward + -0.08 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.492 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.773 1 + 0.012 value_reward_not_chosen[t] + -0.089 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.008 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 228/1000 --- L(Train): 0.3830055 --- L(Val, RNN): 0.3690668 --- L(Val, SINDy): 0.3717582 --- Time: 2.65s; --- Convergence: 3.54e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.963 value_reward_chosen[t] + 0.497 reward + -0.081 value_reward_chosen^2 + -0.658 value_reward_chosen*reward + 0.496 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + 0.005 value_reward_not_chosen[t] + -0.081 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.004 value_reward_not_displayed[t] + 0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 229/1000 --- L(Train): 0.3775379 --- L(Val, RNN): 0.3692470 --- L(Val, SINDy): 0.3705322 --- Time: 2.53s; --- Convergence: 2.67e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.96 value_reward_chosen[t] + 0.5 reward + -0.082 value_reward_chosen^2 + -0.657 value_reward_chosen*reward + 0.499 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.761 1 + -0.001 value_reward_not_chosen[t] + -0.074 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.007 1 + 0.999 value_reward_not_displayed[t] + 0.016 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 230/1000 --- L(Train): 0.3821047 --- L(Val, RNN): 0.3687215 --- L(Val, SINDy): 0.3702326 --- Time: 2.65s; --- Convergence: 3.96e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.955 value_reward_chosen[t] + 0.497 reward + -0.087 value_reward_chosen^2 + -0.661 value_reward_chosen*reward + 0.497 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.761 1 + -0.001 value_reward_not_chosen[t] + -0.072 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.991 value_reward_not_displayed[t] + 0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 231/1000 --- L(Train): 0.3868526 --- L(Val, RNN): 0.3692057 --- L(Val, SINDy): 0.3701178 --- Time: 2.54s; --- Convergence: 4.40e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.044 1 + 0.956 value_reward_chosen[t] + 0.494 reward + -0.094 value_reward_chosen^2 + -0.665 value_reward_chosen*reward + 0.493 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + 0.001 value_reward_not_chosen[t] + -0.073 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.984 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 232/1000 --- L(Train): 0.3986973 --- L(Val, RNN): 0.3685498 --- L(Val, SINDy): 0.3702012 --- Time: 2.58s; --- Convergence: 5.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.961 value_reward_chosen[t] + 0.493 reward + -0.1 value_reward_chosen^2 + -0.666 value_reward_chosen*reward + 0.493 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + 0.004 value_reward_not_chosen[t] + -0.075 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.01 1 + 0.98 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 233/1000 --- L(Train): 0.3766948 --- L(Val, RNN): 0.3688723 --- L(Val, SINDy): 0.3702785 --- Time: 2.59s; --- Convergence: 4.35e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.973 value_reward_chosen[t] + 0.501 reward + -0.1 value_reward_chosen^2 + -0.657 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.008 value_reward_not_chosen[t] + -0.079 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.986 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 234/1000 --- L(Train): 0.3991164 --- L(Val, RNN): 0.3694582 --- L(Val, SINDy): 0.3701879 --- Time: 2.64s; --- Convergence: 5.11e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.978 value_reward_chosen[t] + 0.508 reward + -0.099 value_reward_chosen^2 + -0.65 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.775 1 + 0.012 value_reward_not_chosen[t] + -0.082 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.993 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 235/1000 --- L(Train): 0.3886006 --- L(Val, RNN): 0.3689172 --- L(Val, SINDy): 0.3703219 --- Time: 2.66s; --- Convergence: 5.26e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.046 1 + 0.983 value_reward_chosen[t] + 0.514 reward + -0.097 value_reward_chosen^2 + -0.643 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.015 value_reward_not_chosen[t] + -0.084 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.0 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 236/1000 --- L(Train): 0.4034901 --- L(Val, RNN): 0.3687631 --- L(Val, SINDy): 0.3706058 --- Time: 2.55s; --- Convergence: 3.40e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.988 value_reward_chosen[t] + 0.52 reward + -0.096 value_reward_chosen^2 + -0.637 value_reward_chosen*reward + 0.519 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.018 value_reward_not_chosen[t] + -0.086 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.006 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 237/1000 --- L(Train): 0.3927637 --- L(Val, RNN): 0.3696880 --- L(Val, SINDy): 0.3707868 --- Time: 2.65s; --- Convergence: 6.32e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.982 value_reward_chosen[t] + 0.517 reward + -0.099 value_reward_chosen^2 + -0.639 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.782 1 + 0.017 value_reward_not_chosen[t] + -0.084 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.008 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 238/1000 --- L(Train): 0.3906727 --- L(Val, RNN): 0.3691610 --- L(Val, SINDy): 0.3708144 --- Time: 2.62s; --- Convergence: 5.80e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.967 value_reward_chosen[t] + 0.507 reward + -0.106 value_reward_chosen^2 + -0.649 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.009 value_reward_not_chosen[t] + -0.075 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.003 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 239/1000 --- L(Train): 0.3915543 --- L(Val, RNN): 0.3693452 --- L(Val, SINDy): 0.3708805 --- Time: 2.56s; --- Convergence: 3.82e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.047 1 + 0.953 value_reward_chosen[t] + 0.498 reward + -0.113 value_reward_chosen^2 + -0.658 value_reward_chosen*reward + 0.497 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.002 value_reward_not_chosen[t] + -0.067 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.998 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 240/1000 --- L(Train): 0.3920438 --- L(Val, RNN): 0.3691948 --- L(Val, SINDy): 0.3708354 --- Time: 2.55s; --- Convergence: 2.66e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.046 1 + 0.942 value_reward_chosen[t] + 0.489 reward + -0.119 value_reward_chosen^2 + -0.666 value_reward_chosen*reward + 0.488 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.762 1 + -0.004 value_reward_not_chosen[t] + -0.06 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.993 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 241/1000 --- L(Train): 0.4105766 --- L(Val, RNN): 0.3689838 --- L(Val, SINDy): 0.3707862 --- Time: 2.52s; --- Convergence: 2.39e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.046 1 + 0.931 value_reward_chosen[t] + 0.482 reward + -0.124 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.481 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.756 1 + -0.009 value_reward_not_chosen[t] + -0.054 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.989 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 242/1000 --- L(Train): 0.4203606 --- L(Val, RNN): 0.3694531 --- L(Val, SINDy): 0.3708266 --- Time: 2.62s; --- Convergence: 3.54e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.929 value_reward_chosen[t] + 0.484 reward + -0.122 value_reward_chosen^2 + -0.671 value_reward_chosen*reward + 0.483 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.756 1 + -0.01 value_reward_not_chosen[t] + -0.052 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.986 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 243/1000 --- L(Train): 0.3969105 --- L(Val, RNN): 0.3694277 --- L(Val, SINDy): 0.3708495 --- Time: 2.73s; --- Convergence: 1.90e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.06 1 + 0.939 value_reward_chosen[t] + 0.497 reward + -0.111 value_reward_chosen^2 + -0.66 value_reward_chosen*reward + 0.496 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.762 1 + -0.004 value_reward_not_chosen[t] + -0.057 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.985 value_reward_not_displayed[t] + -0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 244/1000 --- L(Train): 0.4218935 --- L(Val, RNN): 0.3701586 --- L(Val, SINDy): 0.3708073 --- Time: 2.57s; --- Convergence: 4.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.069 1 + 0.948 value_reward_chosen[t] + 0.508 reward + -0.102 value_reward_chosen^2 + -0.65 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.001 value_reward_not_chosen[t] + -0.062 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.984 value_reward_not_displayed[t] + -0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 245/1000 --- L(Train): 0.3969034 --- L(Val, RNN): 0.3704753 --- L(Val, SINDy): 0.3748609 --- Time: 2.78s; --- Convergence: 3.89e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.077 1 + 0.956 value_reward_chosen[t] + 0.518 reward + -0.093 value_reward_chosen^2 + -0.641 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.773 1 + 0.006 value_reward_not_chosen[t] + -0.066 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.983 value_reward_not_displayed[t] + -0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 246/1000 --- L(Train): 0.4111568 --- L(Val, RNN): 0.3685454 --- L(Val, SINDy): 0.3750384 --- Time: 2.55s; --- Convergence: 1.16e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.072 1 + 0.965 value_reward_chosen[t] + 0.523 reward + -0.095 value_reward_chosen^2 + -0.636 value_reward_chosen*reward + 0.522 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.008 value_reward_not_chosen[t] + -0.067 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.983 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 247/1000 --- L(Train): 0.4331789 --- L(Val, RNN): 0.3724017 --- L(Val, SINDy): 0.3712358 --- Time: 2.56s; --- Convergence: 2.51e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.059 1 + 0.975 value_reward_chosen[t] + 0.522 reward + -0.104 value_reward_chosen^2 + -0.634 value_reward_chosen*reward + 0.521 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.773 1 + 0.006 value_reward_not_chosen[t] + -0.065 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.985 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 248/1000 --- L(Train): 0.4214341 --- L(Val, RNN): 0.3705742 --- L(Val, SINDy): 0.3714119 --- Time: 2.54s; --- Convergence: 2.17e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.983 value_reward_chosen[t] + 0.521 reward + -0.112 value_reward_chosen^2 + -0.633 value_reward_chosen*reward + 0.52 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.005 value_reward_not_chosen[t] + -0.063 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.986 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 249/1000 --- L(Train): 0.4101882 --- L(Val, RNN): 0.3683176 --- L(Val, SINDy): 0.3763093 --- Time: 2.58s; --- Convergence: 2.21e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 0.986 value_reward_chosen[t] + 0.518 reward + -0.121 value_reward_chosen^2 + -0.636 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.002 value_reward_not_chosen[t] + -0.058 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.988 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\n",
      ">>> Warmup complete (epoch 250). Reset optimizer state for 3 SINDy parameters (fresh start at full regularization strength).\n",
      "\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 250/1000 --- L(Train): 0.4367967 --- L(Val, RNN): 0.3706944 --- L(Val, SINDy): 0.3761368 --- Time: 2.57s; --- Convergence: 2.29e-03; LR: 1.00e-02; Metric: 0.3706944; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.027 1 + 0.988 value_reward_chosen[t] + 0.515 reward + -0.13 value_reward_chosen^2 + -0.639 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.764 1 + -0.003 value_reward_not_chosen[t] + -0.053 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.99 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 1, 1, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 1, 1, 1\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 251/1000 --- L(Train): 0.4137069 --- L(Val, RNN): 0.3714059 --- L(Val, SINDy): 0.3715188 --- Time: 2.58s; --- Convergence: 1.50e-03; LR: 1.00e-02; Metric: 0.3706944; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.027 1 + 0.988 value_reward_chosen[t] + 0.515 reward + -0.13 value_reward_chosen^2 + -0.639 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.764 1 + -0.003 value_reward_not_chosen[t] + -0.053 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.99 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 2, 2, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 2, 2, 2\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 252/1000 --- L(Train): 0.4126880 --- L(Val, RNN): 0.3699305 --- L(Val, SINDy): 0.3712701 --- Time: 2.51s; --- Convergence: 1.49e-03; LR: 1.00e-02; Metric: 0.3699305; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 0.98 value_reward_chosen[t] + 0.523 reward + -0.122 value_reward_chosen^2 + -0.646 value_reward_chosen*reward + 0.522 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.757 1 + -0.01 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.982 value_reward_not_displayed[t] + -0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 3, 3, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 1\n",
      "value_reward_not_displayed: 3, 3, 3\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 253/1000 --- L(Train): 0.4145763 --- L(Val, RNN): 0.3711464 --- L(Val, SINDy): 0.3713934 --- Time: 2.74s; --- Convergence: 1.35e-03; LR: 1.00e-02; Metric: 0.3699305; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 0.975 value_reward_chosen[t] + 0.528 reward + -0.116 value_reward_chosen^2 + -0.652 value_reward_chosen*reward + 0.528 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.751 1 + -0.016 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.014 1 + 0.977 value_reward_not_displayed[t] + -0.013 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 4, 4, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 2\n",
      "value_reward_not_displayed: 4, 4, 4\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 254/1000 --- L(Train): 0.3942216 --- L(Val, RNN): 0.3717375 --- L(Val, SINDy): 0.3716955 --- Time: 2.54s; --- Convergence: 9.72e-04; LR: 1.00e-02; Metric: 0.3699305; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.968 value_reward_chosen[t] + 0.526 reward + -0.109 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.525 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.757 1 + -0.01 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.982 value_reward_not_displayed[t] + -0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 5, 5, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 3\n",
      "value_reward_not_displayed: 5, 5, 5\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 255/1000 --- L(Train): 0.4152120 --- L(Val, RNN): 0.3694036 --- L(Val, SINDy): 0.3717639 --- Time: 2.55s; --- Convergence: 1.65e-03; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.055 1 + 0.961 value_reward_chosen[t] + 0.523 reward + -0.102 value_reward_chosen^2 + -0.665 value_reward_chosen*reward + 0.522 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + -0.004 value_reward_not_chosen[t] + -0.051 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.987 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 6, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 6, 6, 6\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 256/1000 --- L(Train): 0.4335046 --- L(Val, RNN): 0.3698306 --- L(Val, SINDy): 0.3718302 --- Time: 2.50s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.06 1 + 0.955 value_reward_chosen[t] + 0.517 reward + -0.098 value_reward_chosen^2 + -0.67 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.002 value_reward_not_chosen[t] + -0.057 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.992 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 7, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 7, 7, 7\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 257/1000 --- L(Train): 0.4280809 --- L(Val, RNN): 0.3716227 --- L(Val, SINDy): 0.3719022 --- Time: 2.68s; --- Convergence: 1.42e-03; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.063 1 + 0.95 value_reward_chosen[t] + 0.512 reward + -0.094 value_reward_chosen^2 + -0.675 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.007 value_reward_not_chosen[t] + -0.062 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.005 1 + 0.996 value_reward_not_displayed[t] + 0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 8, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 8, 8, 8\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 258/1000 --- L(Train): 0.3962757 --- L(Val, RNN): 0.3713089 --- L(Val, SINDy): 0.3718730 --- Time: 2.60s; --- Convergence: 8.65e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.067 1 + 0.946 value_reward_chosen[t] + 0.507 reward + -0.091 value_reward_chosen^2 + -0.679 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.012 value_reward_not_chosen[t] + -0.067 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.009 1 + 1.0 value_reward_not_displayed[t] + 0.01 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 9, 9, 9\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 259/1000 --- L(Train): 0.4092786 --- L(Val, RNN): 0.3700924 --- L(Val, SINDy): 0.3717505 --- Time: 2.58s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.067 1 + 0.946 value_reward_chosen[t] + 0.507 reward + -0.092 value_reward_chosen^2 + -0.679 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.014 value_reward_not_chosen[t] + -0.068 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.007 1 + 0.999 value_reward_not_displayed[t] + 0.009 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 10, 10, 10\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 260/1000 --- L(Train): 0.4219324 --- L(Val, RNN): 0.3704888 --- L(Val, SINDy): 0.3716633 --- Time: 2.67s; --- Convergence: 7.19e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.065 1 + 0.946 value_reward_chosen[t] + 0.506 reward + -0.093 value_reward_chosen^2 + -0.678 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.781 1 + 0.014 value_reward_not_chosen[t] + -0.069 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.005 1 + 0.998 value_reward_not_displayed[t] + 0.007 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 11, 11, 11\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 261/1000 --- L(Train): 0.4116608 --- L(Val, RNN): 0.3710912 --- L(Val, SINDy): 0.3715763 --- Time: 2.66s; --- Convergence: 6.60e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.063 1 + 0.948 value_reward_chosen[t] + 0.508 reward + -0.095 value_reward_chosen^2 + -0.676 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.013 value_reward_not_chosen[t] + -0.067 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.995 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 12, 12, 12\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 262/1000 --- L(Train): 0.4192891 --- L(Val, RNN): 0.3699589 --- L(Val, SINDy): 0.3714617 --- Time: 2.59s; --- Convergence: 8.96e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.062 1 + 0.95 value_reward_chosen[t] + 0.51 reward + -0.096 value_reward_chosen^2 + -0.673 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.012 value_reward_not_chosen[t] + -0.065 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.992 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 1, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 13, 13, 13\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 263/1000 --- L(Train): 0.4010041 --- L(Val, RNN): 0.3700417 --- L(Val, SINDy): 0.3714253 --- Time: 2.68s; --- Convergence: 4.90e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 8/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.06 1 + 0.953 value_reward_chosen[t] + 0.511 reward + -0.098 value_reward_chosen^2 + -0.67 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.777 1 + 0.01 value_reward_not_chosen[t] + -0.063 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.989 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 2, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 14, 14, 14\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 264/1000 --- L(Train): 0.4176116 --- L(Val, RNN): 0.3707858 --- L(Val, SINDy): 0.3714881 --- Time: 3.12s; --- Convergence: 6.17e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 9/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.058 1 + 0.955 value_reward_chosen[t] + 0.512 reward + -0.1 value_reward_chosen^2 + -0.667 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.775 1 + 0.008 value_reward_not_chosen[t] + -0.061 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.986 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 3, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 15, 15, 15\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 265/1000 --- L(Train): 0.4085082 --- L(Val, RNN): 0.3703786 --- L(Val, SINDy): 0.3716076 --- Time: 2.73s; --- Convergence: 5.12e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 10/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.055 1 + 0.957 value_reward_chosen[t] + 0.514 reward + -0.102 value_reward_chosen^2 + -0.664 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.006 value_reward_not_chosen[t] + -0.058 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.011 1 + 0.984 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 4, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 16, 16, 16\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 266/1000 --- L(Train): 0.4079645 --- L(Val, RNN): 0.3698444 --- L(Val, SINDy): 0.3716778 --- Time: 2.64s; --- Convergence: 5.23e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 11/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.959 value_reward_chosen[t] + 0.515 reward + -0.104 value_reward_chosen^2 + -0.661 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.77 1 + 0.003 value_reward_not_chosen[t] + -0.056 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.013 1 + 0.982 value_reward_not_displayed[t] + -0.01 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 5, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 17, 17, 17\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 267/1000 --- L(Train): 0.3853129 --- L(Val, RNN): 0.3700950 --- L(Val, SINDy): 0.3716749 --- Time: 2.39s; --- Convergence: 3.87e-04; LR: 1.00e-02; Metric: 0.3694036; Bad epochs: 12/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.961 value_reward_chosen[t] + 0.517 reward + -0.106 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.001 value_reward_not_chosen[t] + -0.053 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.015 1 + 0.98 value_reward_not_displayed[t] + -0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 6, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 18, 18, 18\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 268/1000 --- L(Train): 0.4163417 --- L(Val, RNN): 0.3693813 --- L(Val, SINDy): 0.3715616 --- Time: 3.04s; --- Convergence: 5.50e-04; LR: 1.00e-02; Metric: 0.3693813; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.963 value_reward_chosen[t] + 0.519 reward + -0.106 value_reward_chosen^2 + -0.657 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + -0.0 value_reward_not_chosen[t] + -0.051 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.015 1 + 0.979 value_reward_not_displayed[t] + -0.012 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 7, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 19, 19, 19\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 269/1000 --- L(Train): 0.4012375 --- L(Val, RNN): 0.3692106 --- L(Val, SINDy): 0.3713708 --- Time: 2.58s; --- Convergence: 3.60e-04; LR: 1.00e-02; Metric: 0.3692106; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.963 value_reward_chosen[t] + 0.522 reward + -0.106 value_reward_chosen^2 + -0.656 value_reward_chosen*reward + 0.521 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.765 1 + -0.002 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.015 1 + 0.979 value_reward_not_displayed[t] + -0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 8, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 1\n",
      "value_reward_not_displayed: 20, 20, 20\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 270/1000 --- L(Train): 0.3892913 --- L(Val, RNN): 0.3700348 --- L(Val, SINDy): 0.3711934 --- Time: 2.42s; --- Convergence: 5.92e-04; LR: 1.00e-02; Metric: 0.3692106; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.963 value_reward_chosen[t] + 0.524 reward + -0.106 value_reward_chosen^2 + -0.655 value_reward_chosen*reward + 0.523 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.764 1 + -0.003 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.015 1 + 0.98 value_reward_not_displayed[t] + -0.011 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 9, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 2\n",
      "value_reward_not_displayed: 21, 21, 21\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 271/1000 --- L(Train): 0.3934346 --- L(Val, RNN): 0.3709499 --- L(Val, SINDy): 0.3710842 --- Time: 2.67s; --- Convergence: 7.54e-04; LR: 1.00e-02; Metric: 0.3692106; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.963 value_reward_chosen[t] + 0.522 reward + -0.106 value_reward_chosen^2 + -0.655 value_reward_chosen*reward + 0.521 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + -0.003 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.013 1 + 0.982 value_reward_not_displayed[t] + -0.008 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 10, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 3\n",
      "value_reward_not_displayed: 22, 22, 22\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 272/1000 --- L(Train): 0.3893251 --- L(Val, RNN): 0.3694371 --- L(Val, SINDy): 0.3710024 --- Time: 2.45s; --- Convergence: 1.13e-03; LR: 1.00e-02; Metric: 0.3692106; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.963 value_reward_chosen[t] + 0.52 reward + -0.107 value_reward_chosen^2 + -0.655 value_reward_chosen*reward + 0.52 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + -0.004 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.011 1 + 0.985 value_reward_not_displayed[t] + -0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 1, 11, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 4\n",
      "value_reward_not_displayed: 23, 23, 23\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 273/1000 --- L(Train): 0.3997396 --- L(Val, RNN): 0.3686578 --- L(Val, SINDy): 0.3709756 --- Time: 2.36s; --- Convergence: 9.56e-04; LR: 1.00e-02; Metric: 0.3686578; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.962 value_reward_chosen[t] + 0.519 reward + -0.107 value_reward_chosen^2 + -0.656 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.762 1 + -0.004 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.987 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 2, 12, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 5\n",
      "value_reward_not_displayed: 24, 24, 24\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 274/1000 --- L(Train): 0.3949901 --- L(Val, RNN): 0.3692051 --- L(Val, SINDy): 0.3709290 --- Time: 2.56s; --- Convergence: 7.52e-04; LR: 1.00e-02; Metric: 0.3686578; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.962 value_reward_chosen[t] + 0.517 reward + -0.107 value_reward_chosen^2 + -0.656 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.762 1 + -0.004 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.989 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 3, 13, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 6\n",
      "value_reward_not_displayed: 25, 25, 25\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 275/1000 --- L(Train): 0.3846147 --- L(Val, RNN): 0.3692564 --- L(Val, SINDy): 0.3708136 --- Time: 2.61s; --- Convergence: 4.02e-04; LR: 1.00e-02; Metric: 0.3686578; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.961 value_reward_chosen[t] + 0.516 reward + -0.107 value_reward_chosen^2 + -0.657 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + -0.003 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.992 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 14, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 7\n",
      "value_reward_not_displayed: 26, 26, 26\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 276/1000 --- L(Train): 0.4054117 --- L(Val, RNN): 0.3690909 --- L(Val, SINDy): 0.3706419 --- Time: 2.64s; --- Convergence: 2.84e-04; LR: 1.00e-02; Metric: 0.3686578; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.96 value_reward_chosen[t] + 0.515 reward + -0.106 value_reward_chosen^2 + -0.657 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + -0.003 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.994 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 15, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 8\n",
      "value_reward_not_displayed: 27, 27, 27\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 277/1000 --- L(Train): 0.3775748 --- L(Val, RNN): 0.3694586 --- L(Val, SINDy): 0.3704912 --- Time: 2.49s; --- Convergence: 3.26e-04; LR: 1.00e-02; Metric: 0.3686578; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.959 value_reward_chosen[t] + 0.516 reward + -0.105 value_reward_chosen^2 + -0.658 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.764 1 + -0.002 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.995 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 16, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 9\n",
      "value_reward_not_displayed: 28, 28, 28\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 278/1000 --- L(Train): 0.3965914 --- L(Val, RNN): 0.3688096 --- L(Val, SINDy): 0.3703252 --- Time: 2.44s; --- Convergence: 4.87e-04; LR: 1.00e-02; Metric: 0.3686578; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.958 value_reward_chosen[t] + 0.517 reward + -0.103 value_reward_chosen^2 + -0.658 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.765 1 + -0.001 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.997 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 17, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 10\n",
      "value_reward_not_displayed: 29, 29, 29\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 279/1000 --- L(Train): 0.3930797 --- L(Val, RNN): 0.3682576 --- L(Val, SINDy): 0.3701657 --- Time: 2.79s; --- Convergence: 5.20e-04; LR: 1.00e-02; Metric: 0.3682576; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.055 1 + 0.957 value_reward_chosen[t] + 0.519 reward + -0.101 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + 0.0 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.997 value_reward_not_displayed[t] + 0.006 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 18, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 11\n",
      "value_reward_not_displayed: 30, 30, 30\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 280/1000 --- L(Train): 0.3722890 --- L(Val, RNN): 0.3681509 --- L(Val, SINDy): 0.3699759 --- Time: 2.70s; --- Convergence: 3.13e-04; LR: 1.00e-02; Metric: 0.3681509; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.056 1 + 0.956 value_reward_chosen[t] + 0.52 reward + -0.1 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.519 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.767 1 + 0.001 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.998 value_reward_not_displayed[t] + 0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 19, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 12\n",
      "value_reward_not_displayed: 31, 31, 31\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 281/1000 --- L(Train): 0.4046057 --- L(Val, RNN): 0.3685926 --- L(Val, SINDy): 0.3697974 --- Time: 2.41s; --- Convergence: 3.77e-04; LR: 1.00e-02; Metric: 0.3681509; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.056 1 + 0.956 value_reward_chosen[t] + 0.519 reward + -0.1 value_reward_chosen^2 + -0.66 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.767 1 + 0.002 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.997 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 20, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 13\n",
      "value_reward_not_displayed: 32, 32, 32\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 282/1000 --- L(Train): 0.3722420 --- L(Val, RNN): 0.3684515 --- L(Val, SINDy): 0.3695967 --- Time: 2.49s; --- Convergence: 2.59e-04; LR: 1.00e-02; Metric: 0.3681509; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.055 1 + 0.956 value_reward_chosen[t] + 0.518 reward + -0.1 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.003 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.996 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 21, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 14\n",
      "value_reward_not_displayed: 33, 33, 33\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 283/1000 --- L(Train): 0.3792764 --- L(Val, RNN): 0.3682306 --- L(Val, SINDy): 0.3694181 --- Time: 2.65s; --- Convergence: 2.40e-04; LR: 1.00e-02; Metric: 0.3681509; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.957 value_reward_chosen[t] + 0.516 reward + -0.102 value_reward_chosen^2 + -0.659 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.003 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.995 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 22, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 15\n",
      "value_reward_not_displayed: 34, 34, 34\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 284/1000 --- L(Train): 0.3663760 --- L(Val, RNN): 0.3677817 --- L(Val, SINDy): 0.3692449 --- Time: 2.46s; --- Convergence: 3.45e-04; LR: 1.00e-02; Metric: 0.3677817; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.958 value_reward_chosen[t] + 0.515 reward + -0.103 value_reward_chosen^2 + -0.657 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.003 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.994 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 23, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 16\n",
      "value_reward_not_displayed: 35, 35, 35\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 285/1000 --- L(Train): 0.3957314 --- L(Val, RNN): 0.3673449 --- L(Val, SINDy): 0.3690737 --- Time: 2.42s; --- Convergence: 3.91e-04; LR: 1.00e-02; Metric: 0.3673449; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.958 value_reward_chosen[t] + 0.514 reward + -0.104 value_reward_chosen^2 + -0.656 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.003 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.992 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 24, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 17\n",
      "value_reward_not_displayed: 36, 36, 36\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 286/1000 --- L(Train): 0.3756877 --- L(Val, RNN): 0.3672991 --- L(Val, SINDy): 0.3688509 --- Time: 2.37s; --- Convergence: 2.18e-04; LR: 1.00e-02; Metric: 0.3672991; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.959 value_reward_chosen[t] + 0.515 reward + -0.104 value_reward_chosen^2 + -0.654 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.004 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.994 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 25, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 18\n",
      "value_reward_not_displayed: 37, 37, 37\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 287/1000 --- L(Train): 0.3916595 --- L(Val, RNN): 0.3677340 --- L(Val, SINDy): 0.3686523 --- Time: 2.48s; --- Convergence: 3.27e-04; LR: 1.00e-02; Metric: 0.3672991; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.959 value_reward_chosen[t] + 0.517 reward + -0.102 value_reward_chosen^2 + -0.65 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.004 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.995 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 26, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 19\n",
      "value_reward_not_displayed: 38, 38, 38\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 288/1000 --- L(Train): 0.3874808 --- L(Val, RNN): 0.3677933 --- L(Val, SINDy): 0.3685275 --- Time: 2.70s; --- Convergence: 1.93e-04; LR: 1.00e-02; Metric: 0.3672991; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.959 value_reward_chosen[t] + 0.519 reward + -0.1 value_reward_chosen^2 + -0.647 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.005 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.997 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 27, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 20\n",
      "value_reward_not_displayed: 39, 39, 39\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 289/1000 --- L(Train): 0.3680825 --- L(Val, RNN): 0.3671692 --- L(Val, SINDy): 0.3684480 --- Time: 2.55s; --- Convergence: 4.09e-04; LR: 1.00e-02; Metric: 0.3671692; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.96 value_reward_chosen[t] + 0.52 reward + -0.099 value_reward_chosen^2 + -0.645 value_reward_chosen*reward + 0.52 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.77 1 + 0.005 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 28, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 21\n",
      "value_reward_not_displayed: 40, 40, 40\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 290/1000 --- L(Train): 0.3830804 --- L(Val, RNN): 0.3664747 --- L(Val, SINDy): 0.3683531 --- Time: 2.81s; --- Convergence: 5.51e-04; LR: 1.00e-02; Metric: 0.3664747; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.96 value_reward_chosen[t] + 0.521 reward + -0.099 value_reward_chosen^2 + -0.642 value_reward_chosen*reward + 0.52 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.005 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 29, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 22\n",
      "value_reward_not_displayed: 41, 41, 41\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 291/1000 --- L(Train): 0.4023886 --- L(Val, RNN): 0.3670070 --- L(Val, SINDy): 0.3682197 --- Time: 3.02s; --- Convergence: 5.42e-04; LR: 1.00e-02; Metric: 0.3664747; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.961 value_reward_chosen[t] + 0.521 reward + -0.098 value_reward_chosen^2 + -0.64 value_reward_chosen*reward + 0.521 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.005 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.0 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 30, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 23\n",
      "value_reward_not_displayed: 42, 42, 42\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 292/1000 --- L(Train): 0.3678741 --- L(Val, RNN): 0.3668776 --- L(Val, SINDy): 0.3680326 --- Time: 2.48s; --- Convergence: 3.36e-04; LR: 1.00e-02; Metric: 0.3664747; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.961 value_reward_chosen[t] + 0.52 reward + -0.098 value_reward_chosen^2 + -0.638 value_reward_chosen*reward + 0.519 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.005 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.0 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 31, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 24\n",
      "value_reward_not_displayed: 43, 43, 43\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 293/1000 --- L(Train): 0.3823584 --- L(Val, RNN): 0.3659775 --- L(Val, SINDy): 0.3678118 --- Time: 2.56s; --- Convergence: 6.18e-04; LR: 1.00e-02; Metric: 0.3659775; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.961 value_reward_chosen[t] + 0.516 reward + -0.099 value_reward_chosen^2 + -0.637 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.006 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.997 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 32, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 25\n",
      "value_reward_not_displayed: 44, 44, 44\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 294/1000 --- L(Train): 0.3678541 --- L(Val, RNN): 0.3663257 --- L(Val, SINDy): 0.3676022 --- Time: 2.66s; --- Convergence: 4.83e-04; LR: 1.00e-02; Metric: 0.3659775; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.96 value_reward_chosen[t] + 0.513 reward + -0.099 value_reward_chosen^2 + -0.636 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.008 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.995 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 1, 33, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 26\n",
      "value_reward_not_displayed: 45, 45, 45\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 295/1000 --- L(Train): 0.3905129 --- L(Val, RNN): 0.3669357 --- L(Val, SINDy): 0.3674370 --- Time: 2.65s; --- Convergence: 5.47e-04; LR: 1.00e-02; Metric: 0.3659775; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.96 value_reward_chosen[t] + 0.509 reward + -0.1 value_reward_chosen^2 + -0.634 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.773 1 + 0.008 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.993 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 2, 34, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 27\n",
      "value_reward_not_displayed: 46, 46, 46\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 296/1000 --- L(Train): 0.3762883 --- L(Val, RNN): 0.3660100 --- L(Val, SINDy): 0.3673047 --- Time: 2.51s; --- Convergence: 7.36e-04; LR: 1.00e-02; Metric: 0.3659775; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.96 value_reward_chosen[t] + 0.509 reward + -0.099 value_reward_chosen^2 + -0.633 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.773 1 + 0.008 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.992 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 3, 35, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 28\n",
      "value_reward_not_displayed: 47, 47, 47\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 297/1000 --- L(Train): 0.3749270 --- L(Val, RNN): 0.3655789 --- L(Val, SINDy): 0.3672418 --- Time: 2.60s; --- Convergence: 5.84e-04; LR: 1.00e-02; Metric: 0.3655789; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.96 value_reward_chosen[t] + 0.509 reward + -0.098 value_reward_chosen^2 + -0.63 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.008 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.992 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 4, 36, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 29\n",
      "value_reward_not_displayed: 48, 48, 48\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 298/1000 --- L(Train): 0.3792421 --- L(Val, RNN): 0.3658432 --- L(Val, SINDy): 0.3671520 --- Time: 2.64s; --- Convergence: 4.24e-04; LR: 1.00e-02; Metric: 0.3655789; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.96 value_reward_chosen[t] + 0.509 reward + -0.097 value_reward_chosen^2 + -0.628 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.007 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.992 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 5, 37, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 30\n",
      "value_reward_not_displayed: 49, 49, 49\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 299/1000 --- L(Train): 0.3660085 --- L(Val, RNN): 0.3653633 --- L(Val, SINDy): 0.3670010 --- Time: 2.67s; --- Convergence: 4.52e-04; LR: 1.00e-02; Metric: 0.3653633; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.961 value_reward_chosen[t] + 0.509 reward + -0.096 value_reward_chosen^2 + -0.625 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.007 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.992 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 6, 38, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 31\n",
      "value_reward_not_displayed: 50, 50, 50\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 300/1000 --- L(Train): 0.3732020 --- L(Val, RNN): 0.3648447 --- L(Val, SINDy): 0.3668191 --- Time: 2.52s; --- Convergence: 4.85e-04; LR: 1.00e-02; Metric: 0.3648447; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.961 value_reward_chosen[t] + 0.512 reward + -0.093 value_reward_chosen^2 + -0.622 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.007 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.994 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 39, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 32\n",
      "value_reward_not_displayed: 51, 51, 51\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 301/1000 --- L(Train): 0.3550228 --- L(Val, RNN): 0.3652666 --- L(Val, SINDy): 0.3666718 --- Time: 2.62s; --- Convergence: 4.54e-04; LR: 1.00e-02; Metric: 0.3648447; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.961 value_reward_chosen[t] + 0.515 reward + -0.09 value_reward_chosen^2 + -0.619 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.006 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.996 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 40, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 33\n",
      "value_reward_not_displayed: 52, 52, 52\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 302/1000 --- L(Train): 0.3695070 --- L(Val, RNN): 0.3653730 --- L(Val, SINDy): 0.3664736 --- Time: 2.65s; --- Convergence: 2.80e-04; LR: 1.00e-02; Metric: 0.3648447; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.962 value_reward_chosen[t] + 0.516 reward + -0.089 value_reward_chosen^2 + -0.616 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.006 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 41, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 34\n",
      "value_reward_not_displayed: 53, 53, 53\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 303/1000 --- L(Train): 0.3706034 --- L(Val, RNN): 0.3647547 --- L(Val, SINDy): 0.3662862 --- Time: 2.63s; --- Convergence: 4.49e-04; LR: 1.00e-02; Metric: 0.3647547; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.054 1 + 0.963 value_reward_chosen[t] + 0.517 reward + -0.087 value_reward_chosen^2 + -0.613 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.006 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.001 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 42, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 35\n",
      "value_reward_not_displayed: 54, 54, 54\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 304/1000 --- L(Train): 0.3717789 --- L(Val, RNN): 0.3645571 --- L(Val, SINDy): 0.3660601 --- Time: 2.54s; --- Convergence: 3.23e-04; LR: 1.00e-02; Metric: 0.3645571; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.965 value_reward_chosen[t] + 0.514 reward + -0.089 value_reward_chosen^2 + -0.611 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.006 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 43, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 36\n",
      "value_reward_not_displayed: 55, 55, 55\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 305/1000 --- L(Train): 0.3938281 --- L(Val, RNN): 0.3645267 --- L(Val, SINDy): 0.3659193 --- Time: 2.77s; --- Convergence: 1.77e-04; LR: 1.00e-02; Metric: 0.3645267; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.966 value_reward_chosen[t] + 0.512 reward + -0.091 value_reward_chosen^2 + -0.61 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.006 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 1, 44, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 37\n",
      "value_reward_not_displayed: 56, 56, 56\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 306/1000 --- L(Train): 0.3729231 --- L(Val, RNN): 0.3640329 --- L(Val, SINDy): 0.3658023 --- Time: 2.95s; --- Convergence: 3.35e-04; LR: 1.00e-02; Metric: 0.3640329; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.967 value_reward_chosen[t] + 0.51 reward + -0.092 value_reward_chosen^2 + -0.609 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.007 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 2, 45, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 38\n",
      "value_reward_not_displayed: 57, 57, 57\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 307/1000 --- L(Train): 0.3811487 --- L(Val, RNN): 0.3639919 --- L(Val, SINDy): 0.3656904 --- Time: 2.60s; --- Convergence: 1.88e-04; LR: 1.00e-02; Metric: 0.3639919; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.966 value_reward_chosen[t] + 0.51 reward + -0.089 value_reward_chosen^2 + -0.609 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.008 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.998 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 46, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 39\n",
      "value_reward_not_displayed: 58, 58, 58\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 308/1000 --- L(Train): 0.3672992 --- L(Val, RNN): 0.3642253 --- L(Val, SINDy): 0.3655261 --- Time: 2.55s; --- Convergence: 2.11e-04; LR: 1.00e-02; Metric: 0.3639919; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.053 1 + 0.965 value_reward_chosen[t] + 0.51 reward + -0.087 value_reward_chosen^2 + -0.609 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.775 1 + 0.009 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.995 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 47, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 40\n",
      "value_reward_not_displayed: 59, 59, 59\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 309/1000 --- L(Train): 0.3819569 --- L(Val, RNN): 0.3638862 --- L(Val, SINDy): 0.3652178 --- Time: 2.58s; --- Convergence: 2.75e-04; LR: 1.00e-02; Metric: 0.3638862; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.054 1 + 0.964 value_reward_chosen[t] + 0.51 reward + -0.086 value_reward_chosen^2 + -0.61 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.776 1 + 0.01 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.994 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 48, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 41\n",
      "value_reward_not_displayed: 60, 60, 60\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 310/1000 --- L(Train): 0.3870016 --- L(Val, RNN): 0.3636181 --- L(Val, SINDy): 0.3650189 --- Time: 2.70s; --- Convergence: 2.72e-04; LR: 1.00e-02; Metric: 0.3636181; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.055 1 + 0.963 value_reward_chosen[t] + 0.509 reward + -0.085 value_reward_chosen^2 + -0.61 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.777 1 + 0.011 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.992 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 49, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 42\n",
      "value_reward_not_displayed: 61, 61, 61\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 311/1000 --- L(Train): 0.3693633 --- L(Val, RNN): 0.3638366 --- L(Val, SINDy): 0.3649628 --- Time: 2.63s; --- Convergence: 2.45e-04; LR: 1.00e-02; Metric: 0.3636181; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.056 1 + 0.962 value_reward_chosen[t] + 0.509 reward + -0.084 value_reward_chosen^2 + -0.611 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.778 1 + 0.011 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.991 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 50, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 43\n",
      "value_reward_not_displayed: 62, 62, 62\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 312/1000 --- L(Train): 0.3682622 --- L(Val, RNN): 0.3633870 --- L(Val, SINDy): 0.3649449 --- Time: 2.65s; --- Convergence: 3.47e-04; LR: 1.00e-02; Metric: 0.3633870; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.056 1 + 0.962 value_reward_chosen[t] + 0.509 reward + -0.083 value_reward_chosen^2 + -0.612 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.012 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.99 value_reward_not_displayed[t] + -0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 51, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 44\n",
      "value_reward_not_displayed: 63, 63, 63\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 313/1000 --- L(Train): 0.3756999 --- L(Val, RNN): 0.3627527 --- L(Val, SINDy): 0.3648764 --- Time: 2.91s; --- Convergence: 4.91e-04; LR: 1.00e-02; Metric: 0.3627527; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.057 1 + 0.961 value_reward_chosen[t] + 0.508 reward + -0.082 value_reward_chosen^2 + -0.613 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.012 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.989 value_reward_not_displayed[t] + -0.005 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 52, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 45\n",
      "value_reward_not_displayed: 64, 64, 64\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 314/1000 --- L(Train): 0.3849818 --- L(Val, RNN): 0.3635180 --- L(Val, SINDy): 0.3647560 --- Time: 2.88s; --- Convergence: 6.28e-04; LR: 1.00e-02; Metric: 0.3627527; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.054 1 + 0.963 value_reward_chosen[t] + 0.506 reward + -0.086 value_reward_chosen^2 + -0.613 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.011 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.991 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 53, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 46\n",
      "value_reward_not_displayed: 65, 65, 65\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 315/1000 --- L(Train): 0.3690986 --- L(Val, RNN): 0.3631821 --- L(Val, SINDy): 0.3646413 --- Time: 2.68s; --- Convergence: 4.82e-04; LR: 1.00e-02; Metric: 0.3627527; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.965 value_reward_chosen[t] + 0.505 reward + -0.088 value_reward_chosen^2 + -0.612 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.778 1 + 0.009 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.994 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 54, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 47\n",
      "value_reward_not_displayed: 66, 66, 66\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 316/1000 --- L(Train): 0.3752672 --- L(Val, RNN): 0.3624810 --- L(Val, SINDy): 0.3645291 --- Time: 2.70s; --- Convergence: 5.92e-04; LR: 1.00e-02; Metric: 0.3624810; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.05 1 + 0.967 value_reward_chosen[t] + 0.507 reward + -0.089 value_reward_chosen^2 + -0.61 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.776 1 + 0.008 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 0.997 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 1, 55, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 48\n",
      "value_reward_not_displayed: 67, 67, 67\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 317/1000 --- L(Train): 0.3696325 --- L(Val, RNN): 0.3628053 --- L(Val, SINDy): 0.3644496 --- Time: 2.61s; --- Convergence: 4.58e-04; LR: 1.00e-02; Metric: 0.3624810; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.97 value_reward_chosen[t] + 0.509 reward + -0.091 value_reward_chosen^2 + -0.607 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.775 1 + 0.006 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 0.999 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 2, 56, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 49\n",
      "value_reward_not_displayed: 68, 68, 68\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 318/1000 --- L(Train): 0.3696594 --- L(Val, RNN): 0.3629413 --- L(Val, SINDy): 0.3643146 --- Time: 2.57s; --- Convergence: 2.97e-04; LR: 1.00e-02; Metric: 0.3624810; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.046 1 + 0.973 value_reward_chosen[t] + 0.51 reward + -0.092 value_reward_chosen^2 + -0.604 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.005 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 3, 57, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 50\n",
      "value_reward_not_displayed: 69, 69, 69\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 319/1000 --- L(Train): 0.3708536 --- L(Val, RNN): 0.3623383 --- L(Val, SINDy): 0.3641434 --- Time: 2.57s; --- Convergence: 4.50e-04; LR: 1.00e-02; Metric: 0.3623383; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.044 1 + 0.976 value_reward_chosen[t] + 0.512 reward + -0.094 value_reward_chosen^2 + -0.6 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.004 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.002 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 4, 58, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 51\n",
      "value_reward_not_displayed: 70, 70, 70\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 320/1000 --- L(Train): 0.3850043 --- L(Val, RNN): 0.3620956 --- L(Val, SINDy): 0.3639599 --- Time: 2.58s; --- Convergence: 3.46e-04; LR: 1.00e-02; Metric: 0.3620956; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 0.98 value_reward_chosen[t] + 0.513 reward + -0.096 value_reward_chosen^2 + -0.596 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.004 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 5, 59, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 52\n",
      "value_reward_not_displayed: 71, 71, 71\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 321/1000 --- L(Train): 0.3732240 --- L(Val, RNN): 0.3626031 --- L(Val, SINDy): 0.3637581 --- Time: 2.61s; --- Convergence: 4.27e-04; LR: 1.00e-02; Metric: 0.3620956; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 0.983 value_reward_chosen[t] + 0.514 reward + -0.097 value_reward_chosen^2 + -0.593 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.003 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.004 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 6, 60, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 53\n",
      "value_reward_not_displayed: 72, 72, 72\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 322/1000 --- L(Train): 0.3734958 --- L(Val, RNN): 0.3617853 --- L(Val, SINDy): 0.3635409 --- Time: 2.77s; --- Convergence: 6.22e-04; LR: 1.00e-02; Metric: 0.3617853; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.985 value_reward_chosen[t] + 0.515 reward + -0.098 value_reward_chosen^2 + -0.589 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.775 1 + 0.004 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 7, 61, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 54\n",
      "value_reward_not_displayed: 73, 73, 73\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 323/1000 --- L(Train): 0.3774856 --- L(Val, RNN): 0.3619030 --- L(Val, SINDy): 0.3633685 --- Time: 2.52s; --- Convergence: 3.70e-04; LR: 1.00e-02; Metric: 0.3617853; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 0.987 value_reward_chosen[t] + 0.515 reward + -0.099 value_reward_chosen^2 + -0.586 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.776 1 + 0.005 value_reward_not_chosen[t] + -0.042 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 8, 62, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 55\n",
      "value_reward_not_displayed: 74, 74, 74\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 324/1000 --- L(Train): 0.3710688 --- L(Val, RNN): 0.3622786 --- L(Val, SINDy): 0.3632632 --- Time: 2.61s; --- Convergence: 3.73e-04; LR: 1.00e-02; Metric: 0.3617853; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 0.987 value_reward_chosen[t] + 0.514 reward + -0.098 value_reward_chosen^2 + -0.583 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.778 1 + 0.007 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.996 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 9, 63, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 56\n",
      "value_reward_not_displayed: 75, 75, 75\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 325/1000 --- L(Train): 0.3805885 --- L(Val, RNN): 0.3613408 --- L(Val, SINDy): 0.3631414 --- Time: 2.74s; --- Convergence: 6.55e-04; LR: 1.00e-02; Metric: 0.3613408; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 0.987 value_reward_chosen[t] + 0.514 reward + -0.098 value_reward_chosen^2 + -0.58 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.008 value_reward_not_chosen[t] + -0.045 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.994 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 10, 64, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 57\n",
      "value_reward_not_displayed: 76, 76, 76\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 326/1000 --- L(Train): 0.3790060 --- L(Val, RNN): 0.3613755 --- L(Val, SINDy): 0.3630239 --- Time: 2.51s; --- Convergence: 3.45e-04; LR: 1.00e-02; Metric: 0.3613408; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 0.987 value_reward_chosen[t] + 0.513 reward + -0.097 value_reward_chosen^2 + -0.578 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.782 1 + 0.009 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.993 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 11, 65, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 58\n",
      "value_reward_not_displayed: 77, 77, 77\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 327/1000 --- L(Train): 0.3653777 --- L(Val, RNN): 0.3617477 --- L(Val, SINDy): 0.3628688 --- Time: 2.48s; --- Convergence: 3.59e-04; LR: 1.00e-02; Metric: 0.3613408; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.985 value_reward_chosen[t] + 0.513 reward + -0.095 value_reward_chosen^2 + -0.576 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.01 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.992 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 12, 66, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 59\n",
      "value_reward_not_displayed: 78, 78, 78\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 328/1000 --- L(Train): 0.3682284 --- L(Val, RNN): 0.3607858 --- L(Val, SINDy): 0.3626907 --- Time: 2.68s; --- Convergence: 6.60e-04; LR: 1.00e-02; Metric: 0.3607858; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 0.984 value_reward_chosen[t] + 0.514 reward + -0.093 value_reward_chosen^2 + -0.575 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.784 1 + 0.011 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.992 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 13, 67, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 60\n",
      "value_reward_not_displayed: 79, 79, 79\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 329/1000 --- L(Train): 0.3691866 --- L(Val, RNN): 0.3612520 --- L(Val, SINDy): 0.3625559 --- Time: 2.75s; --- Convergence: 5.63e-04; LR: 1.00e-02; Metric: 0.3607858; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.982 value_reward_chosen[t] + 0.512 reward + -0.092 value_reward_chosen^2 + -0.574 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.784 1 + 0.01 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.992 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 14, 68, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 61\n",
      "value_reward_not_displayed: 80, 80, 80\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 330/1000 --- L(Train): 0.3776205 --- L(Val, RNN): 0.3613104 --- L(Val, SINDy): 0.3623885 --- Time: 2.70s; --- Convergence: 3.11e-04; LR: 1.00e-02; Metric: 0.3607858; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 0.98 value_reward_chosen[t] + 0.51 reward + -0.091 value_reward_chosen^2 + -0.574 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.009 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.994 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 15, 69, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 62\n",
      "value_reward_not_displayed: 81, 81, 81\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 331/1000 --- L(Train): 0.3623534 --- L(Val, RNN): 0.3606034 --- L(Val, SINDy): 0.3621472 --- Time: 2.82s; --- Convergence: 5.09e-04; LR: 1.00e-02; Metric: 0.3606034; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 0.978 value_reward_chosen[t] + 0.507 reward + -0.09 value_reward_chosen^2 + -0.574 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.782 1 + 0.008 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.996 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 16, 70, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 63\n",
      "value_reward_not_displayed: 82, 82, 82\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 332/1000 --- L(Train): 0.3694854 --- L(Val, RNN): 0.3606910 --- L(Val, SINDy): 0.3619270 --- Time: 2.88s; --- Convergence: 2.98e-04; LR: 1.00e-02; Metric: 0.3606034; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.042 1 + 0.977 value_reward_chosen[t] + 0.505 reward + -0.09 value_reward_chosen^2 + -0.575 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.782 1 + 0.007 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.999 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 17, 71, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 64\n",
      "value_reward_not_displayed: 83, 83, 83\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 333/1000 --- L(Train): 0.3535681 --- L(Val, RNN): 0.3603192 --- L(Val, SINDy): 0.3617728 --- Time: 2.73s; --- Convergence: 3.35e-04; LR: 1.00e-02; Metric: 0.3603192; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.975 value_reward_chosen[t] + 0.503 reward + -0.089 value_reward_chosen^2 + -0.575 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.781 1 + 0.005 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.001 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 18, 72, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 65\n",
      "value_reward_not_displayed: 84, 84, 84\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 334/1000 --- L(Train): 0.3797423 --- L(Val, RNN): 0.3597617 --- L(Val, SINDy): 0.3616161 --- Time: 2.68s; --- Convergence: 4.46e-04; LR: 1.00e-02; Metric: 0.3597617; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.975 value_reward_chosen[t] + 0.502 reward + -0.089 value_reward_chosen^2 + -0.574 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.781 1 + 0.005 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.001 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 19, 73, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 66\n",
      "value_reward_not_displayed: 85, 85, 85\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 335/1000 --- L(Train): 0.3617136 --- L(Val, RNN): 0.3603173 --- L(Val, SINDy): 0.3614147 --- Time: 2.70s; --- Convergence: 5.01e-04; LR: 1.00e-02; Metric: 0.3597617; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.042 1 + 0.974 value_reward_chosen[t] + 0.502 reward + -0.089 value_reward_chosen^2 + -0.573 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.004 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 20, 74, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 67\n",
      "value_reward_not_displayed: 86, 86, 86\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 336/1000 --- L(Train): 0.3679864 --- L(Val, RNN): 0.3599782 --- L(Val, SINDy): 0.3611098 --- Time: 2.76s; --- Convergence: 4.20e-04; LR: 1.00e-02; Metric: 0.3597617; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.975 value_reward_chosen[t] + 0.502 reward + -0.087 value_reward_chosen^2 + -0.571 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.781 1 + 0.004 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 1.002 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 21, 75, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 68\n",
      "value_reward_not_displayed: 87, 87, 87\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 337/1000 --- L(Train): 0.3622489 --- L(Val, RNN): 0.3594598 --- L(Val, SINDy): 0.3608711 --- Time: 2.96s; --- Convergence: 4.69e-04; LR: 1.00e-02; Metric: 0.3594598; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.044 1 + 0.976 value_reward_chosen[t] + 0.505 reward + -0.085 value_reward_chosen^2 + -0.567 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.782 1 + 0.005 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 22, 76, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 69\n",
      "value_reward_not_displayed: 88, 88, 88\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 338/1000 --- L(Train): 0.3680854 --- L(Val, RNN): 0.3599593 --- L(Val, SINDy): 0.3607162 --- Time: 2.69s; --- Convergence: 4.84e-04; LR: 1.00e-02; Metric: 0.3594598; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.044 1 + 0.977 value_reward_chosen[t] + 0.508 reward + -0.083 value_reward_chosen^2 + -0.563 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.006 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 23, 77, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 70\n",
      "value_reward_not_displayed: 89, 89, 89\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 339/1000 --- L(Train): 0.3596076 --- L(Val, RNN): 0.3595545 --- L(Val, SINDy): 0.3606215 --- Time: 2.70s; --- Convergence: 4.45e-04; LR: 1.00e-02; Metric: 0.3594598; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.045 1 + 0.979 value_reward_chosen[t] + 0.511 reward + -0.081 value_reward_chosen^2 + -0.559 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.007 value_reward_not_chosen[t] + -0.042 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.997 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 24, 78, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 71\n",
      "value_reward_not_displayed: 90, 90, 90\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 340/1000 --- L(Train): 0.3697616 --- L(Val, RNN): 0.3591926 --- L(Val, SINDy): 0.3605427 --- Time: 2.84s; --- Convergence: 4.03e-04; LR: 1.00e-02; Metric: 0.3591926; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.046 1 + 0.98 value_reward_chosen[t] + 0.513 reward + -0.079 value_reward_chosen^2 + -0.556 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.008 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.996 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 25, 79, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 72\n",
      "value_reward_not_displayed: 91, 91, 91\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 341/1000 --- L(Train): 0.3745123 --- L(Val, RNN): 0.3597416 --- L(Val, SINDy): 0.3604845 --- Time: 2.69s; --- Convergence: 4.76e-04; LR: 1.00e-02; Metric: 0.3591926; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.045 1 + 0.981 value_reward_chosen[t] + 0.514 reward + -0.079 value_reward_chosen^2 + -0.555 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.008 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.995 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 26, 80, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 73\n",
      "value_reward_not_displayed: 92, 92, 92\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 342/1000 --- L(Train): 0.3705338 --- L(Val, RNN): 0.3584717 --- L(Val, SINDy): 0.3603707 --- Time: 2.61s; --- Convergence: 8.73e-04; LR: 1.00e-02; Metric: 0.3584717; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.045 1 + 0.982 value_reward_chosen[t] + 0.514 reward + -0.079 value_reward_chosen^2 + -0.553 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.009 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 0.994 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 27, 81, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 74\n",
      "value_reward_not_displayed: 93, 93, 93\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 343/1000 --- L(Train): 0.3703560 --- L(Val, RNN): 0.3590836 --- L(Val, SINDy): 0.3602815 --- Time: 2.86s; --- Convergence: 7.42e-04; LR: 1.00e-02; Metric: 0.3584717; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.042 1 + 0.983 value_reward_chosen[t] + 0.512 reward + -0.082 value_reward_chosen^2 + -0.554 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.009 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 0.995 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 28, 82, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 75\n",
      "value_reward_not_displayed: 94, 94, 94\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 344/1000 --- L(Train): 0.3647838 --- L(Val, RNN): 0.3587033 --- L(Val, SINDy): 0.3601669 --- Time: 3.04s; --- Convergence: 5.61e-04; LR: 1.00e-02; Metric: 0.3584717; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.984 value_reward_chosen[t] + 0.509 reward + -0.085 value_reward_chosen^2 + -0.556 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.009 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.007 1 + 0.995 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 29, 83, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 76\n",
      "value_reward_not_displayed: 95, 95, 95\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 345/1000 --- L(Train): 0.3855430 --- L(Val, RNN): 0.3587139 --- L(Val, SINDy): 0.3600377 --- Time: 2.98s; --- Convergence: 2.86e-04; LR: 1.00e-02; Metric: 0.3584717; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.985 value_reward_chosen[t] + 0.507 reward + -0.088 value_reward_chosen^2 + -0.557 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.009 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 0.996 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 30, 84, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 77\n",
      "value_reward_not_displayed: 96, 96, 96\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 346/1000 --- L(Train): 0.3691560 --- L(Val, RNN): 0.3589861 --- L(Val, SINDy): 0.3599291 --- Time: 2.71s; --- Convergence: 2.79e-04; LR: 1.00e-02; Metric: 0.3584717; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 0.986 value_reward_chosen[t] + 0.506 reward + -0.089 value_reward_chosen^2 + -0.557 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.008 value_reward_not_chosen[t] + -0.042 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.997 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 31, 85, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 78\n",
      "value_reward_not_displayed: 97, 97, 97\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 347/1000 --- L(Train): 0.3802792 --- L(Val, RNN): 0.3584464 --- L(Val, SINDy): 0.3598228 --- Time: 2.79s; --- Convergence: 4.09e-04; LR: 1.00e-02; Metric: 0.3584464; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 0.987 value_reward_chosen[t] + 0.504 reward + -0.091 value_reward_chosen^2 + -0.557 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.007 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.998 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 32, 86, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 79\n",
      "value_reward_not_displayed: 98, 98, 98\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 348/1000 --- L(Train): 0.3646289 --- L(Val, RNN): 0.3580275 --- L(Val, SINDy): 0.3597271 --- Time: 2.66s; --- Convergence: 4.14e-04; LR: 1.00e-02; Metric: 0.3580275; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.033 1 + 0.989 value_reward_chosen[t] + 0.504 reward + -0.091 value_reward_chosen^2 + -0.556 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.006 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 0.999 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 33, 87, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 80\n",
      "value_reward_not_displayed: 99, 99, 99\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 349/1000 --- L(Train): 0.3750476 --- L(Val, RNN): 0.3586763 --- L(Val, SINDy): 0.3596818 --- Time: 2.67s; --- Convergence: 5.31e-04; LR: 1.00e-02; Metric: 0.3580275; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 11):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.991 value_reward_chosen[t] + 0.505 reward + -0.091 value_reward_chosen^2 + -0.554 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.006 value_reward_not_chosen[t] + -0.039 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.0 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 34, 88, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 81\n",
      "value_reward_not_displayed: 100, -, 100\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 350/1000 --- L(Train): 0.3634508 --- L(Val, RNN): 0.3575383 --- L(Val, SINDy): 0.3596469 --- Time: 2.61s; --- Convergence: 8.35e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 10):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.992 value_reward_chosen[t] + 0.505 reward + -0.09 value_reward_chosen^2 + -0.551 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.005 value_reward_not_chosen[t] + -0.038 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 35, 89, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 82\n",
      "value_reward_not_displayed: 101, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 351/1000 --- L(Train): 0.3586209 --- L(Val, RNN): 0.3583985 --- L(Val, SINDy): 0.3593742 --- Time: 2.70s; --- Convergence: 8.47e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 9):\n",
      "value_reward_chosen[t+1] = 0.031 1 + 0.994 value_reward_chosen[t] + 0.506 reward + -0.09 value_reward_chosen^2 + -0.548 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.005 value_reward_not_chosen[t] + -0.038 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 36, 90, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 83\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 352/1000 --- L(Train): 0.3776142 --- L(Val, RNN): 0.3577067 --- L(Val, SINDy): 0.3594610 --- Time: 2.78s; --- Convergence: 7.70e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.995 value_reward_chosen[t] + 0.508 reward + -0.088 value_reward_chosen^2 + -0.546 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.005 value_reward_not_chosen[t] + -0.038 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 37, 91, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 84\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 353/1000 --- L(Train): 0.3640499 --- L(Val, RNN): 0.3575847 --- L(Val, SINDy): 0.3594640 --- Time: 2.66s; --- Convergence: 4.46e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.997 value_reward_chosen[t] + 0.509 reward + -0.087 value_reward_chosen^2 + -0.543 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.006 value_reward_not_chosen[t] + -0.038 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 38, 92, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 85\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 354/1000 --- L(Train): 0.3802846 --- L(Val, RNN): 0.3587042 --- L(Val, SINDy): 0.3593441 --- Time: 2.76s; --- Convergence: 7.83e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.998 value_reward_chosen[t] + 0.51 reward + -0.086 value_reward_chosen^2 + -0.541 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.006 value_reward_not_chosen[t] + -0.038 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 39, 93, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 86\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 355/1000 --- L(Train): 0.3678274 --- L(Val, RNN): 0.3585341 --- L(Val, SINDy): 0.3592089 --- Time: 2.70s; --- Convergence: 4.76e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.999 value_reward_chosen[t] + 0.511 reward + -0.085 value_reward_chosen^2 + -0.539 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.006 value_reward_not_chosen[t] + -0.037 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 40, 94, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 87\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 356/1000 --- L(Train): 0.3665733 --- L(Val, RNN): 0.3587619 --- L(Val, SINDy): 0.3590898 --- Time: 2.71s; --- Convergence: 3.52e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.033 1 + 0.999 value_reward_chosen[t] + 0.511 reward + -0.084 value_reward_chosen^2 + -0.538 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.006 value_reward_not_chosen[t] + -0.037 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 41, 95, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 88\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 357/1000 --- L(Train): 0.3814211 --- L(Val, RNN): 0.3578835 --- L(Val, SINDy): 0.3590362 --- Time: 2.57s; --- Convergence: 6.15e-04; LR: 1.00e-02; Metric: 0.3575383; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 0.997 value_reward_chosen[t] + 0.511 reward + -0.083 value_reward_chosen^2 + -0.539 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.006 value_reward_not_chosen[t] + -0.037 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 42, 96, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 89\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 358/1000 --- L(Train): 0.3527501 --- L(Val, RNN): 0.3575023 --- L(Val, SINDy): 0.3590779 --- Time: 2.85s; --- Convergence: 4.98e-04; LR: 1.00e-02; Metric: 0.3575023; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 0.996 value_reward_chosen[t] + 0.511 reward + -0.082 value_reward_chosen^2 + -0.54 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.006 value_reward_not_chosen[t] + -0.036 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 43, 97, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 90\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 359/1000 --- L(Train): 0.3738164 --- L(Val, RNN): 0.3574927 --- L(Val, SINDy): 0.3590933 --- Time: 2.83s; --- Convergence: 2.54e-04; LR: 1.00e-02; Metric: 0.3574927; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 0.995 value_reward_chosen[t] + 0.511 reward + -0.082 value_reward_chosen^2 + -0.541 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.036 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 44, 98, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 91\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 360/1000 --- L(Train): 0.3552325 --- L(Val, RNN): 0.3571127 --- L(Val, SINDy): 0.3590658 --- Time: 2.71s; --- Convergence: 3.17e-04; LR: 1.00e-02; Metric: 0.3571127; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 0.994 value_reward_chosen[t] + 0.511 reward + -0.081 value_reward_chosen^2 + -0.542 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.036 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 45, 99, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 92\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 361/1000 --- L(Train): 0.3715648 --- L(Val, RNN): 0.3580245 --- L(Val, SINDy): 0.3590181 --- Time: 2.82s; --- Convergence: 6.14e-04; LR: 1.00e-02; Metric: 0.3571127; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.511 reward + -0.081 value_reward_chosen^2 + -0.542 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.035 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 46, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 93\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 362/1000 --- L(Train): 0.3808124 --- L(Val, RNN): 0.3570452 --- L(Val, SINDy): 0.3588986 --- Time: 2.92s; --- Convergence: 7.97e-04; LR: 1.00e-02; Metric: 0.3570452; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.511 reward + -0.08 value_reward_chosen^2 + -0.543 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.035 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 47, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 94\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 363/1000 --- L(Train): 0.3760915 --- L(Val, RNN): 0.3573807 --- L(Val, SINDy): 0.3588104 --- Time: 2.76s; --- Convergence: 5.66e-04; LR: 1.00e-02; Metric: 0.3570452; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 1.0 value_reward_chosen[t] + 0.51 reward + -0.08 value_reward_chosen^2 + -0.544 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.035 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 48, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 95\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 364/1000 --- L(Train): 0.3753049 --- L(Val, RNN): 0.3573067 --- L(Val, SINDy): 0.3587210 --- Time: 2.69s; --- Convergence: 3.20e-04; LR: 1.00e-02; Metric: 0.3570452; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 1.0 value_reward_chosen[t] + 0.507 reward + -0.081 value_reward_chosen^2 + -0.547 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.034 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 49, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 96\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 365/1000 --- L(Train): 0.3592592 --- L(Val, RNN): 0.3574324 --- L(Val, SINDy): 0.3586631 --- Time: 2.90s; --- Convergence: 2.23e-04; LR: 1.00e-02; Metric: 0.3570452; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.504 reward + -0.084 value_reward_chosen^2 + -0.549 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.034 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 50, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 97\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 366/1000 --- L(Train): 0.3771119 --- L(Val, RNN): 0.3571638 --- L(Val, SINDy): 0.3585691 --- Time: 2.99s; --- Convergence: 2.46e-04; LR: 1.00e-02; Metric: 0.3570452; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.5 reward + -0.087 value_reward_chosen^2 + -0.55 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.034 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 51, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 98\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 367/1000 --- L(Train): 0.3680075 --- L(Val, RNN): 0.3570185 --- L(Val, SINDy): 0.3585070 --- Time: 2.73s; --- Convergence: 1.96e-04; LR: 1.00e-02; Metric: 0.3570185; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.033 1 + 1.0 value_reward_chosen[t] + 0.497 reward + -0.09 value_reward_chosen^2 + -0.551 value_reward_chosen*reward + 0.496 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.005 value_reward_not_chosen[t] + -0.033 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 52, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 99\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 368/1000 --- L(Train): 0.3817546 --- L(Val, RNN): 0.3569023 --- L(Val, SINDy): 0.3584180 --- Time: 3.01s; --- Convergence: 1.56e-04; LR: 1.00e-02; Metric: 0.3569023; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.031 1 + 1.0 value_reward_chosen[t] + 0.494 reward + -0.092 value_reward_chosen^2 + -0.552 value_reward_chosen*reward + 0.493 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 53, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 369/1000 --- L(Train): 0.3731169 --- L(Val, RNN): 0.3565368 --- L(Val, SINDy): 0.3583108 --- Time: 2.67s; --- Convergence: 2.61e-04; LR: 1.00e-02; Metric: 0.3565368; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.031 1 + 1.0 value_reward_chosen[t] + 0.494 reward + -0.092 value_reward_chosen^2 + -0.55 value_reward_chosen*reward + 0.493 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.007 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 54, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 370/1000 --- L(Train): 0.3798729 --- L(Val, RNN): 0.3575580 --- L(Val, SINDy): 0.3582236 --- Time: 2.80s; --- Convergence: 6.41e-04; LR: 1.00e-02; Metric: 0.3565368; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 1.0 value_reward_chosen[t] + 0.495 reward + -0.09 value_reward_chosen^2 + -0.546 value_reward_chosen*reward + 0.494 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.793 1 + 0.009 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 55, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 371/1000 --- L(Train): 0.3892548 --- L(Val, RNN): 0.3563370 --- L(Val, SINDy): 0.3580807 --- Time: 2.52s; --- Convergence: 9.31e-04; LR: 1.00e-02; Metric: 0.3563370; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 1.0 value_reward_chosen[t] + 0.496 reward + -0.089 value_reward_chosen^2 + -0.542 value_reward_chosen*reward + 0.495 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.795 1 + 0.011 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 56, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 372/1000 --- L(Train): 0.3623981 --- L(Val, RNN): 0.3577859 --- L(Val, SINDy): 0.3579426 --- Time: 2.65s; --- Convergence: 1.19e-03; LR: 1.00e-02; Metric: 0.3563370; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 1.0 value_reward_chosen[t] + 0.498 reward + -0.087 value_reward_chosen^2 + -0.536 value_reward_chosen*reward + 0.497 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.798 1 + 0.014 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 57, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 373/1000 --- L(Train): 0.3660708 --- L(Val, RNN): 0.3553836 --- L(Val, SINDy): 0.3578046 --- Time: 2.66s; --- Convergence: 1.80e-03; LR: 1.00e-02; Metric: 0.3553836; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 1.0 value_reward_chosen[t] + 0.501 reward + -0.084 value_reward_chosen^2 + -0.531 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.801 1 + 0.016 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 58, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 374/1000 --- L(Train): 0.3847821 --- L(Val, RNN): 0.3569148 --- L(Val, SINDy): 0.3577319 --- Time: 2.67s; --- Convergence: 1.66e-03; LR: 1.00e-02; Metric: 0.3553836; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 1.0 value_reward_chosen[t] + 0.504 reward + -0.082 value_reward_chosen^2 + -0.526 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.804 1 + 0.019 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 59, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 375/1000 --- L(Train): 0.3616807 --- L(Val, RNN): 0.3569439 --- L(Val, SINDy): 0.3576626 --- Time: 2.60s; --- Convergence: 8.46e-04; LR: 1.00e-02; Metric: 0.3553836; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.505 reward + -0.081 value_reward_chosen^2 + -0.522 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.806 1 + 0.02 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 60, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 376/1000 --- L(Train): 0.3744946 --- L(Val, RNN): 0.3552222 --- L(Val, SINDy): 0.3575597 --- Time: 2.66s; --- Convergence: 1.28e-03; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.507 reward + -0.08 value_reward_chosen^2 + -0.52 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.808 1 + 0.021 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 61, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 377/1000 --- L(Train): 0.3635595 --- L(Val, RNN): 0.3570043 --- L(Val, SINDy): 0.3575285 --- Time: 2.60s; --- Convergence: 1.53e-03; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.507 reward + -0.079 value_reward_chosen^2 + -0.519 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.809 1 + 0.022 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 62, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 378/1000 --- L(Train): 0.3711840 --- L(Val, RNN): 0.3559175 --- L(Val, SINDy): 0.3574377 --- Time: 2.61s; --- Convergence: 1.31e-03; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.508 reward + -0.078 value_reward_chosen^2 + -0.52 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.81 1 + 0.021 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 63, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 379/1000 --- L(Train): 0.3765322 --- L(Val, RNN): 0.3555437 --- L(Val, SINDy): 0.3573606 --- Time: 2.52s; --- Convergence: 8.42e-04; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 1.0 value_reward_chosen[t] + 0.509 reward + -0.077 value_reward_chosen^2 + -0.521 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.811 1 + 0.021 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 64, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 380/1000 --- L(Train): 0.3703887 --- L(Val, RNN): 0.3576250 --- L(Val, SINDy): 0.3573783 --- Time: 2.71s; --- Convergence: 1.46e-03; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.506 reward + -0.08 value_reward_chosen^2 + -0.525 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.81 1 + 0.019 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 65, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 381/1000 --- L(Train): 0.3611626 --- L(Val, RNN): 0.3554964 --- L(Val, SINDy): 0.3573920 --- Time: 2.63s; --- Convergence: 1.80e-03; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 1.0 value_reward_chosen[t] + 0.503 reward + -0.083 value_reward_chosen^2 + -0.528 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.81 1 + 0.017 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 66, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 382/1000 --- L(Train): 0.3643787 --- L(Val, RNN): 0.3554310 --- L(Val, SINDy): 0.3573830 --- Time: 2.52s; --- Convergence: 9.30e-04; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.501 reward + -0.085 value_reward_chosen^2 + -0.531 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.809 1 + 0.015 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 67, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 383/1000 --- L(Train): 0.3542753 --- L(Val, RNN): 0.3562782 --- L(Val, SINDy): 0.3573461 --- Time: 2.70s; --- Convergence: 8.89e-04; LR: 1.00e-02; Metric: 0.3552222; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 1.0 value_reward_chosen[t] + 0.499 reward + -0.087 value_reward_chosen^2 + -0.533 value_reward_chosen*reward + 0.498 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.808 1 + 0.013 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 68, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 384/1000 --- L(Train): 0.3591893 --- L(Val, RNN): 0.3550334 --- L(Val, SINDy): 0.3572132 --- Time: 2.58s; --- Convergence: 1.07e-03; LR: 1.00e-02; Metric: 0.3550334; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.499 reward + -0.086 value_reward_chosen^2 + -0.534 value_reward_chosen*reward + 0.498 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.807 1 + 0.011 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 69, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 385/1000 --- L(Train): 0.3731416 --- L(Val, RNN): 0.3555945 --- L(Val, SINDy): 0.3570765 --- Time: 2.66s; --- Convergence: 8.14e-04; LR: 1.00e-02; Metric: 0.3550334; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.035 1 + 1.0 value_reward_chosen[t] + 0.499 reward + -0.086 value_reward_chosen^2 + -0.534 value_reward_chosen*reward + 0.499 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.807 1 + 0.009 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 70, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 386/1000 --- L(Train): 0.3651965 --- L(Val, RNN): 0.3564099 --- L(Val, SINDy): 0.3569159 --- Time: 2.50s; --- Convergence: 8.15e-04; LR: 1.00e-02; Metric: 0.3550334; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 1.0 value_reward_chosen[t] + 0.501 reward + -0.084 value_reward_chosen^2 + -0.532 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.807 1 + 0.008 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 71, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 387/1000 --- L(Train): 0.3672036 --- L(Val, RNN): 0.3551064 --- L(Val, SINDy): 0.3566944 --- Time: 2.59s; --- Convergence: 1.06e-03; LR: 1.00e-02; Metric: 0.3550334; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.502 reward + -0.083 value_reward_chosen^2 + -0.53 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.806 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 72, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 388/1000 --- L(Train): 0.3690777 --- L(Val, RNN): 0.3553295 --- L(Val, SINDy): 0.3565223 --- Time: 2.64s; --- Convergence: 6.41e-04; LR: 1.00e-02; Metric: 0.3550334; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.503 reward + -0.082 value_reward_chosen^2 + -0.527 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.807 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 73, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 389/1000 --- L(Train): 0.3676171 --- L(Val, RNN): 0.3558802 --- L(Val, SINDy): 0.3564577 --- Time: 2.76s; --- Convergence: 5.96e-04; LR: 1.00e-02; Metric: 0.3550334; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 1.0 value_reward_chosen[t] + 0.505 reward + -0.08 value_reward_chosen^2 + -0.523 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.808 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 74, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 390/1000 --- L(Train): 0.3590076 --- L(Val, RNN): 0.3548414 --- L(Val, SINDy): 0.3564465 --- Time: 2.52s; --- Convergence: 8.17e-04; LR: 1.00e-02; Metric: 0.3548414; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.507 reward + -0.078 value_reward_chosen^2 + -0.519 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.809 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 75, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 391/1000 --- L(Train): 0.3669330 --- L(Val, RNN): 0.3549368 --- L(Val, SINDy): 0.3564851 --- Time: 2.69s; --- Convergence: 4.56e-04; LR: 1.00e-02; Metric: 0.3548414; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 1.0 value_reward_chosen[t] + 0.508 reward + -0.076 value_reward_chosen^2 + -0.515 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.81 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 76, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 392/1000 --- L(Train): 0.3678856 --- L(Val, RNN): 0.3550570 --- L(Val, SINDy): 0.3564706 --- Time: 2.62s; --- Convergence: 2.88e-04; LR: 1.00e-02; Metric: 0.3548414; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 1.0 value_reward_chosen[t] + 0.51 reward + -0.075 value_reward_chosen^2 + -0.512 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.811 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 77, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 393/1000 --- L(Train): 0.3620531 --- L(Val, RNN): 0.3546036 --- L(Val, SINDy): 0.3564189 --- Time: 2.64s; --- Convergence: 3.71e-04; LR: 1.00e-02; Metric: 0.3546036; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.511 reward + -0.073 value_reward_chosen^2 + -0.509 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.811 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 78, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 394/1000 --- L(Train): 0.3801490 --- L(Val, RNN): 0.3553713 --- L(Val, SINDy): 0.3563759 --- Time: 2.53s; --- Convergence: 5.69e-04; LR: 1.00e-02; Metric: 0.3546036; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.511 reward + -0.074 value_reward_chosen^2 + -0.508 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.812 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 79, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 395/1000 --- L(Train): 0.3578936 --- L(Val, RNN): 0.3549188 --- L(Val, SINDy): 0.3563175 --- Time: 2.62s; --- Convergence: 5.11e-04; LR: 1.00e-02; Metric: 0.3546036; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.511 reward + -0.074 value_reward_chosen^2 + -0.508 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 80, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 396/1000 --- L(Train): 0.3502708 --- L(Val, RNN): 0.3547455 --- L(Val, SINDy): 0.3562758 --- Time: 2.75s; --- Convergence: 3.42e-04; LR: 1.00e-02; Metric: 0.3546036; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.51 reward + -0.075 value_reward_chosen^2 + -0.509 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 81, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 397/1000 --- L(Train): 0.3686843 --- L(Val, RNN): 0.3545844 --- L(Val, SINDy): 0.3562559 --- Time: 2.67s; --- Convergence: 2.52e-04; LR: 1.00e-02; Metric: 0.3545844; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.509 reward + -0.076 value_reward_chosen^2 + -0.511 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 82, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 398/1000 --- L(Train): 0.3581403 --- L(Val, RNN): 0.3547466 --- L(Val, SINDy): 0.3561915 --- Time: 2.60s; --- Convergence: 2.07e-04; LR: 1.00e-02; Metric: 0.3545844; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 1.0 value_reward_chosen[t] + 0.507 reward + -0.077 value_reward_chosen^2 + -0.514 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 83, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 399/1000 --- L(Train): 0.3677959 --- L(Val, RNN): 0.3541359 --- L(Val, SINDy): 0.3560925 --- Time: 2.66s; --- Convergence: 4.09e-04; LR: 1.00e-02; Metric: 0.3541359; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 1.0 value_reward_chosen[t] + 0.505 reward + -0.079 value_reward_chosen^2 + -0.517 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 84, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 400/1000 --- L(Train): 0.3694801 --- L(Val, RNN): 0.3544749 --- L(Val, SINDy): 0.3560213 --- Time: 2.53s; --- Convergence: 3.74e-04; LR: 1.00e-02; Metric: 0.3541359; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.504 reward + -0.08 value_reward_chosen^2 + -0.519 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + 0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 85, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 401/1000 --- L(Train): 0.3790910 --- L(Val, RNN): 0.3549803 --- L(Val, SINDy): 0.3559591 --- Time: 2.60s; --- Convergence: 4.40e-04; LR: 1.00e-02; Metric: 0.3541359; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.503 reward + -0.08 value_reward_chosen^2 + -0.52 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + 0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 86, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 402/1000 --- L(Train): 0.3727241 --- L(Val, RNN): 0.3540906 --- L(Val, SINDy): 0.3558882 --- Time: 2.47s; --- Convergence: 6.65e-04; LR: 1.00e-02; Metric: 0.3540906; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.504 reward + -0.08 value_reward_chosen^2 + -0.519 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + 0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 87, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 403/1000 --- L(Train): 0.3863672 --- L(Val, RNN): 0.3545996 --- L(Val, SINDy): 0.3558584 --- Time: 2.61s; --- Convergence: 5.87e-04; LR: 1.00e-02; Metric: 0.3540906; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 1.0 value_reward_chosen[t] + 0.504 reward + -0.08 value_reward_chosen^2 + -0.517 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + 0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 88, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 404/1000 --- L(Train): 0.3797853 --- L(Val, RNN): 0.3545156 --- L(Val, SINDy): 0.3558298 --- Time: 2.56s; --- Convergence: 3.35e-04; LR: 1.00e-02; Metric: 0.3540906; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 1.0 value_reward_chosen[t] + 0.504 reward + -0.081 value_reward_chosen^2 + -0.515 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + 0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 89, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 405/1000 --- L(Train): 0.3553049 --- L(Val, RNN): 0.3536008 --- L(Val, SINDy): 0.3557330 --- Time: 2.62s; --- Convergence: 6.25e-04; LR: 1.00e-02; Metric: 0.3536008; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.505 reward + -0.08 value_reward_chosen^2 + -0.51 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + 0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 90, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 406/1000 --- L(Train): 0.3593244 --- L(Val, RNN): 0.3542211 --- L(Val, SINDy): 0.3556141 --- Time: 2.70s; --- Convergence: 6.23e-04; LR: 1.00e-02; Metric: 0.3536008; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.507 reward + -0.079 value_reward_chosen^2 + -0.506 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + 0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 91, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 407/1000 --- L(Train): 0.3644836 --- L(Val, RNN): 0.3544689 --- L(Val, SINDy): 0.3554871 --- Time: 2.54s; --- Convergence: 4.35e-04; LR: 1.00e-02; Metric: 0.3536008; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.509 reward + -0.078 value_reward_chosen^2 + -0.502 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + 0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 92, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 408/1000 --- L(Train): 0.3566688 --- L(Val, RNN): 0.3533913 --- L(Val, SINDy): 0.3553632 --- Time: 2.70s; --- Convergence: 7.56e-04; LR: 1.00e-02; Metric: 0.3533913; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.51 reward + -0.077 value_reward_chosen^2 + -0.498 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.82 1 + 0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 93, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 409/1000 --- L(Train): 0.3600677 --- L(Val, RNN): 0.3542985 --- L(Val, SINDy): 0.3553346 --- Time: 2.55s; --- Convergence: 8.32e-04; LR: 1.00e-02; Metric: 0.3533913; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.512 reward + -0.076 value_reward_chosen^2 + -0.495 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + 0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 94, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 410/1000 --- L(Train): 0.3602525 --- L(Val, RNN): 0.3540916 --- L(Val, SINDy): 0.3553195 --- Time: 2.54s; --- Convergence: 5.19e-04; LR: 1.00e-02; Metric: 0.3533913; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 1.0 value_reward_chosen[t] + 0.512 reward + -0.076 value_reward_chosen^2 + -0.493 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.822 1 + 0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 95, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 411/1000 --- L(Train): 0.3593105 --- L(Val, RNN): 0.3531057 --- L(Val, SINDy): 0.3553047 --- Time: 2.55s; --- Convergence: 7.53e-04; LR: 1.00e-02; Metric: 0.3531057; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 1.0 value_reward_chosen[t] + 0.51 reward + -0.078 value_reward_chosen^2 + -0.493 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.822 1 + 0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 96, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 412/1000 --- L(Train): 0.3772614 --- L(Val, RNN): 0.3541117 --- L(Val, SINDy): 0.3552607 --- Time: 2.74s; --- Convergence: 8.79e-04; LR: 1.00e-02; Metric: 0.3531057; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 1.0 value_reward_chosen[t] + 0.509 reward + -0.079 value_reward_chosen^2 + -0.494 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.822 1 + 0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 97, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 413/1000 --- L(Train): 0.3591339 --- L(Val, RNN): 0.3535656 --- L(Val, SINDy): 0.3551781 --- Time: 2.70s; --- Convergence: 7.13e-04; LR: 1.00e-02; Metric: 0.3531057; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 1.0 value_reward_chosen[t] + 0.508 reward + -0.08 value_reward_chosen^2 + -0.494 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.822 1 + 0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 98, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 414/1000 --- L(Train): 0.3600696 --- L(Val, RNN): 0.3535034 --- L(Val, SINDy): 0.3551113 --- Time: 2.74s; --- Convergence: 3.87e-04; LR: 1.00e-02; Metric: 0.3531057; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 1.0 value_reward_chosen[t] + 0.508 reward + -0.08 value_reward_chosen^2 + -0.495 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.822 1 + 0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 99, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 415/1000 --- L(Train): 0.3777986 --- L(Val, RNN): 0.3538740 --- L(Val, SINDy): 0.3550843 --- Time: 2.61s; --- Convergence: 3.79e-04; LR: 1.00e-02; Metric: 0.3531057; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.507 reward + -0.081 value_reward_chosen^2 + -0.496 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + 0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 416/1000 --- L(Train): 0.3619646 --- L(Val, RNN): 0.3529087 --- L(Val, SINDy): 0.3550087 --- Time: 2.68s; --- Convergence: 6.72e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.075 value_reward_chosen^2 + -0.494 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 417/1000 --- L(Train): 0.3651938 --- L(Val, RNN): 0.3534500 --- L(Val, SINDy): 0.3549330 --- Time: 2.51s; --- Convergence: 6.07e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.515 reward + -0.067 value_reward_chosen^2 + -0.491 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.82 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 418/1000 --- L(Train): 0.3577031 --- L(Val, RNN): 0.3532739 --- L(Val, SINDy): 0.3547673 --- Time: 2.53s; --- Convergence: 3.91e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.519 reward + -0.061 value_reward_chosen^2 + -0.488 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 419/1000 --- L(Train): 0.3676631 --- L(Val, RNN): 0.3536301 --- L(Val, SINDy): 0.3546662 --- Time: 2.57s; --- Convergence: 3.74e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.521 reward + -0.055 value_reward_chosen^2 + -0.486 value_reward_chosen*reward + 0.521 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 420/1000 --- L(Train): 0.3660114 --- L(Val, RNN): 0.3534282 --- L(Val, SINDy): 0.3545826 --- Time: 2.74s; --- Convergence: 2.88e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.519 reward + -0.054 value_reward_chosen^2 + -0.489 value_reward_chosen*reward + 0.519 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 421/1000 --- L(Train): 0.3563517 --- L(Val, RNN): 0.3535878 --- L(Val, SINDy): 0.3545355 --- Time: 2.72s; --- Convergence: 2.24e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.517 reward + -0.052 value_reward_chosen^2 + -0.492 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 422/1000 --- L(Train): 0.3965977 --- L(Val, RNN): 0.3532256 --- L(Val, SINDy): 0.3545322 --- Time: 2.83s; --- Convergence: 2.93e-04; LR: 1.00e-02; Metric: 0.3529087; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.513 reward + -0.053 value_reward_chosen^2 + -0.499 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 423/1000 --- L(Train): 0.3500566 --- L(Val, RNN): 0.3527947 --- L(Val, SINDy): 0.3544954 --- Time: 2.82s; --- Convergence: 3.62e-04; LR: 1.00e-02; Metric: 0.3527947; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.509 reward + -0.054 value_reward_chosen^2 + -0.507 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 424/1000 --- L(Train): 0.3466142 --- L(Val, RNN): 0.3527929 --- L(Val, SINDy): 0.3544222 --- Time: 2.74s; --- Convergence: 1.82e-04; LR: 1.00e-02; Metric: 0.3527929; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.504 reward + -0.057 value_reward_chosen^2 + -0.515 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 425/1000 --- L(Train): 0.3522865 --- L(Val, RNN): 0.3528379 --- L(Val, SINDy): 0.3543546 --- Time: 2.51s; --- Convergence: 1.13e-04; LR: 1.00e-02; Metric: 0.3527929; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.499 reward + -0.059 value_reward_chosen^2 + -0.521 value_reward_chosen*reward + 0.498 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 426/1000 --- L(Train): 0.3581120 --- L(Val, RNN): 0.3529859 --- L(Val, SINDy): 0.3542596 --- Time: 2.65s; --- Convergence: 1.31e-04; LR: 1.00e-02; Metric: 0.3527929; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.495 reward + -0.062 value_reward_chosen^2 + -0.527 value_reward_chosen*reward + 0.494 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.82 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 427/1000 --- L(Train): 0.3662313 --- L(Val, RNN): 0.3526681 --- L(Val, SINDy): 0.3542067 --- Time: 2.65s; --- Convergence: 2.24e-04; LR: 1.00e-02; Metric: 0.3526681; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.497 reward + -0.059 value_reward_chosen^2 + -0.523 value_reward_chosen*reward + 0.497 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 428/1000 --- L(Train): 0.3727421 --- L(Val, RNN): 0.3530887 --- L(Val, SINDy): 0.3541815 --- Time: 2.84s; --- Convergence: 3.22e-04; LR: 1.00e-02; Metric: 0.3526681; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.5 reward + -0.057 value_reward_chosen^2 + -0.52 value_reward_chosen*reward + 0.499 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 429/1000 --- L(Train): 0.3708357 --- L(Val, RNN): 0.3526276 --- L(Val, SINDy): 0.3541222 --- Time: 2.95s; --- Convergence: 3.92e-04; LR: 1.00e-02; Metric: 0.3526276; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.502 reward + -0.055 value_reward_chosen^2 + -0.518 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 430/1000 --- L(Train): 0.3609364 --- L(Val, RNN): 0.3524423 --- L(Val, SINDy): 0.3540587 --- Time: 2.78s; --- Convergence: 2.89e-04; LR: 1.00e-02; Metric: 0.3524423; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.053 value_reward_chosen^2 + -0.514 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.822 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 431/1000 --- L(Train): 0.3617759 --- L(Val, RNN): 0.3527870 --- L(Val, SINDy): 0.3540077 --- Time: 2.67s; --- Convergence: 3.17e-04; LR: 1.00e-02; Metric: 0.3524423; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.505 reward + -0.053 value_reward_chosen^2 + -0.51 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 432/1000 --- L(Train): 0.3617719 --- L(Val, RNN): 0.3523214 --- L(Val, SINDy): 0.3539185 --- Time: 2.59s; --- Convergence: 3.91e-04; LR: 1.00e-02; Metric: 0.3523214; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.052 value_reward_chosen^2 + -0.505 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.821 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 433/1000 --- L(Train): 0.3420665 --- L(Val, RNN): 0.3527357 --- L(Val, SINDy): 0.3538799 --- Time: 2.67s; --- Convergence: 4.03e-04; LR: 1.00e-02; Metric: 0.3523214; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.052 value_reward_chosen^2 + -0.502 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.82 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 434/1000 --- L(Train): 0.3671471 --- L(Val, RNN): 0.3528344 --- L(Val, SINDy): 0.3538537 --- Time: 2.84s; --- Convergence: 2.51e-04; LR: 1.00e-02; Metric: 0.3523214; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.053 value_reward_chosen^2 + -0.5 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 435/1000 --- L(Train): 0.3665675 --- L(Val, RNN): 0.3518531 --- L(Val, SINDy): 0.3537972 --- Time: 3.00s; --- Convergence: 6.16e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.054 value_reward_chosen^2 + -0.499 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 436/1000 --- L(Train): 0.3504429 --- L(Val, RNN): 0.3528520 --- L(Val, SINDy): 0.3537661 --- Time: 2.65s; --- Convergence: 8.07e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.058 value_reward_chosen^2 + -0.498 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 437/1000 --- L(Train): 0.3730924 --- L(Val, RNN): 0.3520778 --- L(Val, SINDy): 0.3536769 --- Time: 2.79s; --- Convergence: 7.91e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.505 reward + -0.062 value_reward_chosen^2 + -0.497 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 438/1000 --- L(Train): 0.3610926 --- L(Val, RNN): 0.3521296 --- L(Val, SINDy): 0.3536019 --- Time: 2.69s; --- Convergence: 4.21e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.504 reward + -0.064 value_reward_chosen^2 + -0.496 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 439/1000 --- L(Train): 0.3549016 --- L(Val, RNN): 0.3526728 --- L(Val, SINDy): 0.3535548 --- Time: 2.56s; --- Convergence: 4.82e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.066 value_reward_chosen^2 + -0.495 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 440/1000 --- L(Train): 0.3655876 --- L(Val, RNN): 0.3518931 --- L(Val, SINDy): 0.3535092 --- Time: 2.49s; --- Convergence: 6.31e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.067 value_reward_chosen^2 + -0.493 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 441/1000 --- L(Train): 0.3690581 --- L(Val, RNN): 0.3522120 --- L(Val, SINDy): 0.3534817 --- Time: 2.49s; --- Convergence: 4.75e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.066 value_reward_chosen^2 + -0.49 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 442/1000 --- L(Train): 0.3615276 --- L(Val, RNN): 0.3519511 --- L(Val, SINDy): 0.3534403 --- Time: 2.66s; --- Convergence: 3.68e-04; LR: 1.00e-02; Metric: 0.3518531; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.065 value_reward_chosen^2 + -0.487 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 443/1000 --- L(Train): 0.3700307 --- L(Val, RNN): 0.3513767 --- L(Val, SINDy): 0.3533671 --- Time: 2.60s; --- Convergence: 4.71e-04; LR: 1.00e-02; Metric: 0.3513767; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.062 value_reward_chosen^2 + -0.482 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 444/1000 --- L(Train): 0.3544205 --- L(Val, RNN): 0.3524422 --- L(Val, SINDy): 0.3532602 --- Time: 2.80s; --- Convergence: 7.68e-04; LR: 1.00e-02; Metric: 0.3513767; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.059 value_reward_chosen^2 + -0.478 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.82 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 445/1000 --- L(Train): 0.3604435 --- L(Val, RNN): 0.3518659 --- L(Val, SINDy): 0.3531636 --- Time: 2.59s; --- Convergence: 6.72e-04; LR: 1.00e-02; Metric: 0.3513767; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.516 reward + -0.056 value_reward_chosen^2 + -0.475 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.82 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 446/1000 --- L(Train): 0.3618716 --- L(Val, RNN): 0.3520196 --- L(Val, SINDy): 0.3531381 --- Time: 2.62s; --- Convergence: 4.13e-04; LR: 1.00e-02; Metric: 0.3513767; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.515 reward + -0.057 value_reward_chosen^2 + -0.475 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 447/1000 --- L(Train): 0.3652854 --- L(Val, RNN): 0.3516853 --- L(Val, SINDy): 0.3531472 --- Time: 2.53s; --- Convergence: 3.74e-04; LR: 1.00e-02; Metric: 0.3513767; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.512 reward + -0.058 value_reward_chosen^2 + -0.476 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 448/1000 --- L(Train): 0.3620054 --- L(Val, RNN): 0.3513125 --- L(Val, SINDy): 0.3531087 --- Time: 2.58s; --- Convergence: 3.73e-04; LR: 1.00e-02; Metric: 0.3513125; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.059 value_reward_chosen^2 + -0.477 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 449/1000 --- L(Train): 0.3670070 --- L(Val, RNN): 0.3514974 --- L(Val, SINDy): 0.3530616 --- Time: 2.52s; --- Convergence: 2.79e-04; LR: 1.00e-02; Metric: 0.3513125; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.507 reward + -0.062 value_reward_chosen^2 + -0.48 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 450/1000 --- L(Train): 0.3581064 --- L(Val, RNN): 0.3515884 --- L(Val, SINDy): 0.3530191 --- Time: 2.56s; --- Convergence: 1.85e-04; LR: 1.00e-02; Metric: 0.3513125; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.505 reward + -0.065 value_reward_chosen^2 + -0.482 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 451/1000 --- L(Train): 0.3554497 --- L(Val, RNN): 0.3512312 --- L(Val, SINDy): 0.3529579 --- Time: 2.69s; --- Convergence: 2.71e-04; LR: 1.00e-02; Metric: 0.3512312; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.066 value_reward_chosen^2 + -0.485 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 452/1000 --- L(Train): 0.3570979 --- L(Val, RNN): 0.3518133 --- L(Val, SINDy): 0.3529045 --- Time: 2.62s; --- Convergence: 4.27e-04; LR: 1.00e-02; Metric: 0.3512312; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.501 reward + -0.068 value_reward_chosen^2 + -0.486 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 453/1000 --- L(Train): 0.3581443 --- L(Val, RNN): 0.3512819 --- L(Val, SINDy): 0.3528388 --- Time: 2.60s; --- Convergence: 4.79e-04; LR: 1.00e-02; Metric: 0.3512312; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.501 reward + -0.068 value_reward_chosen^2 + -0.487 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 454/1000 --- L(Train): 0.3740890 --- L(Val, RNN): 0.3517427 --- L(Val, SINDy): 0.3528188 --- Time: 2.57s; --- Convergence: 4.70e-04; LR: 1.00e-02; Metric: 0.3512312; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.501 reward + -0.068 value_reward_chosen^2 + -0.487 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 455/1000 --- L(Train): 0.3653864 --- L(Val, RNN): 0.3512564 --- L(Val, SINDy): 0.3527891 --- Time: 2.61s; --- Convergence: 4.78e-04; LR: 1.00e-02; Metric: 0.3512312; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.5 reward + -0.069 value_reward_chosen^2 + -0.487 value_reward_chosen*reward + 0.5 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 456/1000 --- L(Train): 0.3530866 --- L(Val, RNN): 0.3505610 --- L(Val, SINDy): 0.3527089 --- Time: 2.47s; --- Convergence: 5.87e-04; LR: 1.00e-02; Metric: 0.3505610; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.502 reward + -0.068 value_reward_chosen^2 + -0.485 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 457/1000 --- L(Train): 0.3681779 --- L(Val, RNN): 0.3517191 --- L(Val, SINDy): 0.3525743 --- Time: 2.59s; --- Convergence: 8.72e-04; LR: 1.00e-02; Metric: 0.3505610; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.067 value_reward_chosen^2 + -0.484 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 458/1000 --- L(Train): 0.3725944 --- L(Val, RNN): 0.3505701 --- L(Val, SINDy): 0.3524617 --- Time: 2.61s; --- Convergence: 1.01e-03; LR: 1.00e-02; Metric: 0.3505610; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.504 reward + -0.066 value_reward_chosen^2 + -0.482 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 459/1000 --- L(Train): 0.3546516 --- L(Val, RNN): 0.3515359 --- L(Val, SINDy): 0.3524703 --- Time: 2.69s; --- Convergence: 9.88e-04; LR: 1.00e-02; Metric: 0.3505610; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.065 value_reward_chosen^2 + -0.481 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 460/1000 --- L(Train): 0.3639093 --- L(Val, RNN): 0.3510775 --- L(Val, SINDy): 0.3524897 --- Time: 2.72s; --- Convergence: 7.23e-04; LR: 1.00e-02; Metric: 0.3505610; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.507 reward + -0.063 value_reward_chosen^2 + -0.479 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 461/1000 --- L(Train): 0.3653347 --- L(Val, RNN): 0.3504051 --- L(Val, SINDy): 0.3524980 --- Time: 2.67s; --- Convergence: 6.98e-04; LR: 1.00e-02; Metric: 0.3504051; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.062 value_reward_chosen^2 + -0.478 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 462/1000 --- L(Train): 0.3621064 --- L(Val, RNN): 0.3514687 --- L(Val, SINDy): 0.3525350 --- Time: 2.58s; --- Convergence: 8.81e-04; LR: 1.00e-02; Metric: 0.3504051; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.509 reward + -0.062 value_reward_chosen^2 + -0.476 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 463/1000 --- L(Train): 0.3584552 --- L(Val, RNN): 0.3505076 --- L(Val, SINDy): 0.3524917 --- Time: 2.58s; --- Convergence: 9.21e-04; LR: 1.00e-02; Metric: 0.3504051; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.06 value_reward_chosen^2 + -0.474 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 464/1000 --- L(Train): 0.3631107 --- L(Val, RNN): 0.3513586 --- L(Val, SINDy): 0.3524927 --- Time: 2.48s; --- Convergence: 8.86e-04; LR: 1.00e-02; Metric: 0.3504051; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.06 value_reward_chosen^2 + -0.473 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 465/1000 --- L(Train): 0.3613004 --- L(Val, RNN): 0.3511766 --- L(Val, SINDy): 0.3524819 --- Time: 2.58s; --- Convergence: 5.34e-04; LR: 1.00e-02; Metric: 0.3504051; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.061 value_reward_chosen^2 + -0.472 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 466/1000 --- L(Train): 0.3564992 --- L(Val, RNN): 0.3499486 --- L(Val, SINDy): 0.3524927 --- Time: 2.58s; --- Convergence: 8.81e-04; LR: 1.00e-02; Metric: 0.3499486; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.061 value_reward_chosen^2 + -0.472 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 467/1000 --- L(Train): 0.3732956 --- L(Val, RNN): 0.3513868 --- L(Val, SINDy): 0.3524730 --- Time: 2.82s; --- Convergence: 1.16e-03; LR: 1.00e-02; Metric: 0.3499486; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.509 reward + -0.063 value_reward_chosen^2 + -0.472 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 468/1000 --- L(Train): 0.3470722 --- L(Val, RNN): 0.3501662 --- L(Val, SINDy): 0.3522660 --- Time: 2.49s; --- Convergence: 1.19e-03; LR: 1.00e-02; Metric: 0.3499486; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.062 value_reward_chosen^2 + -0.471 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 469/1000 --- L(Train): 0.3576261 --- L(Val, RNN): 0.3511941 --- L(Val, SINDy): 0.3520209 --- Time: 2.69s; --- Convergence: 1.11e-03; LR: 1.00e-02; Metric: 0.3499486; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.061 value_reward_chosen^2 + -0.47 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 470/1000 --- L(Train): 0.3587254 --- L(Val, RNN): 0.3510711 --- L(Val, SINDy): 0.3519538 --- Time: 2.56s; --- Convergence: 6.16e-04; LR: 1.00e-02; Metric: 0.3499486; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.512 reward + -0.059 value_reward_chosen^2 + -0.472 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 471/1000 --- L(Train): 0.3793560 --- L(Val, RNN): 0.3498547 --- L(Val, SINDy): 0.3518950 --- Time: 2.59s; --- Convergence: 9.16e-04; LR: 1.00e-02; Metric: 0.3498547; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.513 reward + -0.057 value_reward_chosen^2 + -0.474 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 472/1000 --- L(Train): 0.3594634 --- L(Val, RNN): 0.3510546 --- L(Val, SINDy): 0.3518735 --- Time: 2.54s; --- Convergence: 1.06e-03; LR: 1.00e-02; Metric: 0.3498547; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.059 value_reward_chosen^2 + -0.477 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 473/1000 --- L(Train): 0.3752450 --- L(Val, RNN): 0.3506665 --- L(Val, SINDy): 0.3518239 --- Time: 2.52s; --- Convergence: 7.23e-04; LR: 1.00e-02; Metric: 0.3498547; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.509 reward + -0.06 value_reward_chosen^2 + -0.48 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + 0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 474/1000 --- L(Train): 0.3476552 --- L(Val, RNN): 0.3500066 --- L(Val, SINDy): 0.3517864 --- Time: 2.66s; --- Convergence: 6.91e-04; LR: 1.00e-02; Metric: 0.3498547; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.061 value_reward_chosen^2 + -0.482 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.819 1 + -0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 475/1000 --- L(Train): 0.3588654 --- L(Val, RNN): 0.3510749 --- L(Val, SINDy): 0.3518068 --- Time: 2.68s; --- Convergence: 8.80e-04; LR: 1.00e-02; Metric: 0.3498547; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.507 reward + -0.062 value_reward_chosen^2 + -0.484 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 476/1000 --- L(Train): 0.3689399 --- L(Val, RNN): 0.3502054 --- L(Val, SINDy): 0.3517739 --- Time: 2.57s; --- Convergence: 8.75e-04; LR: 1.00e-02; Metric: 0.3498547; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.063 value_reward_chosen^2 + -0.485 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.818 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 477/1000 --- L(Train): 0.3518572 --- L(Val, RNN): 0.3496794 --- L(Val, SINDy): 0.3516988 --- Time: 2.53s; --- Convergence: 7.00e-04; LR: 1.00e-02; Metric: 0.3496794; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.063 value_reward_chosen^2 + -0.486 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 478/1000 --- L(Train): 0.3747384 --- L(Val, RNN): 0.3513477 --- L(Val, SINDy): 0.3516646 --- Time: 2.70s; --- Convergence: 1.18e-03; LR: 1.00e-02; Metric: 0.3496794; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.064 value_reward_chosen^2 + -0.486 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 479/1000 --- L(Train): 0.3630645 --- L(Val, RNN): 0.3502385 --- L(Val, SINDy): 0.3515910 --- Time: 2.53s; --- Convergence: 1.15e-03; LR: 1.00e-02; Metric: 0.3496794; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.063 value_reward_chosen^2 + -0.485 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 480/1000 --- L(Train): 0.3522772 --- L(Val, RNN): 0.3499355 --- L(Val, SINDy): 0.3515503 --- Time: 2.60s; --- Convergence: 7.25e-04; LR: 1.00e-02; Metric: 0.3496794; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.061 value_reward_chosen^2 + -0.482 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 481/1000 --- L(Train): 0.3599088 --- L(Val, RNN): 0.3511887 --- L(Val, SINDy): 0.3515602 --- Time: 2.56s; --- Convergence: 9.89e-04; LR: 1.00e-02; Metric: 0.3496794; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.059 value_reward_chosen^2 + -0.479 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 482/1000 --- L(Train): 0.3515720 --- L(Val, RNN): 0.3496525 --- L(Val, SINDy): 0.3515286 --- Time: 2.73s; --- Convergence: 1.26e-03; LR: 1.00e-02; Metric: 0.3496525; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.056 value_reward_chosen^2 + -0.475 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 483/1000 --- L(Train): 0.3773820 --- L(Val, RNN): 0.3492360 --- L(Val, SINDy): 0.3515078 --- Time: 2.78s; --- Convergence: 8.40e-04; LR: 1.00e-02; Metric: 0.3492360; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.516 reward + -0.053 value_reward_chosen^2 + -0.472 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 484/1000 --- L(Train): 0.3471593 --- L(Val, RNN): 0.3510927 --- L(Val, SINDy): 0.3514965 --- Time: 2.67s; --- Convergence: 1.35e-03; LR: 1.00e-02; Metric: 0.3492360; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.517 reward + -0.052 value_reward_chosen^2 + -0.47 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.006 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 485/1000 --- L(Train): 0.3718938 --- L(Val, RNN): 0.3502515 --- L(Val, SINDy): 0.3514210 --- Time: 2.65s; --- Convergence: 1.09e-03; LR: 1.00e-02; Metric: 0.3492360; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.518 reward + -0.052 value_reward_chosen^2 + -0.468 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 486/1000 --- L(Train): 0.3682115 --- L(Val, RNN): 0.3492844 --- L(Val, SINDy): 0.3513016 --- Time: 2.72s; --- Convergence: 1.03e-03; LR: 1.00e-02; Metric: 0.3492360; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.516 reward + -0.055 value_reward_chosen^2 + -0.469 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 487/1000 --- L(Train): 0.3572086 --- L(Val, RNN): 0.3506033 --- L(Val, SINDy): 0.3512941 --- Time: 2.53s; --- Convergence: 1.17e-03; LR: 1.00e-02; Metric: 0.3492360; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.513 reward + -0.058 value_reward_chosen^2 + -0.47 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 488/1000 --- L(Train): 0.3591812 --- L(Val, RNN): 0.3508600 --- L(Val, SINDy): 0.3513120 --- Time: 2.58s; --- Convergence: 7.16e-04; LR: 1.00e-02; Metric: 0.3492360; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.061 value_reward_chosen^2 + -0.471 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 489/1000 --- L(Train): 0.3572298 --- L(Val, RNN): 0.3490204 --- L(Val, SINDy): 0.3512957 --- Time: 2.62s; --- Convergence: 1.28e-03; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.062 value_reward_chosen^2 + -0.471 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 490/1000 --- L(Train): 0.3780526 --- L(Val, RNN): 0.3497068 --- L(Val, SINDy): 0.3512778 --- Time: 2.85s; --- Convergence: 9.82e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.508 reward + -0.065 value_reward_chosen^2 + -0.47 value_reward_chosen*reward + 0.507 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 491/1000 --- L(Train): 0.3636561 --- L(Val, RNN): 0.3509564 --- L(Val, SINDy): 0.3512651 --- Time: 2.53s; --- Convergence: 1.12e-03; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.506 reward + -0.068 value_reward_chosen^2 + -0.469 value_reward_chosen*reward + 0.505 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 492/1000 --- L(Train): 0.3610697 --- L(Val, RNN): 0.3496372 --- L(Val, SINDy): 0.3512138 --- Time: 2.62s; --- Convergence: 1.22e-03; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.505 reward + -0.07 value_reward_chosen^2 + -0.468 value_reward_chosen*reward + 0.504 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 493/1000 --- L(Train): 0.3656891 --- L(Val, RNN): 0.3494248 --- L(Val, SINDy): 0.3511962 --- Time: 2.58s; --- Convergence: 7.15e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.504 reward + -0.071 value_reward_chosen^2 + -0.465 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 494/1000 --- L(Train): 0.3609914 --- L(Val, RNN): 0.3506317 --- L(Val, SINDy): 0.3511848 --- Time: 2.72s; --- Convergence: 9.61e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.072 value_reward_chosen^2 + -0.463 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 495/1000 --- L(Train): 0.3604594 --- L(Val, RNN): 0.3496223 --- L(Val, SINDy): 0.3511479 --- Time: 2.55s; --- Convergence: 9.85e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.503 reward + -0.073 value_reward_chosen^2 + -0.462 value_reward_chosen*reward + 0.502 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 496/1000 --- L(Train): 0.3658107 --- L(Val, RNN): 0.3492424 --- L(Val, SINDy): 0.3511125 --- Time: 2.54s; --- Convergence: 6.83e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.502 reward + -0.074 value_reward_chosen^2 + -0.46 value_reward_chosen*reward + 0.501 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.813 1 + -0.005 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 497/1000 --- L(Train): 0.3762561 --- L(Val, RNN): 0.3498420 --- L(Val, SINDy): 0.3510597 --- Time: 2.66s; --- Convergence: 6.41e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 8/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.504 reward + -0.073 value_reward_chosen^2 + -0.456 value_reward_chosen*reward + 0.503 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.004 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 498/1000 --- L(Train): 0.3569716 --- L(Val, RNN): 0.3495178 --- L(Val, SINDy): 0.3509685 --- Time: 2.84s; --- Convergence: 4.83e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 9/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.507 reward + -0.07 value_reward_chosen^2 + -0.452 value_reward_chosen*reward + 0.506 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 499/1000 --- L(Train): 0.3759317 --- L(Val, RNN): 0.3496606 --- L(Val, SINDy): 0.3508960 --- Time: 2.61s; --- Convergence: 3.13e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 10/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.509 reward + -0.068 value_reward_chosen^2 + -0.449 value_reward_chosen*reward + 0.508 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 500/1000 --- L(Train): 0.3590028 --- L(Val, RNN): 0.3502464 --- L(Val, SINDy): 0.3508684 --- Time: 2.61s; --- Convergence: 4.49e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 11/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.066 value_reward_chosen^2 + -0.446 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 501/1000 --- L(Train): 0.3613947 --- L(Val, RNN): 0.3492033 --- L(Val, SINDy): 0.3508123 --- Time: 2.71s; --- Convergence: 7.46e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 12/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.512 reward + -0.064 value_reward_chosen^2 + -0.447 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 502/1000 --- L(Train): 0.3661877 --- L(Val, RNN): 0.3492657 --- L(Val, SINDy): 0.3507726 --- Time: 2.59s; --- Convergence: 4.04e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 13/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.062 value_reward_chosen^2 + -0.45 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + 0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 503/1000 --- L(Train): 0.3586603 --- L(Val, RNN): 0.3494815 --- L(Val, SINDy): 0.3507436 --- Time: 2.58s; --- Convergence: 3.10e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 14/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.515 reward + -0.06 value_reward_chosen^2 + -0.453 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 504/1000 --- L(Train): 0.3649685 --- L(Val, RNN): 0.3491384 --- L(Val, SINDy): 0.3507147 --- Time: 2.51s; --- Convergence: 3.27e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 15/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.516 reward + -0.058 value_reward_chosen^2 + -0.456 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 505/1000 --- L(Train): 0.3654864 --- L(Val, RNN): 0.3496830 --- L(Val, SINDy): 0.3506983 --- Time: 2.75s; --- Convergence: 4.36e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 16/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.518 reward + -0.057 value_reward_chosen^2 + -0.458 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 506/1000 --- L(Train): 0.3602188 --- L(Val, RNN): 0.3497650 --- L(Val, SINDy): 0.3506744 --- Time: 2.77s; --- Convergence: 2.59e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 17/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.519 reward + -0.055 value_reward_chosen^2 + -0.46 value_reward_chosen*reward + 0.518 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 507/1000 --- L(Train): 0.3720734 --- L(Val, RNN): 0.3495677 --- L(Val, SINDy): 0.3507028 --- Time: 2.62s; --- Convergence: 2.28e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 18/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.518 reward + -0.055 value_reward_chosen^2 + -0.463 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 508/1000 --- L(Train): 0.3504514 --- L(Val, RNN): 0.3493380 --- L(Val, SINDy): 0.3507458 --- Time: 2.59s; --- Convergence: 2.29e-04; LR: 1.00e-02; Metric: 0.3490204; Bad epochs: 19/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.517 reward + -0.056 value_reward_chosen^2 + -0.467 value_reward_chosen*reward + 0.516 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 509/1000 --- L(Train): 0.3477720 --- L(Val, RNN): 0.3487671 --- L(Val, SINDy): 0.3507638 --- Time: 2.70s; --- Convergence: 4.00e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.516 reward + -0.056 value_reward_chosen^2 + -0.47 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 510/1000 --- L(Train): 0.3483509 --- L(Val, RNN): 0.3488083 --- L(Val, SINDy): 0.3506855 --- Time: 2.57s; --- Convergence: 2.21e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.515 reward + -0.057 value_reward_chosen^2 + -0.473 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 511/1000 --- L(Train): 0.3509423 --- L(Val, RNN): 0.3492211 --- L(Val, SINDy): 0.3506292 --- Time: 2.59s; --- Convergence: 3.17e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.057 value_reward_chosen^2 + -0.476 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 512/1000 --- L(Train): 0.3698497 --- L(Val, RNN): 0.3493065 --- L(Val, SINDy): 0.3505346 --- Time: 2.57s; --- Convergence: 2.01e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.513 reward + -0.059 value_reward_chosen^2 + -0.478 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 513/1000 --- L(Train): 0.3560689 --- L(Val, RNN): 0.3492724 --- L(Val, SINDy): 0.3504613 --- Time: 2.82s; --- Convergence: 1.18e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.512 reward + -0.059 value_reward_chosen^2 + -0.479 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 514/1000 --- L(Train): 0.3605075 --- L(Val, RNN): 0.3496739 --- L(Val, SINDy): 0.3504450 --- Time: 2.62s; --- Convergence: 2.60e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.061 value_reward_chosen^2 + -0.48 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 515/1000 --- L(Train): 0.3516160 --- L(Val, RNN): 0.3491966 --- L(Val, SINDy): 0.3504374 --- Time: 2.63s; --- Convergence: 3.68e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.061 value_reward_chosen^2 + -0.477 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 516/1000 --- L(Train): 0.3622784 --- L(Val, RNN): 0.3488268 --- L(Val, SINDy): 0.3504212 --- Time: 2.63s; --- Convergence: 3.69e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.061 value_reward_chosen^2 + -0.475 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 517/1000 --- L(Train): 0.3577752 --- L(Val, RNN): 0.3495190 --- L(Val, SINDy): 0.3503820 --- Time: 2.69s; --- Convergence: 5.31e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 8/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.061 value_reward_chosen^2 + -0.473 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 518/1000 --- L(Train): 0.3785051 --- L(Val, RNN): 0.3491478 --- L(Val, SINDy): 0.3503532 --- Time: 2.50s; --- Convergence: 4.51e-04; LR: 1.00e-02; Metric: 0.3487671; Bad epochs: 9/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.06 value_reward_chosen^2 + -0.47 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 519/1000 --- L(Train): 0.3589855 --- L(Val, RNN): 0.3485802 --- L(Val, SINDy): 0.3503313 --- Time: 2.52s; --- Convergence: 5.09e-04; LR: 1.00e-02; Metric: 0.3485802; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.512 reward + -0.06 value_reward_chosen^2 + -0.467 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 520/1000 --- L(Train): 0.3615501 --- L(Val, RNN): 0.3495995 --- L(Val, SINDy): 0.3503679 --- Time: 2.67s; --- Convergence: 7.64e-04; LR: 1.00e-02; Metric: 0.3485802; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.513 reward + -0.059 value_reward_chosen^2 + -0.464 value_reward_chosen*reward + 0.512 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 521/1000 --- L(Train): 0.3465848 --- L(Val, RNN): 0.3487584 --- L(Val, SINDy): 0.3503639 --- Time: 2.77s; --- Convergence: 8.03e-04; LR: 1.00e-02; Metric: 0.3485802; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.058 value_reward_chosen^2 + -0.461 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 522/1000 --- L(Train): 0.3673467 --- L(Val, RNN): 0.3480982 --- L(Val, SINDy): 0.3502929 --- Time: 2.66s; --- Convergence: 7.31e-04; LR: 1.00e-02; Metric: 0.3480982; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.058 value_reward_chosen^2 + -0.459 value_reward_chosen*reward + 0.514 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 523/1000 --- L(Train): 0.3626011 --- L(Val, RNN): 0.3494533 --- L(Val, SINDy): 0.3502260 --- Time: 2.57s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: 0.3480982; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.513 reward + -0.06 value_reward_chosen^2 + -0.456 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 524/1000 --- L(Train): 0.3599086 --- L(Val, RNN): 0.3489407 --- L(Val, SINDy): 0.3501221 --- Time: 2.67s; --- Convergence: 7.78e-04; LR: 1.00e-02; Metric: 0.3480982; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.512 reward + -0.061 value_reward_chosen^2 + -0.454 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 525/1000 --- L(Train): 0.3503619 --- L(Val, RNN): 0.3485214 --- L(Val, SINDy): 0.3500410 --- Time: 2.64s; --- Convergence: 5.99e-04; LR: 1.00e-02; Metric: 0.3480982; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.063 value_reward_chosen^2 + -0.452 value_reward_chosen*reward + 0.511 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 526/1000 --- L(Train): 0.3553542 --- L(Val, RNN): 0.3497800 --- L(Val, SINDy): 0.3500503 --- Time: 2.59s; --- Convergence: 9.29e-04; LR: 1.00e-02; Metric: 0.3480982; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.064 value_reward_chosen^2 + -0.45 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 527/1000 --- L(Train): 0.3771125 --- L(Val, RNN): 0.3485985 --- L(Val, SINDy): 0.3500542 --- Time: 2.50s; --- Convergence: 1.06e-03; LR: 1.00e-02; Metric: 0.3480982; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.065 value_reward_chosen^2 + -0.448 value_reward_chosen*reward + 0.509 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 528/1000 --- L(Train): 0.3702904 --- L(Val, RNN): 0.3476736 --- L(Val, SINDy): 0.3500160 --- Time: 2.71s; --- Convergence: 9.90e-04; LR: 1.00e-02; Metric: 0.3476736; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.51 reward + -0.064 value_reward_chosen^2 + -0.445 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 529/1000 --- L(Train): 0.3448412 --- L(Val, RNN): 0.3490863 --- L(Val, SINDy): 0.3499766 --- Time: 2.80s; --- Convergence: 1.20e-03; LR: 1.00e-02; Metric: 0.3476736; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.511 reward + -0.064 value_reward_chosen^2 + -0.442 value_reward_chosen*reward + 0.51 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.814 1 + -0.003 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 530/1000 --- L(Train): 0.3599856 --- L(Val, RNN): 0.3490945 --- L(Val, SINDy): 0.3499244 --- Time: 2.62s; --- Convergence: 6.05e-04; LR: 1.00e-02; Metric: 0.3476736; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.514 reward + -0.06 value_reward_chosen^2 + -0.441 value_reward_chosen*reward + 0.513 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.815 1 + -0.002 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 531/1000 --- L(Train): 0.3457819 --- L(Val, RNN): 0.3479940 --- L(Val, SINDy): 0.3498879 --- Time: 2.57s; --- Convergence: 8.53e-04; LR: 1.00e-02; Metric: 0.3476736; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.516 reward + -0.056 value_reward_chosen^2 + -0.442 value_reward_chosen*reward + 0.515 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.816 1 + -0.001 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 532/1000 --- L(Train): 0.3625834 --- L(Val, RNN): 0.3487987 --- L(Val, SINDy): 0.3499302 --- Time: 2.78s; --- Convergence: 8.29e-04; LR: 1.00e-02; Metric: 0.3476736; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.518 reward + -0.053 value_reward_chosen^2 + -0.442 value_reward_chosen*reward + 0.517 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.817 1 + -0.0 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, -, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\n",
      "Training interrupted. Continuing with further operations...\n",
      "\n",
      "================================================================================\n",
      "Starting second stage SINDy fitting (threshold=0.05, single model)\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 1/1000 --- L(Train): 0.0858423 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.009 1 + 0.991 value_reward_chosen[t] + 0.01 reward + 0.013 value_reward_chosen^2 + -0.009 value_reward_chosen*reward + 0.009 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.01 1 + 0.99 value_reward_not_chosen[t] + -0.009 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.009 1 + 1.009 value_reward_not_displayed[t] + -0.01 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 2/1000 --- L(Train): 0.0823888 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.019 1 + 0.981 value_reward_chosen[t] + 0.02 reward + 0.023 value_reward_chosen^2 + -0.019 value_reward_chosen*reward + 0.019 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.02 1 + 0.98 value_reward_not_chosen[t] + -0.019 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.017 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 3/1000 --- L(Train): 0.0783558 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.029 1 + 0.971 value_reward_chosen[t] + 0.03 reward + 0.031 value_reward_chosen^2 + -0.029 value_reward_chosen*reward + 0.029 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.03 1 + 0.97 value_reward_not_chosen[t] + -0.029 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.004 1 + 1.012 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 4/1000 --- L(Train): 0.0748734 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.038 1 + 0.961 value_reward_chosen[t] + 0.04 reward + 0.036 value_reward_chosen^2 + -0.039 value_reward_chosen*reward + 0.039 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.04 1 + 0.96 value_reward_not_chosen[t] + -0.039 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.006 1 + 1.005 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 5/1000 --- L(Train): 0.0716601 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.048 1 + 0.951 value_reward_chosen[t] + 0.05 reward + 0.037 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.049 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.05 1 + 0.95 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.007 1 + 0.998 value_reward_not_displayed[t] + -0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 6/1000 --- L(Train): 0.0684724 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.057 1 + 0.94 value_reward_chosen[t] + 0.059 reward + 0.037 value_reward_chosen^2 + -0.059 value_reward_chosen*reward + 0.058 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.06 1 + 0.94 value_reward_not_chosen[t] + -0.058 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.006 1 + 0.993 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 7/1000 --- L(Train): 0.0654129 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.067 1 + 0.93 value_reward_chosen[t] + 0.069 reward + 0.035 value_reward_chosen^2 + -0.069 value_reward_chosen*reward + 0.068 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.069 1 + 0.93 value_reward_not_chosen[t] + -0.068 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.005 1 + 0.992 value_reward_not_displayed[t] + -0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 8/1000 --- L(Train): 0.0625579 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.076 1 + 0.92 value_reward_chosen[t] + 0.079 reward + 0.032 value_reward_chosen^2 + -0.079 value_reward_chosen*reward + 0.077 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.079 1 + 0.92 value_reward_not_chosen[t] + -0.078 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.005 1 + 0.993 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 9/1000 --- L(Train): 0.0599101 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.085 1 + 0.911 value_reward_chosen[t] + 0.088 reward + 0.029 value_reward_chosen^2 + -0.088 value_reward_chosen*reward + 0.087 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.088 1 + 0.91 value_reward_not_chosen[t] + -0.087 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.004 1 + 0.995 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 10/1000 --- L(Train): 0.0573923 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.094 1 + 0.901 value_reward_chosen[t] + 0.098 reward + 0.027 value_reward_chosen^2 + -0.097 value_reward_chosen*reward + 0.096 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.098 1 + 0.9 value_reward_not_chosen[t] + -0.097 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.998 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 11/1000 --- L(Train): 0.0549619 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.103 1 + 0.892 value_reward_chosen[t] + 0.107 reward + 0.026 value_reward_chosen^2 + -0.105 value_reward_chosen*reward + 0.106 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.107 1 + 0.889 value_reward_not_chosen[t] + -0.106 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.004 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 12/1000 --- L(Train): 0.0526335 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.112 1 + 0.884 value_reward_chosen[t] + 0.117 reward + 0.025 value_reward_chosen^2 + -0.111 value_reward_chosen*reward + 0.115 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.115 1 + 0.879 value_reward_not_chosen[t] + -0.115 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.006 1 + 1.002 value_reward_not_displayed[t] + 0.003 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 13/1000 --- L(Train): 0.0504478 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.121 1 + 0.876 value_reward_chosen[t] + 0.126 reward + 0.025 value_reward_chosen^2 + -0.117 value_reward_chosen*reward + 0.124 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.124 1 + 0.869 value_reward_not_chosen[t] + -0.124 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 1.002 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 14/1000 --- L(Train): 0.0484294 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.13 1 + 0.868 value_reward_chosen[t] + 0.136 reward + 0.026 value_reward_chosen^2 + -0.122 value_reward_chosen*reward + 0.134 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.132 1 + 0.859 value_reward_not_chosen[t] + -0.133 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.01 1 + 1.002 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 15/1000 --- L(Train): 0.0465371 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.138 1 + 0.861 value_reward_chosen[t] + 0.145 reward + 0.026 value_reward_chosen^2 + -0.126 value_reward_chosen*reward + 0.143 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.14 1 + 0.849 value_reward_not_chosen[t] + -0.142 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.01 1 + 1.002 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 16/1000 --- L(Train): 0.0447282 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.146 1 + 0.854 value_reward_chosen[t] + 0.154 reward + 0.026 value_reward_chosen^2 + -0.13 value_reward_chosen*reward + 0.152 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.148 1 + 0.839 value_reward_not_chosen[t] + -0.15 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.008 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 17/1000 --- L(Train): 0.0429951 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.154 1 + 0.847 value_reward_chosen[t] + 0.163 reward + 0.026 value_reward_chosen^2 + -0.134 value_reward_chosen*reward + 0.161 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.155 1 + 0.829 value_reward_not_chosen[t] + -0.159 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.005 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 18/1000 --- L(Train): 0.0413579 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.162 1 + 0.84 value_reward_chosen[t] + 0.172 reward + 0.024 value_reward_chosen^2 + -0.137 value_reward_chosen*reward + 0.169 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.162 1 + 0.819 value_reward_not_chosen[t] + -0.167 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 19/1000 --- L(Train): 0.0398365 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.169 1 + 0.834 value_reward_chosen[t] + 0.181 reward + 0.021 value_reward_chosen^2 + -0.141 value_reward_chosen*reward + 0.178 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.169 1 + 0.809 value_reward_not_chosen[t] + -0.175 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.998 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 20/1000 --- L(Train): 0.0384181 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.176 1 + 0.827 value_reward_chosen[t] + 0.189 reward + 0.017 value_reward_chosen^2 + -0.144 value_reward_chosen*reward + 0.186 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.175 1 + 0.798 value_reward_not_chosen[t] + -0.183 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 21/1000 --- L(Train): 0.0370732 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.183 1 + 0.822 value_reward_chosen[t] + 0.197 reward + 0.013 value_reward_chosen^2 + -0.147 value_reward_chosen*reward + 0.194 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.181 1 + 0.788 value_reward_not_chosen[t] + -0.19 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.998 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 22/1000 --- L(Train): 0.0357880 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.189 1 + 0.816 value_reward_chosen[t] + 0.205 reward + 0.009 value_reward_chosen^2 + -0.15 value_reward_chosen*reward + 0.202 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.187 1 + 0.778 value_reward_not_chosen[t] + -0.197 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.003 1 + 0.998 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 23/1000 --- L(Train): 0.0345683 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.195 1 + 0.811 value_reward_chosen[t] + 0.213 reward + 0.005 value_reward_chosen^2 + -0.152 value_reward_chosen*reward + 0.21 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.192 1 + 0.768 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 0.999 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 24/1000 --- L(Train): 0.0334237 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.201 1 + 0.807 value_reward_chosen[t] + 0.221 reward + 0.001 value_reward_chosen^2 + -0.154 value_reward_chosen*reward + 0.218 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.196 1 + 0.758 value_reward_not_chosen[t] + -0.211 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 1.001 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 25/1000 --- L(Train): 0.0323530 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.207 1 + 0.804 value_reward_chosen[t] + 0.229 reward + -0.003 value_reward_chosen^2 + -0.155 value_reward_chosen*reward + 0.225 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.201 1 + 0.748 value_reward_not_chosen[t] + -0.218 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.002 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 26/1000 --- L(Train): 0.0313410 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.212 1 + 0.801 value_reward_chosen[t] + 0.236 reward + -0.006 value_reward_chosen^2 + -0.155 value_reward_chosen*reward + 0.232 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.204 1 + 0.738 value_reward_not_chosen[t] + -0.224 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.003 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 27/1000 --- L(Train): 0.0303754 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.217 1 + 0.799 value_reward_chosen[t] + 0.244 reward + -0.008 value_reward_chosen^2 + -0.156 value_reward_chosen*reward + 0.239 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.208 1 + 0.728 value_reward_not_chosen[t] + -0.23 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.003 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 28/1000 --- L(Train): 0.0294557 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.221 1 + 0.797 value_reward_chosen[t] + 0.251 reward + -0.009 value_reward_chosen^2 + -0.156 value_reward_chosen*reward + 0.246 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.211 1 + 0.718 value_reward_not_chosen[t] + -0.236 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.003 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 29/1000 --- L(Train): 0.0285875 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.225 1 + 0.795 value_reward_chosen[t] + 0.257 reward + -0.011 value_reward_chosen^2 + -0.156 value_reward_chosen*reward + 0.253 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.214 1 + 0.708 value_reward_not_chosen[t] + -0.241 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.002 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 30/1000 --- L(Train): 0.0277710 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.229 1 + 0.794 value_reward_chosen[t] + 0.264 reward + -0.013 value_reward_chosen^2 + -0.157 value_reward_chosen*reward + 0.259 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.216 1 + 0.698 value_reward_not_chosen[t] + -0.247 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 31/1000 --- L(Train): 0.0269982 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.232 1 + 0.793 value_reward_chosen[t] + 0.27 reward + -0.015 value_reward_chosen^2 + -0.159 value_reward_chosen*reward + 0.265 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.218 1 + 0.688 value_reward_not_chosen[t] + -0.252 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 32/1000 --- L(Train): 0.0262602 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.234 1 + 0.792 value_reward_chosen[t] + 0.276 reward + -0.019 value_reward_chosen^2 + -0.162 value_reward_chosen*reward + 0.271 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.22 1 + 0.679 value_reward_not_chosen[t] + -0.257 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 33/1000 --- L(Train): 0.0255555 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.236 1 + 0.791 value_reward_chosen[t] + 0.282 reward + -0.023 value_reward_chosen^2 + -0.164 value_reward_chosen*reward + 0.276 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.222 1 + 0.669 value_reward_not_chosen[t] + -0.262 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 34/1000 --- L(Train): 0.0248861 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.238 1 + 0.79 value_reward_chosen[t] + 0.287 reward + -0.027 value_reward_chosen^2 + -0.168 value_reward_chosen*reward + 0.281 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.223 1 + 0.66 value_reward_not_chosen[t] + -0.266 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 35/1000 --- L(Train): 0.0242528 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.239 1 + 0.79 value_reward_chosen[t] + 0.292 reward + -0.031 value_reward_chosen^2 + -0.171 value_reward_chosen*reward + 0.286 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.224 1 + 0.65 value_reward_not_chosen[t] + -0.271 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 36/1000 --- L(Train): 0.0236498 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.24 1 + 0.791 value_reward_chosen[t] + 0.297 reward + -0.034 value_reward_chosen^2 + -0.174 value_reward_chosen*reward + 0.291 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.226 1 + 0.641 value_reward_not_chosen[t] + -0.275 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 37/1000 --- L(Train): 0.0230718 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.241 1 + 0.792 value_reward_chosen[t] + 0.302 reward + -0.036 value_reward_chosen^2 + -0.178 value_reward_chosen*reward + 0.295 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.227 1 + 0.631 value_reward_not_chosen[t] + -0.279 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 38/1000 --- L(Train): 0.0225178 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.241 1 + 0.793 value_reward_chosen[t] + 0.307 reward + -0.038 value_reward_chosen^2 + -0.18 value_reward_chosen*reward + 0.299 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.227 1 + 0.622 value_reward_not_chosen[t] + -0.283 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.002 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 39/1000 --- L(Train): 0.0219904 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.241 1 + 0.796 value_reward_chosen[t] + 0.311 reward + -0.039 value_reward_chosen^2 + -0.183 value_reward_chosen*reward + 0.303 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.228 1 + 0.613 value_reward_not_chosen[t] + -0.286 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 40/1000 --- L(Train): 0.0214876 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.241 1 + 0.798 value_reward_chosen[t] + 0.315 reward + -0.04 value_reward_chosen^2 + -0.185 value_reward_chosen*reward + 0.307 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.229 1 + 0.604 value_reward_not_chosen[t] + -0.29 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 41/1000 --- L(Train): 0.0210056 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.24 1 + 0.801 value_reward_chosen[t] + 0.319 reward + -0.041 value_reward_chosen^2 + -0.188 value_reward_chosen*reward + 0.311 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.23 1 + 0.595 value_reward_not_chosen[t] + -0.293 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 42/1000 --- L(Train): 0.0205426 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.24 1 + 0.804 value_reward_chosen[t] + 0.323 reward + -0.042 value_reward_chosen^2 + -0.191 value_reward_chosen*reward + 0.314 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.231 1 + 0.587 value_reward_not_chosen[t] + -0.297 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 43/1000 --- L(Train): 0.0200991 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.238 1 + 0.807 value_reward_chosen[t] + 0.327 reward + -0.043 value_reward_chosen^2 + -0.195 value_reward_chosen*reward + 0.318 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.232 1 + 0.578 value_reward_not_chosen[t] + -0.3 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 44/1000 --- L(Train): 0.0196746 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.237 1 + 0.81 value_reward_chosen[t] + 0.33 reward + -0.045 value_reward_chosen^2 + -0.199 value_reward_chosen*reward + 0.32 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.233 1 + 0.57 value_reward_not_chosen[t] + -0.303 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 45/1000 --- L(Train): 0.0192672 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.235 1 + 0.813 value_reward_chosen[t] + 0.334 reward + -0.047 value_reward_chosen^2 + -0.203 value_reward_chosen*reward + 0.323 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.234 1 + 0.561 value_reward_not_chosen[t] + -0.307 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 46/1000 --- L(Train): 0.0188757 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.233 1 + 0.816 value_reward_chosen[t] + 0.337 reward + -0.049 value_reward_chosen^2 + -0.207 value_reward_chosen*reward + 0.326 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.235 1 + 0.553 value_reward_not_chosen[t] + -0.31 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 47/1000 --- L(Train): 0.0184996 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.231 1 + 0.82 value_reward_chosen[t] + 0.34 reward + -0.051 value_reward_chosen^2 + -0.211 value_reward_chosen*reward + 0.328 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.236 1 + 0.545 value_reward_not_chosen[t] + -0.313 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 48/1000 --- L(Train): 0.0181389 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.229 1 + 0.824 value_reward_chosen[t] + 0.342 reward + -0.053 value_reward_chosen^2 + -0.215 value_reward_chosen*reward + 0.331 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.238 1 + 0.538 value_reward_not_chosen[t] + -0.315 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 49/1000 --- L(Train): 0.0177932 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.226 1 + 0.828 value_reward_chosen[t] + 0.345 reward + -0.054 value_reward_chosen^2 + -0.219 value_reward_chosen*reward + 0.333 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.239 1 + 0.53 value_reward_not_chosen[t] + -0.318 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 50/1000 --- L(Train): 0.0174599 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.224 1 + 0.832 value_reward_chosen[t] + 0.348 reward + -0.055 value_reward_chosen^2 + -0.223 value_reward_chosen*reward + 0.335 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.241 1 + 0.522 value_reward_not_chosen[t] + -0.321 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 51/1000 --- L(Train): 0.0171401 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.221 1 + 0.837 value_reward_chosen[t] + 0.35 reward + -0.056 value_reward_chosen^2 + -0.226 value_reward_chosen*reward + 0.337 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.243 1 + 0.515 value_reward_not_chosen[t] + -0.324 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 52/1000 --- L(Train): 0.0168339 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.218 1 + 0.841 value_reward_chosen[t] + 0.353 reward + -0.056 value_reward_chosen^2 + -0.23 value_reward_chosen*reward + 0.339 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.245 1 + 0.508 value_reward_not_chosen[t] + -0.326 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 53/1000 --- L(Train): 0.0165395 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.215 1 + 0.846 value_reward_chosen[t] + 0.355 reward + -0.056 value_reward_chosen^2 + -0.233 value_reward_chosen*reward + 0.341 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.247 1 + 0.501 value_reward_not_chosen[t] + -0.329 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 54/1000 --- L(Train): 0.0162562 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.212 1 + 0.85 value_reward_chosen[t] + 0.357 reward + -0.057 value_reward_chosen^2 + -0.237 value_reward_chosen*reward + 0.342 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.25 1 + 0.494 value_reward_not_chosen[t] + -0.332 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 55/1000 --- L(Train): 0.0159845 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.209 1 + 0.855 value_reward_chosen[t] + 0.36 reward + -0.058 value_reward_chosen^2 + -0.241 value_reward_chosen*reward + 0.344 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.252 1 + 0.487 value_reward_not_chosen[t] + -0.334 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 56/1000 --- L(Train): 0.0157240 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.206 1 + 0.859 value_reward_chosen[t] + 0.362 reward + -0.059 value_reward_chosen^2 + -0.245 value_reward_chosen*reward + 0.346 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.255 1 + 0.48 value_reward_not_chosen[t] + -0.336 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 57/1000 --- L(Train): 0.0154738 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.202 1 + 0.863 value_reward_chosen[t] + 0.364 reward + -0.06 value_reward_chosen^2 + -0.249 value_reward_chosen*reward + 0.347 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.257 1 + 0.474 value_reward_not_chosen[t] + -0.339 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 58/1000 --- L(Train): 0.0152340 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.199 1 + 0.867 value_reward_chosen[t] + 0.366 reward + -0.061 value_reward_chosen^2 + -0.253 value_reward_chosen*reward + 0.348 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.26 1 + 0.467 value_reward_not_chosen[t] + -0.341 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 59/1000 --- L(Train): 0.0150042 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.196 1 + 0.871 value_reward_chosen[t] + 0.368 reward + -0.062 value_reward_chosen^2 + -0.256 value_reward_chosen*reward + 0.35 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.263 1 + 0.461 value_reward_not_chosen[t] + -0.343 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 60/1000 --- L(Train): 0.0147832 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.192 1 + 0.876 value_reward_chosen[t] + 0.37 reward + -0.062 value_reward_chosen^2 + -0.26 value_reward_chosen*reward + 0.351 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.266 1 + 0.455 value_reward_not_chosen[t] + -0.345 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 61/1000 --- L(Train): 0.0145714 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.189 1 + 0.88 value_reward_chosen[t] + 0.372 reward + -0.063 value_reward_chosen^2 + -0.263 value_reward_chosen*reward + 0.353 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.269 1 + 0.449 value_reward_not_chosen[t] + -0.347 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 62/1000 --- L(Train): 0.0143685 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.186 1 + 0.884 value_reward_chosen[t] + 0.373 reward + -0.063 value_reward_chosen^2 + -0.266 value_reward_chosen*reward + 0.354 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.272 1 + 0.443 value_reward_not_chosen[t] + -0.348 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 63/1000 --- L(Train): 0.0141739 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.183 1 + 0.888 value_reward_chosen[t] + 0.375 reward + -0.063 value_reward_chosen^2 + -0.269 value_reward_chosen*reward + 0.355 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.275 1 + 0.437 value_reward_not_chosen[t] + -0.35 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 64/1000 --- L(Train): 0.0139878 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.179 1 + 0.892 value_reward_chosen[t] + 0.377 reward + -0.063 value_reward_chosen^2 + -0.272 value_reward_chosen*reward + 0.356 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.278 1 + 0.432 value_reward_not_chosen[t] + -0.352 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 65/1000 --- L(Train): 0.0138089 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.176 1 + 0.895 value_reward_chosen[t] + 0.379 reward + -0.063 value_reward_chosen^2 + -0.275 value_reward_chosen*reward + 0.358 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.281 1 + 0.426 value_reward_not_chosen[t] + -0.353 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 66/1000 --- L(Train): 0.0136377 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.173 1 + 0.899 value_reward_chosen[t] + 0.381 reward + -0.064 value_reward_chosen^2 + -0.278 value_reward_chosen*reward + 0.359 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.284 1 + 0.421 value_reward_not_chosen[t] + -0.354 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 67/1000 --- L(Train): 0.0134737 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.17 1 + 0.902 value_reward_chosen[t] + 0.383 reward + -0.064 value_reward_chosen^2 + -0.281 value_reward_chosen*reward + 0.36 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.287 1 + 0.415 value_reward_not_chosen[t] + -0.356 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 68/1000 --- L(Train): 0.0133165 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.166 1 + 0.905 value_reward_chosen[t] + 0.385 reward + -0.065 value_reward_chosen^2 + -0.285 value_reward_chosen*reward + 0.362 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.29 1 + 0.41 value_reward_not_chosen[t] + -0.357 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 69/1000 --- L(Train): 0.0131655 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.163 1 + 0.908 value_reward_chosen[t] + 0.387 reward + -0.065 value_reward_chosen^2 + -0.287 value_reward_chosen*reward + 0.363 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.293 1 + 0.405 value_reward_not_chosen[t] + -0.357 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 70/1000 --- L(Train): 0.0130207 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.16 1 + 0.911 value_reward_chosen[t] + 0.389 reward + -0.066 value_reward_chosen^2 + -0.29 value_reward_chosen*reward + 0.364 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.296 1 + 0.4 value_reward_not_chosen[t] + -0.358 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 71/1000 --- L(Train): 0.0128821 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.157 1 + 0.913 value_reward_chosen[t] + 0.391 reward + -0.066 value_reward_chosen^2 + -0.293 value_reward_chosen*reward + 0.365 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.299 1 + 0.395 value_reward_not_chosen[t] + -0.359 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 72/1000 --- L(Train): 0.0127492 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.154 1 + 0.916 value_reward_chosen[t] + 0.393 reward + -0.066 value_reward_chosen^2 + -0.295 value_reward_chosen*reward + 0.367 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.302 1 + 0.39 value_reward_not_chosen[t] + -0.359 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 73/1000 --- L(Train): 0.0126214 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.151 1 + 0.919 value_reward_chosen[t] + 0.395 reward + -0.066 value_reward_chosen^2 + -0.298 value_reward_chosen*reward + 0.368 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.305 1 + 0.385 value_reward_not_chosen[t] + -0.36 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 74/1000 --- L(Train): 0.0124987 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.148 1 + 0.921 value_reward_chosen[t] + 0.397 reward + -0.065 value_reward_chosen^2 + -0.3 value_reward_chosen*reward + 0.369 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.308 1 + 0.381 value_reward_not_chosen[t] + -0.36 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 75/1000 --- L(Train): 0.0123808 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.146 1 + 0.923 value_reward_chosen[t] + 0.399 reward + -0.065 value_reward_chosen^2 + -0.302 value_reward_chosen*reward + 0.371 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.311 1 + 0.376 value_reward_not_chosen[t] + -0.36 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 76/1000 --- L(Train): 0.0122674 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.143 1 + 0.925 value_reward_chosen[t] + 0.401 reward + -0.065 value_reward_chosen^2 + -0.305 value_reward_chosen*reward + 0.372 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.314 1 + 0.372 value_reward_not_chosen[t] + -0.36 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 77/1000 --- L(Train): 0.0121585 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.14 1 + 0.927 value_reward_chosen[t] + 0.403 reward + -0.065 value_reward_chosen^2 + -0.307 value_reward_chosen*reward + 0.373 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.316 1 + 0.367 value_reward_not_chosen[t] + -0.36 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 78/1000 --- L(Train): 0.0120537 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.138 1 + 0.928 value_reward_chosen[t] + 0.405 reward + -0.066 value_reward_chosen^2 + -0.309 value_reward_chosen*reward + 0.374 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.319 1 + 0.363 value_reward_not_chosen[t] + -0.36 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 79/1000 --- L(Train): 0.0119526 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.135 1 + 0.93 value_reward_chosen[t] + 0.407 reward + -0.066 value_reward_chosen^2 + -0.312 value_reward_chosen*reward + 0.376 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.322 1 + 0.359 value_reward_not_chosen[t] + -0.359 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 80/1000 --- L(Train): 0.0118552 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.133 1 + 0.931 value_reward_chosen[t] + 0.409 reward + -0.066 value_reward_chosen^2 + -0.314 value_reward_chosen*reward + 0.377 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.325 1 + 0.354 value_reward_not_chosen[t] + -0.359 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 81/1000 --- L(Train): 0.0117616 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.13 1 + 0.932 value_reward_chosen[t] + 0.411 reward + -0.066 value_reward_chosen^2 + -0.316 value_reward_chosen*reward + 0.378 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.327 1 + 0.35 value_reward_not_chosen[t] + -0.359 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 82/1000 --- L(Train): 0.0116712 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.128 1 + 0.934 value_reward_chosen[t] + 0.413 reward + -0.065 value_reward_chosen^2 + -0.318 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.33 1 + 0.346 value_reward_not_chosen[t] + -0.358 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 83/1000 --- L(Train): 0.0115842 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.125 1 + 0.935 value_reward_chosen[t] + 0.415 reward + -0.065 value_reward_chosen^2 + -0.319 value_reward_chosen*reward + 0.381 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.333 1 + 0.342 value_reward_not_chosen[t] + -0.357 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 84/1000 --- L(Train): 0.0115004 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.123 1 + 0.936 value_reward_chosen[t] + 0.417 reward + -0.065 value_reward_chosen^2 + -0.321 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.335 1 + 0.338 value_reward_not_chosen[t] + -0.356 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 85/1000 --- L(Train): 0.0114191 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.121 1 + 0.937 value_reward_chosen[t] + 0.419 reward + -0.064 value_reward_chosen^2 + -0.323 value_reward_chosen*reward + 0.384 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.338 1 + 0.335 value_reward_not_chosen[t] + -0.355 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 86/1000 --- L(Train): 0.0113407 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.119 1 + 0.937 value_reward_chosen[t] + 0.421 reward + -0.064 value_reward_chosen^2 + -0.324 value_reward_chosen*reward + 0.385 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.341 1 + 0.331 value_reward_not_chosen[t] + -0.355 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 87/1000 --- L(Train): 0.0112647 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.116 1 + 0.938 value_reward_chosen[t] + 0.423 reward + -0.064 value_reward_chosen^2 + -0.326 value_reward_chosen*reward + 0.386 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.344 1 + 0.327 value_reward_not_chosen[t] + -0.354 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 88/1000 --- L(Train): 0.0111915 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.114 1 + 0.939 value_reward_chosen[t] + 0.425 reward + -0.064 value_reward_chosen^2 + -0.328 value_reward_chosen*reward + 0.387 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.346 1 + 0.324 value_reward_not_chosen[t] + -0.352 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 89/1000 --- L(Train): 0.0111207 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.112 1 + 0.939 value_reward_chosen[t] + 0.427 reward + -0.064 value_reward_chosen^2 + -0.33 value_reward_chosen*reward + 0.389 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.349 1 + 0.32 value_reward_not_chosen[t] + -0.351 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 90/1000 --- L(Train): 0.0110516 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.11 1 + 0.94 value_reward_chosen[t] + 0.429 reward + -0.063 value_reward_chosen^2 + -0.331 value_reward_chosen*reward + 0.39 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.352 1 + 0.317 value_reward_not_chosen[t] + -0.35 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 91/1000 --- L(Train): 0.0109850 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.108 1 + 0.94 value_reward_chosen[t] + 0.43 reward + -0.063 value_reward_chosen^2 + -0.333 value_reward_chosen*reward + 0.391 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.355 1 + 0.313 value_reward_not_chosen[t] + -0.349 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 92/1000 --- L(Train): 0.0109205 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.106 1 + 0.94 value_reward_chosen[t] + 0.432 reward + -0.063 value_reward_chosen^2 + -0.334 value_reward_chosen*reward + 0.392 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.358 1 + 0.31 value_reward_not_chosen[t] + -0.347 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 93/1000 --- L(Train): 0.0108580 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.104 1 + 0.941 value_reward_chosen[t] + 0.434 reward + -0.062 value_reward_chosen^2 + -0.335 value_reward_chosen*reward + 0.393 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.361 1 + 0.307 value_reward_not_chosen[t] + -0.346 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 94/1000 --- L(Train): 0.0107974 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.102 1 + 0.941 value_reward_chosen[t] + 0.436 reward + -0.062 value_reward_chosen^2 + -0.337 value_reward_chosen*reward + 0.394 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.364 1 + 0.304 value_reward_not_chosen[t] + -0.344 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 95/1000 --- L(Train): 0.0107384 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.1 1 + 0.941 value_reward_chosen[t] + 0.438 reward + -0.061 value_reward_chosen^2 + -0.338 value_reward_chosen*reward + 0.396 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.367 1 + 0.301 value_reward_not_chosen[t] + -0.343 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 96/1000 --- L(Train): 0.0106814 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.098 1 + 0.941 value_reward_chosen[t] + 0.44 reward + -0.061 value_reward_chosen^2 + -0.339 value_reward_chosen*reward + 0.397 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.37 1 + 0.298 value_reward_not_chosen[t] + -0.341 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 97/1000 --- L(Train): 0.0106258 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.096 1 + 0.942 value_reward_chosen[t] + 0.442 reward + -0.06 value_reward_chosen^2 + -0.341 value_reward_chosen*reward + 0.398 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.373 1 + 0.295 value_reward_not_chosen[t] + -0.34 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 98/1000 --- L(Train): 0.0105717 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.094 1 + 0.942 value_reward_chosen[t] + 0.443 reward + -0.06 value_reward_chosen^2 + -0.342 value_reward_chosen*reward + 0.399 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.376 1 + 0.292 value_reward_not_chosen[t] + -0.338 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 99/1000 --- L(Train): 0.0105198 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.092 1 + 0.942 value_reward_chosen[t] + 0.445 reward + -0.06 value_reward_chosen^2 + -0.343 value_reward_chosen*reward + 0.4 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.379 1 + 0.289 value_reward_not_chosen[t] + -0.337 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 100/1000 --- L(Train): 0.0104689 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.09 1 + 0.942 value_reward_chosen[t] + 0.447 reward + -0.059 value_reward_chosen^2 + -0.344 value_reward_chosen*reward + 0.4 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.382 1 + 0.286 value_reward_not_chosen[t] + -0.335 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 101/1000 --- L(Train): 0.0104190 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.088 1 + 0.942 value_reward_chosen[t] + 0.448 reward + -0.059 value_reward_chosen^2 + -0.345 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.385 1 + 0.284 value_reward_not_chosen[t] + -0.333 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 102/1000 --- L(Train): 0.0103706 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.086 1 + 0.942 value_reward_chosen[t] + 0.45 reward + -0.058 value_reward_chosen^2 + -0.346 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.388 1 + 0.281 value_reward_not_chosen[t] + -0.331 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 103/1000 --- L(Train): 0.0103239 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.084 1 + 0.942 value_reward_chosen[t] + 0.452 reward + -0.058 value_reward_chosen^2 + -0.348 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.391 1 + 0.278 value_reward_not_chosen[t] + -0.329 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 104/1000 --- L(Train): 0.0102775 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.083 1 + 0.942 value_reward_chosen[t] + 0.453 reward + -0.057 value_reward_chosen^2 + -0.349 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.394 1 + 0.276 value_reward_not_chosen[t] + -0.327 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 105/1000 --- L(Train): 0.0102327 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.081 1 + 0.942 value_reward_chosen[t] + 0.455 reward + -0.057 value_reward_chosen^2 + -0.35 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.397 1 + 0.273 value_reward_not_chosen[t] + -0.325 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 106/1000 --- L(Train): 0.0101892 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.079 1 + 0.943 value_reward_chosen[t] + 0.456 reward + -0.056 value_reward_chosen^2 + -0.351 value_reward_chosen*reward + 0.405 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.4 1 + 0.271 value_reward_not_chosen[t] + -0.324 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.002 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 107/1000 --- L(Train): 0.0101463 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.077 1 + 0.943 value_reward_chosen[t] + 0.458 reward + -0.056 value_reward_chosen^2 + -0.352 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.403 1 + 0.268 value_reward_not_chosen[t] + -0.322 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 108/1000 --- L(Train): 0.0101046 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.075 1 + 0.943 value_reward_chosen[t] + 0.459 reward + -0.055 value_reward_chosen^2 + -0.353 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.406 1 + 0.266 value_reward_not_chosen[t] + -0.319 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 109/1000 --- L(Train): 0.0100639 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.073 1 + 0.943 value_reward_chosen[t] + 0.461 reward + -0.055 value_reward_chosen^2 + -0.354 value_reward_chosen*reward + 0.407 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.409 1 + 0.264 value_reward_not_chosen[t] + -0.317 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 110/1000 --- L(Train): 0.0100242 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.071 1 + 0.943 value_reward_chosen[t] + 0.462 reward + -0.054 value_reward_chosen^2 + -0.355 value_reward_chosen*reward + 0.408 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.412 1 + 0.261 value_reward_not_chosen[t] + -0.315 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 111/1000 --- L(Train): 0.0099854 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.07 1 + 0.943 value_reward_chosen[t] + 0.464 reward + -0.054 value_reward_chosen^2 + -0.356 value_reward_chosen*reward + 0.408 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.415 1 + 0.259 value_reward_not_chosen[t] + -0.313 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 112/1000 --- L(Train): 0.0099473 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.068 1 + 0.943 value_reward_chosen[t] + 0.465 reward + -0.053 value_reward_chosen^2 + -0.356 value_reward_chosen*reward + 0.409 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.418 1 + 0.257 value_reward_not_chosen[t] + -0.311 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 113/1000 --- L(Train): 0.0099102 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.066 1 + 0.943 value_reward_chosen[t] + 0.466 reward + -0.053 value_reward_chosen^2 + -0.357 value_reward_chosen*reward + 0.409 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.421 1 + 0.255 value_reward_not_chosen[t] + -0.309 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 114/1000 --- L(Train): 0.0098735 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.064 1 + 0.943 value_reward_chosen[t] + 0.468 reward + -0.052 value_reward_chosen^2 + -0.358 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.424 1 + 0.252 value_reward_not_chosen[t] + -0.307 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 115/1000 --- L(Train): 0.0098381 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.062 1 + 0.943 value_reward_chosen[t] + 0.469 reward + -0.052 value_reward_chosen^2 + -0.359 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.427 1 + 0.25 value_reward_not_chosen[t] + -0.304 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 116/1000 --- L(Train): 0.0098033 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.061 1 + 0.943 value_reward_chosen[t] + 0.47 reward + -0.051 value_reward_chosen^2 + -0.36 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.43 1 + 0.248 value_reward_not_chosen[t] + -0.302 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 117/1000 --- L(Train): 0.0097693 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.059 1 + 0.943 value_reward_chosen[t] + 0.472 reward + -0.051 value_reward_chosen^2 + -0.361 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.433 1 + 0.246 value_reward_not_chosen[t] + -0.3 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 118/1000 --- L(Train): 0.0097362 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.057 1 + 0.943 value_reward_chosen[t] + 0.473 reward + -0.05 value_reward_chosen^2 + -0.361 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.436 1 + 0.244 value_reward_not_chosen[t] + -0.297 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 119/1000 --- L(Train): 0.0097032 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.056 1 + 0.943 value_reward_chosen[t] + 0.474 reward + -0.05 value_reward_chosen^2 + -0.362 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.439 1 + 0.242 value_reward_not_chosen[t] + -0.295 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 120/1000 --- L(Train): 0.0096711 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.054 1 + 0.943 value_reward_chosen[t] + 0.475 reward + -0.049 value_reward_chosen^2 + -0.363 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.442 1 + 0.24 value_reward_not_chosen[t] + -0.293 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 121/1000 --- L(Train): 0.0096400 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.052 1 + 0.943 value_reward_chosen[t] + 0.476 reward + -0.049 value_reward_chosen^2 + -0.364 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.445 1 + 0.238 value_reward_not_chosen[t] + -0.29 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 122/1000 --- L(Train): 0.0096090 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.051 1 + 0.943 value_reward_chosen[t] + 0.478 reward + -0.048 value_reward_chosen^2 + -0.364 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.448 1 + 0.236 value_reward_not_chosen[t] + -0.288 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 123/1000 --- L(Train): 0.0095789 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.049 1 + 0.943 value_reward_chosen[t] + 0.479 reward + -0.048 value_reward_chosen^2 + -0.365 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.451 1 + 0.234 value_reward_not_chosen[t] + -0.286 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 124/1000 --- L(Train): 0.0095492 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.047 1 + 0.943 value_reward_chosen[t] + 0.48 reward + -0.047 value_reward_chosen^2 + -0.366 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.454 1 + 0.232 value_reward_not_chosen[t] + -0.283 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 125/1000 --- L(Train): 0.0095200 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.046 1 + 0.943 value_reward_chosen[t] + 0.481 reward + -0.046 value_reward_chosen^2 + -0.367 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.457 1 + 0.23 value_reward_not_chosen[t] + -0.281 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 126/1000 --- L(Train): 0.0094916 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.044 1 + 0.943 value_reward_chosen[t] + 0.482 reward + -0.046 value_reward_chosen^2 + -0.367 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.46 1 + 0.228 value_reward_not_chosen[t] + -0.278 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 127/1000 --- L(Train): 0.0094637 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.043 1 + 0.943 value_reward_chosen[t] + 0.483 reward + -0.046 value_reward_chosen^2 + -0.368 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.463 1 + 0.226 value_reward_not_chosen[t] + -0.276 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 128/1000 --- L(Train): 0.0094361 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.041 1 + 0.943 value_reward_chosen[t] + 0.484 reward + -0.045 value_reward_chosen^2 + -0.369 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.465 1 + 0.225 value_reward_not_chosen[t] + -0.273 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 129/1000 --- L(Train): 0.0094091 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.04 1 + 0.943 value_reward_chosen[t] + 0.485 reward + -0.045 value_reward_chosen^2 + -0.369 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.468 1 + 0.223 value_reward_not_chosen[t] + -0.271 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 130/1000 --- L(Train): 0.0093826 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.039 1 + 0.943 value_reward_chosen[t] + 0.486 reward + -0.044 value_reward_chosen^2 + -0.37 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.471 1 + 0.221 value_reward_not_chosen[t] + -0.269 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 131/1000 --- L(Train): 0.0093565 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.037 1 + 0.943 value_reward_chosen[t] + 0.488 reward + -0.043 value_reward_chosen^2 + -0.37 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.474 1 + 0.219 value_reward_not_chosen[t] + -0.266 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 132/1000 --- L(Train): 0.0093308 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.036 1 + 0.943 value_reward_chosen[t] + 0.489 reward + -0.043 value_reward_chosen^2 + -0.371 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.477 1 + 0.218 value_reward_not_chosen[t] + -0.264 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 133/1000 --- L(Train): 0.0093056 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.034 1 + 0.943 value_reward_chosen[t] + 0.49 reward + -0.043 value_reward_chosen^2 + -0.372 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.48 1 + 0.216 value_reward_not_chosen[t] + -0.261 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 134/1000 --- L(Train): 0.0092807 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.033 1 + 0.943 value_reward_chosen[t] + 0.491 reward + -0.042 value_reward_chosen^2 + -0.372 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.483 1 + 0.214 value_reward_not_chosen[t] + -0.259 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 135/1000 --- L(Train): 0.0092567 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.032 1 + 0.943 value_reward_chosen[t] + 0.492 reward + -0.042 value_reward_chosen^2 + -0.373 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.486 1 + 0.212 value_reward_not_chosen[t] + -0.256 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 136/1000 --- L(Train): 0.0092330 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.031 1 + 0.943 value_reward_chosen[t] + 0.493 reward + -0.041 value_reward_chosen^2 + -0.373 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.488 1 + 0.211 value_reward_not_chosen[t] + -0.254 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 137/1000 --- L(Train): 0.0092097 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.029 1 + 0.943 value_reward_chosen[t] + 0.494 reward + -0.041 value_reward_chosen^2 + -0.374 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.491 1 + 0.209 value_reward_not_chosen[t] + -0.252 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 138/1000 --- L(Train): 0.0091865 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.028 1 + 0.943 value_reward_chosen[t] + 0.495 reward + -0.04 value_reward_chosen^2 + -0.374 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.494 1 + 0.207 value_reward_not_chosen[t] + -0.249 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 139/1000 --- L(Train): 0.0091635 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.027 1 + 0.943 value_reward_chosen[t] + 0.496 reward + -0.04 value_reward_chosen^2 + -0.375 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.497 1 + 0.206 value_reward_not_chosen[t] + -0.247 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 140/1000 --- L(Train): 0.0091410 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.026 1 + 0.943 value_reward_chosen[t] + 0.497 reward + -0.039 value_reward_chosen^2 + -0.375 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.5 1 + 0.204 value_reward_not_chosen[t] + -0.244 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 141/1000 --- L(Train): 0.0091193 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.025 1 + 0.943 value_reward_chosen[t] + 0.498 reward + -0.039 value_reward_chosen^2 + -0.375 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.502 1 + 0.202 value_reward_not_chosen[t] + -0.242 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 142/1000 --- L(Train): 0.0090977 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.023 1 + 0.942 value_reward_chosen[t] + 0.499 reward + -0.039 value_reward_chosen^2 + -0.376 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.505 1 + 0.201 value_reward_not_chosen[t] + -0.239 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 143/1000 --- L(Train): 0.0090764 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.022 1 + 0.942 value_reward_chosen[t] + 0.5 reward + -0.038 value_reward_chosen^2 + -0.376 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.508 1 + 0.199 value_reward_not_chosen[t] + -0.237 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 144/1000 --- L(Train): 0.0090554 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.021 1 + 0.942 value_reward_chosen[t] + 0.501 reward + -0.038 value_reward_chosen^2 + -0.377 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.511 1 + 0.198 value_reward_not_chosen[t] + -0.235 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 145/1000 --- L(Train): 0.0090343 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.02 1 + 0.942 value_reward_chosen[t] + 0.502 reward + -0.037 value_reward_chosen^2 + -0.377 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.514 1 + 0.196 value_reward_not_chosen[t] + -0.232 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 146/1000 --- L(Train): 0.0090141 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.019 1 + 0.942 value_reward_chosen[t] + 0.503 reward + -0.037 value_reward_chosen^2 + -0.378 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.516 1 + 0.194 value_reward_not_chosen[t] + -0.23 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 147/1000 --- L(Train): 0.0089941 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.018 1 + 0.942 value_reward_chosen[t] + 0.503 reward + -0.037 value_reward_chosen^2 + -0.378 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.519 1 + 0.193 value_reward_not_chosen[t] + -0.228 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 148/1000 --- L(Train): 0.0089746 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.017 1 + 0.942 value_reward_chosen[t] + 0.504 reward + -0.036 value_reward_chosen^2 + -0.378 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.522 1 + 0.191 value_reward_not_chosen[t] + -0.225 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 149/1000 --- L(Train): 0.0089550 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.016 1 + 0.942 value_reward_chosen[t] + 0.505 reward + -0.036 value_reward_chosen^2 + -0.379 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.524 1 + 0.19 value_reward_not_chosen[t] + -0.223 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 150/1000 --- L(Train): 0.0089359 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.015 1 + 0.941 value_reward_chosen[t] + 0.506 reward + -0.036 value_reward_chosen^2 + -0.379 value_reward_chosen*reward + 0.414 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.527 1 + 0.188 value_reward_not_chosen[t] + -0.22 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 151/1000 --- L(Train): 0.0089177 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.014 1 + 0.941 value_reward_chosen[t] + 0.507 reward + -0.035 value_reward_chosen^2 + -0.379 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.53 1 + 0.187 value_reward_not_chosen[t] + -0.218 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 152/1000 --- L(Train): 0.0088988 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.013 1 + 0.941 value_reward_chosen[t] + 0.508 reward + -0.035 value_reward_chosen^2 + -0.38 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.532 1 + 0.185 value_reward_not_chosen[t] + -0.216 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 153/1000 --- L(Train): 0.0088803 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.013 1 + 0.941 value_reward_chosen[t] + 0.509 reward + -0.035 value_reward_chosen^2 + -0.38 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.535 1 + 0.184 value_reward_not_chosen[t] + -0.213 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 154/1000 --- L(Train): 0.0088624 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.012 1 + 0.941 value_reward_chosen[t] + 0.51 reward + -0.034 value_reward_chosen^2 + -0.38 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.538 1 + 0.182 value_reward_not_chosen[t] + -0.211 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 155/1000 --- L(Train): 0.0088447 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.011 1 + 0.941 value_reward_chosen[t] + 0.511 reward + -0.034 value_reward_chosen^2 + -0.381 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.54 1 + 0.181 value_reward_not_chosen[t] + -0.209 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 156/1000 --- L(Train): 0.0088272 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.01 1 + 0.941 value_reward_chosen[t] + 0.512 reward + -0.034 value_reward_chosen^2 + -0.381 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.543 1 + 0.179 value_reward_not_chosen[t] + -0.206 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 157/1000 --- L(Train): 0.0088098 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.009 1 + 0.941 value_reward_chosen[t] + 0.513 reward + -0.033 value_reward_chosen^2 + -0.381 value_reward_chosen*reward + 0.413 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.545 1 + 0.178 value_reward_not_chosen[t] + -0.204 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 158/1000 --- L(Train): 0.0087928 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.009 1 + 0.941 value_reward_chosen[t] + 0.513 reward + -0.033 value_reward_chosen^2 + -0.382 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.548 1 + 0.176 value_reward_not_chosen[t] + -0.202 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 159/1000 --- L(Train): 0.0087760 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.008 1 + 0.94 value_reward_chosen[t] + 0.514 reward + -0.033 value_reward_chosen^2 + -0.382 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.551 1 + 0.175 value_reward_not_chosen[t] + -0.2 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 160/1000 --- L(Train): 0.0087597 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.007 1 + 0.94 value_reward_chosen[t] + 0.515 reward + -0.032 value_reward_chosen^2 + -0.382 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.553 1 + 0.173 value_reward_not_chosen[t] + -0.197 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 161/1000 --- L(Train): 0.0087433 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.006 1 + 0.94 value_reward_chosen[t] + 0.516 reward + -0.032 value_reward_chosen^2 + -0.382 value_reward_chosen*reward + 0.412 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.556 1 + 0.172 value_reward_not_chosen[t] + -0.195 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 162/1000 --- L(Train): 0.0087277 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.006 1 + 0.94 value_reward_chosen[t] + 0.517 reward + -0.032 value_reward_chosen^2 + -0.383 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.558 1 + 0.17 value_reward_not_chosen[t] + -0.193 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 163/1000 --- L(Train): 0.0087117 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.005 1 + 0.94 value_reward_chosen[t] + 0.518 reward + -0.032 value_reward_chosen^2 + -0.383 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.561 1 + 0.169 value_reward_not_chosen[t] + -0.191 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 164/1000 --- L(Train): 0.0086956 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.004 1 + 0.94 value_reward_chosen[t] + 0.519 reward + -0.031 value_reward_chosen^2 + -0.383 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.563 1 + 0.168 value_reward_not_chosen[t] + -0.188 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.003 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 165/1000 --- L(Train): 0.0086806 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.004 1 + 0.94 value_reward_chosen[t] + 0.519 reward + -0.031 value_reward_chosen^2 + -0.383 value_reward_chosen*reward + 0.411 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.566 1 + 0.166 value_reward_not_chosen[t] + -0.186 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 166/1000 --- L(Train): 0.0086654 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.003 1 + 0.94 value_reward_chosen[t] + 0.52 reward + -0.031 value_reward_chosen^2 + -0.384 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.568 1 + 0.165 value_reward_not_chosen[t] + -0.184 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 167/1000 --- L(Train): 0.0086502 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.002 1 + 0.94 value_reward_chosen[t] + 0.521 reward + -0.031 value_reward_chosen^2 + -0.384 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.571 1 + 0.163 value_reward_not_chosen[t] + -0.182 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 168/1000 --- L(Train): 0.0086354 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.002 1 + 0.94 value_reward_chosen[t] + 0.522 reward + -0.03 value_reward_chosen^2 + -0.384 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.573 1 + 0.162 value_reward_not_chosen[t] + -0.18 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 169/1000 --- L(Train): 0.0086209 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.94 value_reward_chosen[t] + 0.523 reward + -0.03 value_reward_chosen^2 + -0.384 value_reward_chosen*reward + 0.41 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.576 1 + 0.16 value_reward_not_chosen[t] + -0.178 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 170/1000 --- L(Train): 0.0086065 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.939 value_reward_chosen[t] + 0.523 reward + -0.03 value_reward_chosen^2 + -0.385 value_reward_chosen*reward + 0.409 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.578 1 + 0.159 value_reward_not_chosen[t] + -0.175 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 171/1000 --- L(Train): 0.0085923 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.939 value_reward_chosen[t] + 0.524 reward + -0.03 value_reward_chosen^2 + -0.385 value_reward_chosen*reward + 0.409 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.58 1 + 0.158 value_reward_not_chosen[t] + -0.173 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 172/1000 --- L(Train): 0.0085782 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.939 value_reward_chosen[t] + 0.525 reward + -0.029 value_reward_chosen^2 + -0.385 value_reward_chosen*reward + 0.409 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.583 1 + 0.156 value_reward_not_chosen[t] + -0.171 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 173/1000 --- L(Train): 0.0085646 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.939 value_reward_chosen[t] + 0.526 reward + -0.029 value_reward_chosen^2 + -0.385 value_reward_chosen*reward + 0.408 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.585 1 + 0.155 value_reward_not_chosen[t] + -0.169 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 174/1000 --- L(Train): 0.0085511 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.939 value_reward_chosen[t] + 0.527 reward + -0.029 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.408 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.588 1 + 0.154 value_reward_not_chosen[t] + -0.167 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 175/1000 --- L(Train): 0.0085374 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.939 value_reward_chosen[t] + 0.527 reward + -0.029 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.408 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.59 1 + 0.152 value_reward_not_chosen[t] + -0.165 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 176/1000 --- L(Train): 0.0085243 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.939 value_reward_chosen[t] + 0.528 reward + -0.029 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.407 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.592 1 + 0.151 value_reward_not_chosen[t] + -0.163 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 177/1000 --- L(Train): 0.0085113 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.939 value_reward_chosen[t] + 0.529 reward + -0.029 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.407 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.595 1 + 0.15 value_reward_not_chosen[t] + -0.161 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 178/1000 --- L(Train): 0.0084978 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.939 value_reward_chosen[t] + 0.53 reward + -0.029 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.597 1 + 0.148 value_reward_not_chosen[t] + -0.159 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 179/1000 --- L(Train): 0.0084853 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.939 value_reward_chosen[t] + 0.53 reward + -0.029 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.599 1 + 0.147 value_reward_not_chosen[t] + -0.157 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 180/1000 --- L(Train): 0.0084731 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.531 reward + -0.029 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.406 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.601 1 + 0.146 value_reward_not_chosen[t] + -0.155 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 181/1000 --- L(Train): 0.0084608 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.532 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.405 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.604 1 + 0.144 value_reward_not_chosen[t] + -0.153 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 182/1000 --- L(Train): 0.0084483 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.532 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.405 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.606 1 + 0.143 value_reward_not_chosen[t] + -0.151 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 183/1000 --- L(Train): 0.0084362 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.533 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.608 1 + 0.142 value_reward_not_chosen[t] + -0.149 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 184/1000 --- L(Train): 0.0084244 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.534 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.404 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.611 1 + 0.141 value_reward_not_chosen[t] + -0.147 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 185/1000 --- L(Train): 0.0084123 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.535 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.613 1 + 0.139 value_reward_not_chosen[t] + -0.145 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 186/1000 --- L(Train): 0.0084009 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.939 value_reward_chosen[t] + 0.535 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.403 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.615 1 + 0.138 value_reward_not_chosen[t] + -0.143 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 187/1000 --- L(Train): 0.0083896 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.939 value_reward_chosen[t] + 0.536 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.617 1 + 0.137 value_reward_not_chosen[t] + -0.141 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 188/1000 --- L(Train): 0.0083779 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.939 value_reward_chosen[t] + 0.536 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.402 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.619 1 + 0.135 value_reward_not_chosen[t] + -0.139 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 189/1000 --- L(Train): 0.0083667 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.939 value_reward_chosen[t] + 0.537 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.401 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.621 1 + 0.134 value_reward_not_chosen[t] + -0.137 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 190/1000 --- L(Train): 0.0083557 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.94 value_reward_chosen[t] + 0.538 reward + -0.03 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.4 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.624 1 + 0.133 value_reward_not_chosen[t] + -0.135 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 191/1000 --- L(Train): 0.0083451 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.001 1 + 0.94 value_reward_chosen[t] + 0.538 reward + -0.03 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.4 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.626 1 + 0.132 value_reward_not_chosen[t] + -0.134 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 192/1000 --- L(Train): 0.0083338 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.94 value_reward_chosen[t] + 0.539 reward + -0.03 value_reward_chosen^2 + -0.386 value_reward_chosen*reward + 0.399 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.628 1 + 0.131 value_reward_not_chosen[t] + -0.132 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 193/1000 --- L(Train): 0.0083233 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.94 value_reward_chosen[t] + 0.54 reward + -0.03 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.399 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.63 1 + 0.129 value_reward_not_chosen[t] + -0.13 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 194/1000 --- L(Train): 0.0083129 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.94 value_reward_chosen[t] + 0.54 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.398 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.632 1 + 0.128 value_reward_not_chosen[t] + -0.128 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 195/1000 --- L(Train): 0.0083025 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.94 value_reward_chosen[t] + 0.541 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.398 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.634 1 + 0.127 value_reward_not_chosen[t] + -0.126 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 196/1000 --- L(Train): 0.0082924 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.94 value_reward_chosen[t] + 0.541 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.397 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.636 1 + 0.126 value_reward_not_chosen[t] + -0.124 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 197/1000 --- L(Train): 0.0082820 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.94 value_reward_chosen[t] + 0.542 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.396 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.638 1 + 0.125 value_reward_not_chosen[t] + -0.123 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 198/1000 --- L(Train): 0.0082721 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.94 value_reward_chosen[t] + 0.543 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.396 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.64 1 + 0.123 value_reward_not_chosen[t] + -0.121 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 199/1000 --- L(Train): 0.0082622 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.543 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.395 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.642 1 + 0.122 value_reward_not_chosen[t] + -0.119 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 200/1000 --- L(Train): 0.0082530 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.544 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.394 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.644 1 + 0.121 value_reward_not_chosen[t] + -0.117 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 201/1000 --- L(Train): 0.0082429 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.544 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.394 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.646 1 + 0.12 value_reward_not_chosen[t] + -0.116 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 202/1000 --- L(Train): 0.0082332 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.545 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.393 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.648 1 + 0.119 value_reward_not_chosen[t] + -0.114 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 203/1000 --- L(Train): 0.0082242 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.546 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.393 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.65 1 + 0.118 value_reward_not_chosen[t] + -0.112 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 204/1000 --- L(Train): 0.0082149 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.546 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.392 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.652 1 + 0.116 value_reward_not_chosen[t] + -0.11 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 205/1000 --- L(Train): 0.0082054 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.547 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.391 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.654 1 + 0.115 value_reward_not_chosen[t] + -0.109 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 206/1000 --- L(Train): 0.0081964 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.548 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.391 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.656 1 + 0.114 value_reward_not_chosen[t] + -0.107 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 207/1000 --- L(Train): 0.0081876 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.548 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.39 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.658 1 + 0.113 value_reward_not_chosen[t] + -0.105 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 208/1000 --- L(Train): 0.0081783 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.549 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.389 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.66 1 + 0.112 value_reward_not_chosen[t] + -0.104 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 209/1000 --- L(Train): 0.0081697 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.549 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.389 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.662 1 + 0.111 value_reward_not_chosen[t] + -0.102 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 210/1000 --- L(Train): 0.0081614 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.55 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.388 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.664 1 + 0.11 value_reward_not_chosen[t] + -0.1 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 211/1000 --- L(Train): 0.0081528 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.551 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.387 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.666 1 + 0.109 value_reward_not_chosen[t] + -0.099 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 212/1000 --- L(Train): 0.0081442 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.551 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.387 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.667 1 + 0.108 value_reward_not_chosen[t] + -0.097 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 213/1000 --- L(Train): 0.0081358 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.552 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.386 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.669 1 + 0.107 value_reward_not_chosen[t] + -0.096 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 214/1000 --- L(Train): 0.0081275 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.553 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.386 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.671 1 + 0.106 value_reward_not_chosen[t] + -0.094 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 215/1000 --- L(Train): 0.0081196 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.942 value_reward_chosen[t] + 0.553 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.385 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.673 1 + 0.105 value_reward_not_chosen[t] + -0.092 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 216/1000 --- L(Train): 0.0081110 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.942 value_reward_chosen[t] + 0.554 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.384 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.675 1 + 0.103 value_reward_not_chosen[t] + -0.091 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 217/1000 --- L(Train): 0.0081029 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.942 value_reward_chosen[t] + 0.554 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.384 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.676 1 + 0.102 value_reward_not_chosen[t] + -0.089 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 218/1000 --- L(Train): 0.0080954 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.942 value_reward_chosen[t] + 0.555 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.383 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.678 1 + 0.101 value_reward_not_chosen[t] + -0.088 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 219/1000 --- L(Train): 0.0080872 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.942 value_reward_chosen[t] + 0.556 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.68 1 + 0.1 value_reward_not_chosen[t] + -0.086 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 220/1000 --- L(Train): 0.0080799 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.942 value_reward_chosen[t] + 0.556 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.382 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.682 1 + 0.099 value_reward_not_chosen[t] + -0.085 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.998 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 221/1000 --- L(Train): 0.0080723 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.557 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.381 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.683 1 + 0.098 value_reward_not_chosen[t] + -0.083 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 222/1000 --- L(Train): 0.0080644 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.558 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.685 1 + 0.097 value_reward_not_chosen[t] + -0.082 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 223/1000 --- L(Train): 0.0080567 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.558 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.38 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.687 1 + 0.096 value_reward_not_chosen[t] + -0.08 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.002 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 224/1000 --- L(Train): 0.0080494 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.559 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.379 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.688 1 + 0.095 value_reward_not_chosen[t] + -0.079 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 225/1000 --- L(Train): 0.0080427 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.56 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.378 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.69 1 + 0.094 value_reward_not_chosen[t] + -0.077 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 226/1000 --- L(Train): 0.0080354 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.56 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.378 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.692 1 + 0.093 value_reward_not_chosen[t] + -0.076 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 227/1000 --- L(Train): 0.0080281 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.561 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.377 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.693 1 + 0.093 value_reward_not_chosen[t] + -0.074 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.998 value_reward_not_displayed[t] + 0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 228/1000 --- L(Train): 0.0080207 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.562 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.377 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.695 1 + 0.092 value_reward_not_chosen[t] + -0.073 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 229/1000 --- L(Train): 0.0080145 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.562 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.376 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.697 1 + 0.091 value_reward_not_chosen[t] + -0.072 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 230/1000 --- L(Train): 0.0080075 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.563 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.375 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.698 1 + 0.09 value_reward_not_chosen[t] + -0.07 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 231/1000 --- L(Train): 0.0080005 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.564 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.375 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.7 1 + 0.089 value_reward_not_chosen[t] + -0.069 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.002 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 232/1000 --- L(Train): 0.0079937 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.564 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.374 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.702 1 + 0.088 value_reward_not_chosen[t] + -0.067 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.002 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 233/1000 --- L(Train): 0.0079867 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.565 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.373 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.703 1 + 0.087 value_reward_not_chosen[t] + -0.066 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 234/1000 --- L(Train): 0.0079801 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.566 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.373 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.705 1 + 0.086 value_reward_not_chosen[t] + -0.065 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 235/1000 --- L(Train): 0.0079739 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.566 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.372 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.706 1 + 0.085 value_reward_not_chosen[t] + -0.063 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.998 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 236/1000 --- L(Train): 0.0079678 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.567 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.371 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.708 1 + 0.084 value_reward_not_chosen[t] + -0.062 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.998 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 237/1000 --- L(Train): 0.0079608 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.568 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.371 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.709 1 + 0.083 value_reward_not_chosen[t] + -0.061 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 238/1000 --- L(Train): 0.0079547 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.568 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.37 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.711 1 + 0.083 value_reward_not_chosen[t] + -0.059 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 239/1000 --- L(Train): 0.0079485 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.569 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.369 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.712 1 + 0.082 value_reward_not_chosen[t] + -0.058 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 240/1000 --- L(Train): 0.0079425 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.57 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.369 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.714 1 + 0.081 value_reward_not_chosen[t] + -0.057 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 241/1000 --- L(Train): 0.0079364 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.57 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.368 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.715 1 + 0.08 value_reward_not_chosen[t] + -0.055 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 242/1000 --- L(Train): 0.0079303 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.571 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.367 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.717 1 + 0.079 value_reward_not_chosen[t] + -0.054 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 243/1000 --- L(Train): 0.0079243 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.572 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.367 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.718 1 + 0.078 value_reward_not_chosen[t] + -0.053 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.002 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 244/1000 --- L(Train): 0.0079186 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.572 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.366 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.72 1 + 0.077 value_reward_not_chosen[t] + -0.052 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 245/1000 --- L(Train): 0.0079126 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.573 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.365 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.721 1 + 0.077 value_reward_not_chosen[t] + -0.05 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 246/1000 --- L(Train): 0.0079070 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.574 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.364 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.722 1 + 0.076 value_reward_not_chosen[t] + -0.049 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 247/1000 --- L(Train): 0.0079015 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.575 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.364 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.724 1 + 0.075 value_reward_not_chosen[t] + -0.048 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 248/1000 --- L(Train): 0.0078957 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.575 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.363 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.725 1 + 0.074 value_reward_not_chosen[t] + -0.047 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 249/1000 --- L(Train): 0.0078897 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.576 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.362 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.727 1 + 0.073 value_reward_not_chosen[t] + -0.046 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 250/1000 --- L(Train): 0.0078840 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.577 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.362 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.728 1 + 0.073 value_reward_not_chosen[t] + -0.044 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 0, 0, 0, 0, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 0\n",
      "value_reward_not_displayed: 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 251/1000 --- L(Train): 0.0078791 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.577 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.361 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.729 1 + 0.072 value_reward_not_chosen[t] + -0.043 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 1, 0, 0, 1, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 1\n",
      "value_reward_not_displayed: 1, 1, 1\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 252/1000 --- L(Train): 0.0078734 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.578 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.36 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.731 1 + 0.071 value_reward_not_chosen[t] + -0.042 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 2, 0, 0, 2, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 2\n",
      "value_reward_not_displayed: 2, 2, 2\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 253/1000 --- L(Train): 0.0078681 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.579 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.36 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.732 1 + 0.07 value_reward_not_chosen[t] + -0.041 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.002 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 3, 0, 0, 3, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 3\n",
      "value_reward_not_displayed: 3, 3, 3\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 254/1000 --- L(Train): 0.0078626 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.579 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.359 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.733 1 + 0.07 value_reward_not_chosen[t] + -0.04 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 4, 0, 0, 4, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 4\n",
      "value_reward_not_displayed: 4, 4, 4\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 255/1000 --- L(Train): 0.0078572 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.58 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.358 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.735 1 + 0.069 value_reward_not_chosen[t] + -0.039 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 5, 0, 0, 5, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 5\n",
      "value_reward_not_displayed: 5, 5, 5\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 256/1000 --- L(Train): 0.0078522 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.581 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.358 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.736 1 + 0.068 value_reward_not_chosen[t] + -0.037 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 6, 0, 0, 6, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 6\n",
      "value_reward_not_displayed: 6, 6, 6\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 257/1000 --- L(Train): 0.0078471 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.581 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.357 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.737 1 + 0.067 value_reward_not_chosen[t] + -0.036 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 7, 0, 0, 7, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 7\n",
      "value_reward_not_displayed: 7, 7, 7\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 258/1000 --- L(Train): 0.0078420 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.582 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.356 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.738 1 + 0.067 value_reward_not_chosen[t] + -0.035 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 8, 0, 0, 8, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 8\n",
      "value_reward_not_displayed: 8, 8, 8\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 259/1000 --- L(Train): 0.0078370 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.583 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.355 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.74 1 + 0.066 value_reward_not_chosen[t] + -0.034 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 9, 0, 0, 9, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 9\n",
      "value_reward_not_displayed: 9, 9, 9\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 260/1000 --- L(Train): 0.0078318 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.584 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.355 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.741 1 + 0.065 value_reward_not_chosen[t] + -0.033 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 10, 0, 0, 10, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 10\n",
      "value_reward_not_displayed: 10, 10, 10\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 261/1000 --- L(Train): 0.0078269 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.584 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.354 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.742 1 + 0.064 value_reward_not_chosen[t] + -0.032 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 11, 0, 0, 11, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 11\n",
      "value_reward_not_displayed: 11, 11, 11\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 262/1000 --- L(Train): 0.0078221 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.585 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.353 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.743 1 + 0.064 value_reward_not_chosen[t] + -0.031 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 12, 0, 0, 12, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 12\n",
      "value_reward_not_displayed: 12, 12, 12\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 263/1000 --- L(Train): 0.0078173 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.586 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.353 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.745 1 + 0.063 value_reward_not_chosen[t] + -0.03 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 13, 0, 0, 13, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 13\n",
      "value_reward_not_displayed: 13, 13, 13\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 264/1000 --- L(Train): 0.0078126 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.586 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.352 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.746 1 + 0.062 value_reward_not_chosen[t] + -0.029 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 14, 0, 0, 14, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 14\n",
      "value_reward_not_displayed: 14, 14, 14\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 265/1000 --- L(Train): 0.0078079 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.587 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.351 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.747 1 + 0.062 value_reward_not_chosen[t] + -0.028 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 15, 0, 0, 15, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 15\n",
      "value_reward_not_displayed: 15, 15, 15\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 266/1000 --- L(Train): 0.0078035 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.588 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.35 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.748 1 + 0.061 value_reward_not_chosen[t] + -0.027 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 16, 0, 0, 16, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 16\n",
      "value_reward_not_displayed: 16, 16, 16\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 267/1000 --- L(Train): 0.0077983 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.589 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.35 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.749 1 + 0.06 value_reward_not_chosen[t] + -0.026 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 17, 0, 0, 17, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 17\n",
      "value_reward_not_displayed: 17, 17, 17\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 268/1000 --- L(Train): 0.0077941 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.589 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.349 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.75 1 + 0.06 value_reward_not_chosen[t] + -0.025 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 18, 0, 0, 18, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 18\n",
      "value_reward_not_displayed: 18, 18, 18\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 269/1000 --- L(Train): 0.0077895 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.59 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.348 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.752 1 + 0.059 value_reward_not_chosen[t] + -0.024 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 19, 0, 0, 19, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 19\n",
      "value_reward_not_displayed: 19, 19, 19\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 270/1000 --- L(Train): 0.0077848 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.591 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.348 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.753 1 + 0.058 value_reward_not_chosen[t] + -0.023 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 20, 0, 0, 20, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 20\n",
      "value_reward_not_displayed: 20, 20, 20\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 271/1000 --- L(Train): 0.0077806 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.591 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.347 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.754 1 + 0.058 value_reward_not_chosen[t] + -0.022 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 21, 0, 0, 21, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 21\n",
      "value_reward_not_displayed: 21, 21, 21\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 272/1000 --- L(Train): 0.0077760 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.592 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.346 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.755 1 + 0.057 value_reward_not_chosen[t] + -0.021 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 22, 0, 0, 22, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 22\n",
      "value_reward_not_displayed: 22, 22, 22\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 273/1000 --- L(Train): 0.0077712 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.593 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.345 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.756 1 + 0.056 value_reward_not_chosen[t] + -0.02 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 23, 0, 0, 23, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 23\n",
      "value_reward_not_displayed: 23, 23, 23\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 274/1000 --- L(Train): 0.0077669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.594 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.345 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.757 1 + 0.056 value_reward_not_chosen[t] + -0.019 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 24, 0, 0, 24, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 24\n",
      "value_reward_not_displayed: 24, 24, 24\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 275/1000 --- L(Train): 0.0077628 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.594 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.344 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.758 1 + 0.055 value_reward_not_chosen[t] + -0.018 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 25, 0, 0, 25, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 25\n",
      "value_reward_not_displayed: 25, 25, 25\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 276/1000 --- L(Train): 0.0077583 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.595 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.343 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.759 1 + 0.054 value_reward_not_chosen[t] + -0.017 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 26, 0, 0, 26, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 26\n",
      "value_reward_not_displayed: 26, 26, 26\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 277/1000 --- L(Train): 0.0077542 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.596 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.342 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.76 1 + 0.054 value_reward_not_chosen[t] + -0.016 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 27, 0, 0, 27, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 27\n",
      "value_reward_not_displayed: 27, 27, 27\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 278/1000 --- L(Train): 0.0077505 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.597 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.342 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.761 1 + 0.053 value_reward_not_chosen[t] + -0.015 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 28, 0, 0, 28, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 28\n",
      "value_reward_not_displayed: 28, 28, 28\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 279/1000 --- L(Train): 0.0077456 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.597 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.341 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.762 1 + 0.053 value_reward_not_chosen[t] + -0.014 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 29, 0, 0, 29, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 29\n",
      "value_reward_not_displayed: 29, 29, 29\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 280/1000 --- L(Train): 0.0077415 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.598 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.34 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.763 1 + 0.052 value_reward_not_chosen[t] + -0.013 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 30, 0, 0, 30, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 30\n",
      "value_reward_not_displayed: 30, 30, 30\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 281/1000 --- L(Train): 0.0077376 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.599 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.339 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.765 1 + 0.051 value_reward_not_chosen[t] + -0.012 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 31, 0, 0, 31, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 31\n",
      "value_reward_not_displayed: 31, 31, 31\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 282/1000 --- L(Train): 0.0077339 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.6 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.339 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.766 1 + 0.051 value_reward_not_chosen[t] + -0.012 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 32, 0, 0, 32, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 32\n",
      "value_reward_not_displayed: 32, 32, 32\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 283/1000 --- L(Train): 0.0077296 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.6 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.338 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.767 1 + 0.05 value_reward_not_chosen[t] + -0.011 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 33, 0, 0, 33, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 33\n",
      "value_reward_not_displayed: 33, 33, 33\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 284/1000 --- L(Train): 0.0077258 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.601 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.337 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.05 value_reward_not_chosen[t] + -0.01 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 34, 0, 0, 34, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 34\n",
      "value_reward_not_displayed: 34, 34, 34\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 285/1000 --- L(Train): 0.0077216 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.602 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.337 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.768 1 + 0.049 value_reward_not_chosen[t] + -0.009 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 35, 0, 0, 35, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 35\n",
      "value_reward_not_displayed: 35, 35, 35\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 286/1000 --- L(Train): 0.0077180 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.603 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.336 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.769 1 + 0.049 value_reward_not_chosen[t] + -0.008 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 36, 0, 0, 36, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 36\n",
      "value_reward_not_displayed: 36, 36, 36\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 287/1000 --- L(Train): 0.0077141 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.603 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.335 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.77 1 + 0.048 value_reward_not_chosen[t] + -0.007 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 37, 0, 0, 37, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 37\n",
      "value_reward_not_displayed: 37, 37, 37\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 288/1000 --- L(Train): 0.0077101 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.604 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.334 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.771 1 + 0.047 value_reward_not_chosen[t] + -0.006 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 38, 0, 0, 38, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 38\n",
      "value_reward_not_displayed: 38, 38, 38\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 289/1000 --- L(Train): 0.0077062 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.605 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.334 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.772 1 + 0.047 value_reward_not_chosen[t] + -0.006 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 39, 0, 0, 39, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 39\n",
      "value_reward_not_displayed: 39, 39, 39\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 290/1000 --- L(Train): 0.0077026 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.606 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.333 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.773 1 + 0.046 value_reward_not_chosen[t] + -0.005 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 40, 0, 0, 40, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 40\n",
      "value_reward_not_displayed: 40, 40, 40\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 291/1000 --- L(Train): 0.0076990 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.606 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.332 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.774 1 + 0.046 value_reward_not_chosen[t] + -0.004 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 41, 0, 0, 41, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 41\n",
      "value_reward_not_displayed: 41, 41, 41\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 292/1000 --- L(Train): 0.0076951 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.607 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.331 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.775 1 + 0.045 value_reward_not_chosen[t] + -0.003 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 42, 0, 0, 42, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 42\n",
      "value_reward_not_displayed: 42, 42, 42\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 293/1000 --- L(Train): 0.0076913 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.608 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.331 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.776 1 + 0.045 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 43, 0, 0, 43, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 43\n",
      "value_reward_not_displayed: 43, 43, 43\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 294/1000 --- L(Train): 0.0076878 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.609 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.33 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.777 1 + 0.044 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 44, 0, 0, 44, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 44\n",
      "value_reward_not_displayed: 44, 44, 44\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 295/1000 --- L(Train): 0.0076846 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.609 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.329 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.778 1 + 0.044 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 45, 0, 0, 45, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 45\n",
      "value_reward_not_displayed: 45, 45, 45\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 296/1000 --- L(Train): 0.0076809 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.61 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.328 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.043 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 46, 0, 0, 46, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 46\n",
      "value_reward_not_displayed: 46, 46, 46\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 297/1000 --- L(Train): 0.0076771 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.611 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.327 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.043 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 47, 0, 0, 47, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 47\n",
      "value_reward_not_displayed: 47, 47, 47\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 298/1000 --- L(Train): 0.0076738 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.612 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.327 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.78 1 + 0.042 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 48, 0, 0, 48, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 48\n",
      "value_reward_not_displayed: 48, 48, 48\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 299/1000 --- L(Train): 0.0076702 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.612 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.326 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.781 1 + 0.042 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 49, 0, 0, 49, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 49\n",
      "value_reward_not_displayed: 49, 49, 49\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 300/1000 --- L(Train): 0.0076670 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.613 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.325 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.782 1 + 0.041 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 50, 0, 0, 50, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 50\n",
      "value_reward_not_displayed: 50, 50, 50\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 301/1000 --- L(Train): 0.0076632 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.614 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.324 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.041 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 51, 0, 0, 51, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 51\n",
      "value_reward_not_displayed: 51, 51, 51\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 302/1000 --- L(Train): 0.0076600 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.615 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.324 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.783 1 + 0.04 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 52, 0, 0, 52, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 52\n",
      "value_reward_not_displayed: 52, 52, 52\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 303/1000 --- L(Train): 0.0076568 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.615 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.323 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.784 1 + 0.04 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 53, 0, 0, 53, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 53\n",
      "value_reward_not_displayed: 53, 53, 53\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 304/1000 --- L(Train): 0.0076533 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.616 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.322 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.784 1 + 0.039 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 54, 0, 0, 54, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 54\n",
      "value_reward_not_displayed: 54, 54, 54\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 305/1000 --- L(Train): 0.0076500 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.617 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.321 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.039 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 55, 0, 0, 55, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 55\n",
      "value_reward_not_displayed: 55, 55, 55\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 306/1000 --- L(Train): 0.0076469 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.618 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.321 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.038 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 56, 0, 0, 56, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 56\n",
      "value_reward_not_displayed: 56, 56, 56\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 307/1000 --- L(Train): 0.0076437 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.619 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.32 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.037 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 57, 0, 0, 57, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 57\n",
      "value_reward_not_displayed: 57, 57, 57\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 308/1000 --- L(Train): 0.0076405 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.619 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.319 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.037 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 58, 0, 0, 58, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 58\n",
      "value_reward_not_displayed: 58, 58, 58\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 309/1000 --- L(Train): 0.0076375 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.62 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.318 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.036 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 59, 0, 0, 59, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 59\n",
      "value_reward_not_displayed: 59, 59, 59\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 310/1000 --- L(Train): 0.0076341 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.621 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.317 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.036 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 60, 0, 0, 60, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 60\n",
      "value_reward_not_displayed: 60, 60, 60\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 311/1000 --- L(Train): 0.0076311 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.622 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.317 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.035 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 61, 0, 0, 61, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 61\n",
      "value_reward_not_displayed: 61, 61, 61\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 312/1000 --- L(Train): 0.0076281 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.622 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.316 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.035 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 62, 0, 0, 62, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 62\n",
      "value_reward_not_displayed: 62, 62, 62\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 313/1000 --- L(Train): 0.0076251 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.623 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.315 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.035 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 63, 0, 0, 63, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 63\n",
      "value_reward_not_displayed: 63, 63, 63\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 314/1000 --- L(Train): 0.0076220 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.624 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.314 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.034 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 64, 0, 0, 64, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 64\n",
      "value_reward_not_displayed: 64, 64, 64\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 315/1000 --- L(Train): 0.0076186 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.625 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.314 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.785 1 + 0.034 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 65, 0, 0, 65, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 65\n",
      "value_reward_not_displayed: 65, 65, 65\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 316/1000 --- L(Train): 0.0076158 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.626 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.313 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.033 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 66, 0, 0, 66, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 66\n",
      "value_reward_not_displayed: 66, 66, 66\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 317/1000 --- L(Train): 0.0076131 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.626 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.312 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.033 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 67, 0, 0, 67, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 67\n",
      "value_reward_not_displayed: 67, 67, 67\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 318/1000 --- L(Train): 0.0076103 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.627 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.311 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.033 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 68, 0, 0, 68, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 68\n",
      "value_reward_not_displayed: 68, 68, 68\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 319/1000 --- L(Train): 0.0076073 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.628 reward + -0.031 value_reward_chosen^2 + -0.387 value_reward_chosen*reward + 0.31 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.786 1 + 0.032 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 69, 0, 0, 69, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 69\n",
      "value_reward_not_displayed: 69, 69, 69\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 320/1000 --- L(Train): 0.0076044 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.629 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.31 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.032 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 70, 0, 0, 70, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 70\n",
      "value_reward_not_displayed: 70, 70, 70\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 321/1000 --- L(Train): 0.0076015 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.63 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.309 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.032 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 71, 0, 0, 71, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 71\n",
      "value_reward_not_displayed: 71, 71, 71\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 322/1000 --- L(Train): 0.0075987 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.63 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.308 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.032 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 72, 0, 0, 72, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 72\n",
      "value_reward_not_displayed: 72, 72, 72\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 323/1000 --- L(Train): 0.0075961 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.631 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.307 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.787 1 + 0.031 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 73, 0, 0, 73, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 73\n",
      "value_reward_not_displayed: 73, 73, 73\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 324/1000 --- L(Train): 0.0075934 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.632 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.306 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.031 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 74, 0, 0, 74, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 74\n",
      "value_reward_not_displayed: 74, 74, 74\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 325/1000 --- L(Train): 0.0075906 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.001 1 + 0.941 value_reward_chosen[t] + 0.633 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.306 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.031 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 75, 0, 0, 75, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 75\n",
      "value_reward_not_displayed: 75, 75, 75\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 326/1000 --- L(Train): 0.0075878 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.634 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.305 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.031 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 76, 0, 0, 76, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 76\n",
      "value_reward_not_displayed: 76, 76, 76\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 327/1000 --- L(Train): 0.0075853 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.634 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.304 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.788 1 + 0.031 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.002 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 77, 0, 0, 77, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 77\n",
      "value_reward_not_displayed: 77, 77, 77\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 328/1000 --- L(Train): 0.0075824 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.635 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.303 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.03 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.002 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 78, 0, 0, 78, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 78\n",
      "value_reward_not_displayed: 78, 78, 78\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 329/1000 --- L(Train): 0.0075802 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.636 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.302 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.03 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 79, 0, 0, 79, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 79\n",
      "value_reward_not_displayed: 79, 79, 79\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 330/1000 --- L(Train): 0.0075776 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.637 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.302 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.03 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 80, 0, 0, 80, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 80\n",
      "value_reward_not_displayed: 80, 80, 80\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 331/1000 --- L(Train): 0.0075747 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.638 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.301 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.03 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 81, 0, 0, 81, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 81\n",
      "value_reward_not_displayed: 81, 81, 81\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 332/1000 --- L(Train): 0.0075717 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.638 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.3 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.03 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 82, 0, 0, 82, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 82\n",
      "value_reward_not_displayed: 82, 82, 82\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 333/1000 --- L(Train): 0.0075691 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.639 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.299 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.03 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 83, 0, 0, 83, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 83\n",
      "value_reward_not_displayed: 83, 83, 83\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 334/1000 --- L(Train): 0.0075670 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.64 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.298 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.029 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 84, 0, 0, 84, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 84\n",
      "value_reward_not_displayed: 84, 84, 84\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 335/1000 --- L(Train): 0.0075642 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.641 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.297 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.029 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 85, 0, 0, 85, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 85\n",
      "value_reward_not_displayed: 85, 85, 85\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 336/1000 --- L(Train): 0.0075619 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.642 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.297 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.029 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 86, 0, 0, 86, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 86\n",
      "value_reward_not_displayed: 86, 86, 86\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 337/1000 --- L(Train): 0.0075593 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.642 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.296 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.029 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 87, 0, 0, 87, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 87\n",
      "value_reward_not_displayed: 87, 87, 87\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 338/1000 --- L(Train): 0.0075572 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.643 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.295 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.029 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.0 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 88, 0, 0, 88, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 88\n",
      "value_reward_not_displayed: 88, 88, 88\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 339/1000 --- L(Train): 0.0075541 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.644 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.294 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.789 1 + 0.029 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 89, 0, 0, 89, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 89\n",
      "value_reward_not_displayed: 89, 89, 89\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 340/1000 --- L(Train): 0.0075523 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.645 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.293 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.029 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.0 value_reward_not_displayed[t] + -0.002 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 90, 0, 0, 90, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 90\n",
      "value_reward_not_displayed: 90, 90, 90\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 341/1000 --- L(Train): 0.0075496 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.646 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.293 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.029 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 91, 0, 0, 91, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 91\n",
      "value_reward_not_displayed: 91, 91, 91\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 342/1000 --- L(Train): 0.0075471 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.647 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.292 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.029 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 92, 0, 0, 92, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 92\n",
      "value_reward_not_displayed: 92, 92, 92\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 343/1000 --- L(Train): 0.0075444 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.647 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.291 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 0.999 value_reward_not_displayed[t] + 0.001 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 93, 0, 0, 93, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 93\n",
      "value_reward_not_displayed: 93, 93, 93\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 344/1000 --- L(Train): 0.0075426 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.648 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.29 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 94, 0, 0, 94, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 94\n",
      "value_reward_not_displayed: 94, 94, 94\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 345/1000 --- L(Train): 0.0075401 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.649 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.289 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.002 1 + 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 95, 0, 0, 95, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 95\n",
      "value_reward_not_displayed: 95, 95, 95\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 346/1000 --- L(Train): 0.0075376 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.65 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.288 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.001 1 + 1.001 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 96, 0, 0, 96, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 96\n",
      "value_reward_not_displayed: 96, 96, 96\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 347/1000 --- L(Train): 0.0075351 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.651 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.288 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = -0.0 1 + 1.002 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 97, 0, 0, 97, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 97\n",
      "value_reward_not_displayed: 97, 97, 97\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 348/1000 --- L(Train): 0.0075332 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = -0.0 1 + 0.941 value_reward_chosen[t] + 0.652 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.287 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.001 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 98, 0, 0, 98, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 98\n",
      "value_reward_not_displayed: 98, 98, 98\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 349/1000 --- L(Train): 0.0075309 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.652 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.286 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.001 1 + 1.0 value_reward_not_displayed[t] + -0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 99, 0, 0, 99, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 99\n",
      "value_reward_not_displayed: 99, 99, 99\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 350/1000 --- L(Train): 0.0075284 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 11):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.653 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.285 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.999 value_reward_not_displayed[t] + 0.0 value_reward_not_displayed^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 100, 0, 0, 100, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 100\n",
      "value_reward_not_displayed: -, 100, 100\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 351/1000 --- L(Train): 0.0075256 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 10):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.654 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.284 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] + -0.0 value_reward_not_chosen^2 \n",
      "value_reward_not_displayed[t+1] = 0.998 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 101, 0, 0, 101, 0, 0\n",
      "value_reward_not_chosen: 0, 0, 101\n",
      "value_reward_not_displayed: -, 101, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 352/1000 --- L(Train): 0.0075234 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 9):\n",
      "value_reward_chosen[t+1] = 0.0 1 + 0.941 value_reward_chosen[t] + 0.655 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.283 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 0.999 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: 102, 0, 0, 102, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, 102, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 353/1000 --- L(Train): 0.0075222 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_chosen[t+1] = 0.941 value_reward_chosen[t] + 0.656 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.283 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.001 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, 103, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, 103, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 354/1000 --- L(Train): 0.0076400 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.941 value_reward_chosen[t] + 0.657 reward + -0.031 value_reward_chosen^2 + -0.388 value_reward_chosen*reward + 0.282 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, 104, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 355/1000 --- L(Train): 0.0078121 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.941 value_reward_chosen[t] + 0.657 reward + -0.388 value_reward_chosen*reward + 0.281 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 356/1000 --- L(Train): 0.0078437 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.937 value_reward_chosen[t] + 0.657 reward + -0.396 value_reward_chosen*reward + 0.279 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 357/1000 --- L(Train): 0.0077559 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.929 value_reward_chosen[t] + 0.657 reward + -0.408 value_reward_chosen*reward + 0.277 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 358/1000 --- L(Train): 0.0077391 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.92 value_reward_chosen[t] + 0.656 reward + -0.421 value_reward_chosen*reward + 0.274 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 359/1000 --- L(Train): 0.0077513 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.913 value_reward_chosen[t] + 0.655 reward + -0.431 value_reward_chosen*reward + 0.271 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 360/1000 --- L(Train): 0.0077440 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.908 value_reward_chosen[t] + 0.655 reward + -0.434 value_reward_chosen*reward + 0.269 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 361/1000 --- L(Train): 0.0077121 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.906 value_reward_chosen[t] + 0.655 reward + -0.431 value_reward_chosen*reward + 0.268 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 362/1000 --- L(Train): 0.0076812 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.907 value_reward_chosen[t] + 0.656 reward + -0.423 value_reward_chosen*reward + 0.267 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 363/1000 --- L(Train): 0.0076745 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.91 value_reward_chosen[t] + 0.658 reward + -0.411 value_reward_chosen*reward + 0.267 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 364/1000 --- L(Train): 0.0076857 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.913 value_reward_chosen[t] + 0.659 reward + -0.399 value_reward_chosen*reward + 0.267 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 365/1000 --- L(Train): 0.0076800 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.915 value_reward_chosen[t] + 0.661 reward + -0.388 value_reward_chosen*reward + 0.267 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 366/1000 --- L(Train): 0.0076537 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.916 value_reward_chosen[t] + 0.661 reward + -0.38 value_reward_chosen*reward + 0.266 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 367/1000 --- L(Train): 0.0076277 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.915 value_reward_chosen[t] + 0.662 reward + -0.377 value_reward_chosen*reward + 0.265 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 368/1000 --- L(Train): 0.0076183 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.911 value_reward_chosen[t] + 0.662 reward + -0.377 value_reward_chosen*reward + 0.263 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 369/1000 --- L(Train): 0.0076200 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.906 value_reward_chosen[t] + 0.661 reward + -0.379 value_reward_chosen*reward + 0.261 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 370/1000 --- L(Train): 0.0076212 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.661 reward + -0.383 value_reward_chosen*reward + 0.258 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 371/1000 --- L(Train): 0.0076263 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.896 value_reward_chosen[t] + 0.66 reward + -0.387 value_reward_chosen*reward + 0.256 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 372/1000 --- L(Train): 0.0076120 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.892 value_reward_chosen[t] + 0.66 reward + -0.389 value_reward_chosen*reward + 0.254 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 373/1000 --- L(Train): 0.0075974 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.89 value_reward_chosen[t] + 0.66 reward + -0.388 value_reward_chosen*reward + 0.252 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 374/1000 --- L(Train): 0.0075903 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.89 value_reward_chosen[t] + 0.66 reward + -0.386 value_reward_chosen*reward + 0.251 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 375/1000 --- L(Train): 0.0075907 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.891 value_reward_chosen[t] + 0.661 reward + -0.381 value_reward_chosen*reward + 0.25 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 376/1000 --- L(Train): 0.0075901 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.893 value_reward_chosen[t] + 0.662 reward + -0.376 value_reward_chosen*reward + 0.249 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 377/1000 --- L(Train): 0.0075844 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.896 value_reward_chosen[t] + 0.663 reward + -0.37 value_reward_chosen*reward + 0.248 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 378/1000 --- L(Train): 0.0075759 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.897 value_reward_chosen[t] + 0.664 reward + -0.366 value_reward_chosen*reward + 0.247 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 379/1000 --- L(Train): 0.0075701 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.664 reward + -0.363 value_reward_chosen*reward + 0.246 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 380/1000 --- L(Train): 0.0075671 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.665 reward + -0.363 value_reward_chosen*reward + 0.245 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 381/1000 --- L(Train): 0.0075655 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.665 reward + -0.364 value_reward_chosen*reward + 0.243 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 382/1000 --- L(Train): 0.0075621 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.896 value_reward_chosen[t] + 0.665 reward + -0.367 value_reward_chosen*reward + 0.242 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 383/1000 --- L(Train): 0.0075571 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.895 value_reward_chosen[t] + 0.665 reward + -0.37 value_reward_chosen*reward + 0.24 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 384/1000 --- L(Train): 0.0075521 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.893 value_reward_chosen[t] + 0.665 reward + -0.373 value_reward_chosen*reward + 0.238 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 385/1000 --- L(Train): 0.0075487 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.892 value_reward_chosen[t] + 0.666 reward + -0.375 value_reward_chosen*reward + 0.237 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 386/1000 --- L(Train): 0.0075472 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.892 value_reward_chosen[t] + 0.666 reward + -0.376 value_reward_chosen*reward + 0.236 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 387/1000 --- L(Train): 0.0075464 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.893 value_reward_chosen[t] + 0.667 reward + -0.375 value_reward_chosen*reward + 0.235 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 388/1000 --- L(Train): 0.0075454 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.895 value_reward_chosen[t] + 0.668 reward + -0.374 value_reward_chosen*reward + 0.234 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 389/1000 --- L(Train): 0.0075424 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.897 value_reward_chosen[t] + 0.669 reward + -0.372 value_reward_chosen*reward + 0.233 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 390/1000 --- L(Train): 0.0075384 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.67 reward + -0.37 value_reward_chosen*reward + 0.233 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 391/1000 --- L(Train): 0.0075348 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.671 reward + -0.369 value_reward_chosen*reward + 0.232 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 392/1000 --- L(Train): 0.0075329 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.673 reward + -0.369 value_reward_chosen*reward + 0.231 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 393/1000 --- L(Train): 0.0075320 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.673 reward + -0.369 value_reward_chosen*reward + 0.231 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 394/1000 --- L(Train): 0.0075301 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.674 reward + -0.371 value_reward_chosen*reward + 0.23 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 395/1000 --- L(Train): 0.0075282 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.675 reward + -0.373 value_reward_chosen*reward + 0.229 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 396/1000 --- L(Train): 0.0075260 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.676 reward + -0.375 value_reward_chosen*reward + 0.228 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 397/1000 --- L(Train): 0.0075239 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.677 reward + -0.377 value_reward_chosen*reward + 0.227 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 398/1000 --- L(Train): 0.0075218 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.678 reward + -0.378 value_reward_chosen*reward + 0.226 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 399/1000 --- L(Train): 0.0075201 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.679 reward + -0.379 value_reward_chosen*reward + 0.225 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 400/1000 --- L(Train): 0.0075183 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.68 reward + -0.378 value_reward_chosen*reward + 0.225 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 401/1000 --- L(Train): 0.0075163 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.681 reward + -0.378 value_reward_chosen*reward + 0.224 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 402/1000 --- L(Train): 0.0075143 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.682 reward + -0.377 value_reward_chosen*reward + 0.223 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 403/1000 --- L(Train): 0.0075122 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.684 reward + -0.376 value_reward_chosen*reward + 0.223 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 404/1000 --- L(Train): 0.0075099 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.685 reward + -0.375 value_reward_chosen*reward + 0.222 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 405/1000 --- L(Train): 0.0075077 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.902 value_reward_chosen[t] + 0.686 reward + -0.375 value_reward_chosen*reward + 0.222 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 406/1000 --- L(Train): 0.0075062 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.687 reward + -0.376 value_reward_chosen*reward + 0.221 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 407/1000 --- L(Train): 0.0075047 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.901 value_reward_chosen[t] + 0.688 reward + -0.376 value_reward_chosen*reward + 0.22 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 408/1000 --- L(Train): 0.0075026 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.689 reward + -0.377 value_reward_chosen*reward + 0.219 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 409/1000 --- L(Train): 0.0075005 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.69 reward + -0.378 value_reward_chosen*reward + 0.218 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 410/1000 --- L(Train): 0.0074991 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.691 reward + -0.379 value_reward_chosen*reward + 0.218 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 411/1000 --- L(Train): 0.0074973 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.692 reward + -0.379 value_reward_chosen*reward + 0.217 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 412/1000 --- L(Train): 0.0074962 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.693 reward + -0.379 value_reward_chosen*reward + 0.216 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 413/1000 --- L(Train): 0.0074948 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.694 reward + -0.379 value_reward_chosen*reward + 0.215 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 414/1000 --- L(Train): 0.0075028 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.695 reward + -0.379 value_reward_chosen*reward + 0.214 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 415/1000 --- L(Train): 0.0074949 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.696 reward + -0.378 value_reward_chosen*reward + 0.214 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 416/1000 --- L(Train): 0.0074905 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.697 reward + -0.377 value_reward_chosen*reward + 0.213 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 417/1000 --- L(Train): 0.0074918 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.698 reward + -0.377 value_reward_chosen*reward + 0.212 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 418/1000 --- L(Train): 0.0074931 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.699 reward + -0.377 value_reward_chosen*reward + 0.211 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 419/1000 --- L(Train): 0.0074904 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.7 reward + -0.377 value_reward_chosen*reward + 0.21 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 420/1000 --- L(Train): 0.0074858 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.701 reward + -0.377 value_reward_chosen*reward + 0.209 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 421/1000 --- L(Train): 0.0074829 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.702 reward + -0.378 value_reward_chosen*reward + 0.208 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 422/1000 --- L(Train): 0.0074822 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.702 reward + -0.378 value_reward_chosen*reward + 0.207 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 423/1000 --- L(Train): 0.0074822 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.703 reward + -0.379 value_reward_chosen*reward + 0.206 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 424/1000 --- L(Train): 0.0074808 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.704 reward + -0.379 value_reward_chosen*reward + 0.205 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 425/1000 --- L(Train): 0.0074779 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.705 reward + -0.378 value_reward_chosen*reward + 0.204 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 426/1000 --- L(Train): 0.0074754 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.706 reward + -0.378 value_reward_chosen*reward + 0.203 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 427/1000 --- L(Train): 0.0074740 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.707 reward + -0.378 value_reward_chosen*reward + 0.202 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 428/1000 --- L(Train): 0.0074728 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.708 reward + -0.377 value_reward_chosen*reward + 0.202 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 429/1000 --- L(Train): 0.0074717 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.709 reward + -0.377 value_reward_chosen*reward + 0.201 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 430/1000 --- L(Train): 0.0074699 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.71 reward + -0.377 value_reward_chosen*reward + 0.2 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 431/1000 --- L(Train): 0.0074677 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.711 reward + -0.377 value_reward_chosen*reward + 0.199 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 432/1000 --- L(Train): 0.0074655 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.711 reward + -0.377 value_reward_chosen*reward + 0.198 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 433/1000 --- L(Train): 0.0074646 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.712 reward + -0.377 value_reward_chosen*reward + 0.197 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 434/1000 --- L(Train): 0.0074636 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.713 reward + -0.378 value_reward_chosen*reward + 0.196 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 435/1000 --- L(Train): 0.0074617 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.714 reward + -0.378 value_reward_chosen*reward + 0.195 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 436/1000 --- L(Train): 0.0074597 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.715 reward + -0.378 value_reward_chosen*reward + 0.194 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 437/1000 --- L(Train): 0.0074581 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.716 reward + -0.378 value_reward_chosen*reward + 0.193 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 438/1000 --- L(Train): 0.0074566 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.717 reward + -0.378 value_reward_chosen*reward + 0.192 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 439/1000 --- L(Train): 0.0074548 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.718 reward + -0.377 value_reward_chosen*reward + 0.191 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 440/1000 --- L(Train): 0.0074539 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.719 reward + -0.377 value_reward_chosen*reward + 0.19 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 441/1000 --- L(Train): 0.0074529 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.72 reward + -0.377 value_reward_chosen*reward + 0.189 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 442/1000 --- L(Train): 0.0074514 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.72 reward + -0.377 value_reward_chosen*reward + 0.188 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 443/1000 --- L(Train): 0.0074497 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.721 reward + -0.377 value_reward_chosen*reward + 0.187 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 444/1000 --- L(Train): 0.0074482 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.722 reward + -0.377 value_reward_chosen*reward + 0.186 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 445/1000 --- L(Train): 0.0074471 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.723 reward + -0.377 value_reward_chosen*reward + 0.185 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 446/1000 --- L(Train): 0.0074456 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.724 reward + -0.377 value_reward_chosen*reward + 0.184 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 447/1000 --- L(Train): 0.0074443 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.725 reward + -0.377 value_reward_chosen*reward + 0.183 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 448/1000 --- L(Train): 0.0074432 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.726 reward + -0.377 value_reward_chosen*reward + 0.182 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 449/1000 --- L(Train): 0.0074419 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.727 reward + -0.377 value_reward_chosen*reward + 0.181 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 450/1000 --- L(Train): 0.0074402 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.728 reward + -0.377 value_reward_chosen*reward + 0.18 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 451/1000 --- L(Train): 0.0074391 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.729 reward + -0.377 value_reward_chosen*reward + 0.179 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.79 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 452/1000 --- L(Train): 0.0074380 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.73 reward + -0.377 value_reward_chosen*reward + 0.178 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 453/1000 --- L(Train): 0.0074363 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.731 reward + -0.377 value_reward_chosen*reward + 0.177 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 454/1000 --- L(Train): 0.0074349 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.732 reward + -0.377 value_reward_chosen*reward + 0.176 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 455/1000 --- L(Train): 0.0074335 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.733 reward + -0.377 value_reward_chosen*reward + 0.175 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 456/1000 --- L(Train): 0.0074340 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.734 reward + -0.377 value_reward_chosen*reward + 0.174 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 457/1000 --- L(Train): 0.0074344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.735 reward + -0.377 value_reward_chosen*reward + 0.173 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 458/1000 --- L(Train): 0.0074352 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.736 reward + -0.377 value_reward_chosen*reward + 0.172 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 459/1000 --- L(Train): 0.0074310 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.737 reward + -0.377 value_reward_chosen*reward + 0.172 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 460/1000 --- L(Train): 0.0074293 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.738 reward + -0.377 value_reward_chosen*reward + 0.171 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 461/1000 --- L(Train): 0.0074294 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.739 reward + -0.377 value_reward_chosen*reward + 0.17 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 462/1000 --- L(Train): 0.0074375 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.74 reward + -0.377 value_reward_chosen*reward + 0.169 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 463/1000 --- L(Train): 0.0074295 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.741 reward + -0.377 value_reward_chosen*reward + 0.168 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 464/1000 --- L(Train): 0.0074241 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.742 reward + -0.377 value_reward_chosen*reward + 0.167 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 465/1000 --- L(Train): 0.0074286 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.743 reward + -0.377 value_reward_chosen*reward + 0.166 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 466/1000 --- L(Train): 0.0074308 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.744 reward + -0.377 value_reward_chosen*reward + 0.165 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 467/1000 --- L(Train): 0.0074286 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.745 reward + -0.377 value_reward_chosen*reward + 0.164 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 468/1000 --- L(Train): 0.0074320 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.746 reward + -0.377 value_reward_chosen*reward + 0.163 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 469/1000 --- L(Train): 0.0074239 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.747 reward + -0.377 value_reward_chosen*reward + 0.162 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 470/1000 --- L(Train): 0.0074207 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.748 reward + -0.377 value_reward_chosen*reward + 0.161 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 471/1000 --- L(Train): 0.0074229 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.749 reward + -0.377 value_reward_chosen*reward + 0.16 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 472/1000 --- L(Train): 0.0074246 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.75 reward + -0.377 value_reward_chosen*reward + 0.159 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 473/1000 --- L(Train): 0.0074212 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.751 reward + -0.377 value_reward_chosen*reward + 0.158 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 474/1000 --- L(Train): 0.0074162 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.752 reward + -0.377 value_reward_chosen*reward + 0.157 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 475/1000 --- L(Train): 0.0074131 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.753 reward + -0.377 value_reward_chosen*reward + 0.156 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 476/1000 --- L(Train): 0.0074131 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.754 reward + -0.377 value_reward_chosen*reward + 0.155 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 477/1000 --- L(Train): 0.0074140 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.755 reward + -0.377 value_reward_chosen*reward + 0.154 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 478/1000 --- L(Train): 0.0074130 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.756 reward + -0.377 value_reward_chosen*reward + 0.153 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 479/1000 --- L(Train): 0.0074101 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.757 reward + -0.377 value_reward_chosen*reward + 0.152 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 480/1000 --- L(Train): 0.0074070 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.758 reward + -0.377 value_reward_chosen*reward + 0.151 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 481/1000 --- L(Train): 0.0074056 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.759 reward + -0.377 value_reward_chosen*reward + 0.15 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 482/1000 --- L(Train): 0.0074052 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.76 reward + -0.377 value_reward_chosen*reward + 0.149 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 483/1000 --- L(Train): 0.0074048 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.761 reward + -0.377 value_reward_chosen*reward + 0.148 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 484/1000 --- L(Train): 0.0074033 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.762 reward + -0.377 value_reward_chosen*reward + 0.147 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 485/1000 --- L(Train): 0.0074009 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.763 reward + -0.377 value_reward_chosen*reward + 0.146 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 486/1000 --- L(Train): 0.0073986 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.764 reward + -0.377 value_reward_chosen*reward + 0.145 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 487/1000 --- L(Train): 0.0073974 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.765 reward + -0.377 value_reward_chosen*reward + 0.144 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 488/1000 --- L(Train): 0.0073969 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.766 reward + -0.377 value_reward_chosen*reward + 0.143 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 489/1000 --- L(Train): 0.0073958 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.767 reward + -0.377 value_reward_chosen*reward + 0.142 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 490/1000 --- L(Train): 0.0073942 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.768 reward + -0.377 value_reward_chosen*reward + 0.141 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 491/1000 --- L(Train): 0.0073921 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.769 reward + -0.377 value_reward_chosen*reward + 0.14 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 492/1000 --- L(Train): 0.0073905 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.77 reward + -0.377 value_reward_chosen*reward + 0.139 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 493/1000 --- L(Train): 0.0073892 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.771 reward + -0.377 value_reward_chosen*reward + 0.138 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 494/1000 --- L(Train): 0.0073880 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.772 reward + -0.377 value_reward_chosen*reward + 0.137 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 495/1000 --- L(Train): 0.0073866 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.773 reward + -0.377 value_reward_chosen*reward + 0.136 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 496/1000 --- L(Train): 0.0073857 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.774 reward + -0.377 value_reward_chosen*reward + 0.135 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 497/1000 --- L(Train): 0.0073846 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.775 reward + -0.377 value_reward_chosen*reward + 0.134 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 498/1000 --- L(Train): 0.0073833 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.776 reward + -0.377 value_reward_chosen*reward + 0.133 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 499/1000 --- L(Train): 0.0073824 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.777 reward + -0.377 value_reward_chosen*reward + 0.132 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 500/1000 --- L(Train): 0.0073816 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.778 reward + -0.377 value_reward_chosen*reward + 0.131 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 501/1000 --- L(Train): 0.0073807 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.779 reward + -0.377 value_reward_chosen*reward + 0.13 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 502/1000 --- L(Train): 0.0073797 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.78 reward + -0.377 value_reward_chosen*reward + 0.129 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 503/1000 --- L(Train): 0.0073785 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.781 reward + -0.377 value_reward_chosen*reward + 0.128 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 504/1000 --- L(Train): 0.0073782 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.782 reward + -0.377 value_reward_chosen*reward + 0.127 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 505/1000 --- L(Train): 0.0073765 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.783 reward + -0.377 value_reward_chosen*reward + 0.126 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 506/1000 --- L(Train): 0.0073747 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.784 reward + -0.377 value_reward_chosen*reward + 0.125 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 507/1000 --- L(Train): 0.0073734 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.785 reward + -0.377 value_reward_chosen*reward + 0.124 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 508/1000 --- L(Train): 0.0073723 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.786 reward + -0.377 value_reward_chosen*reward + 0.123 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 509/1000 --- L(Train): 0.0073712 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.787 reward + -0.377 value_reward_chosen*reward + 0.122 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 510/1000 --- L(Train): 0.0073703 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.788 reward + -0.377 value_reward_chosen*reward + 0.121 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 511/1000 --- L(Train): 0.0073720 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.789 reward + -0.377 value_reward_chosen*reward + 0.119 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 512/1000 --- L(Train): 0.0073678 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.79 reward + -0.377 value_reward_chosen*reward + 0.118 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.028 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 513/1000 --- L(Train): 0.0073681 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.791 reward + -0.377 value_reward_chosen*reward + 0.117 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 514/1000 --- L(Train): 0.0073676 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.792 reward + -0.377 value_reward_chosen*reward + 0.116 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 515/1000 --- L(Train): 0.0073646 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.793 reward + -0.377 value_reward_chosen*reward + 0.115 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 516/1000 --- L(Train): 0.0073633 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.794 reward + -0.377 value_reward_chosen*reward + 0.114 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 517/1000 --- L(Train): 0.0073633 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.795 reward + -0.377 value_reward_chosen*reward + 0.113 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 518/1000 --- L(Train): 0.0073617 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.796 reward + -0.377 value_reward_chosen*reward + 0.112 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 519/1000 --- L(Train): 0.0073606 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.797 reward + -0.377 value_reward_chosen*reward + 0.111 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 520/1000 --- L(Train): 0.0073590 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.798 reward + -0.377 value_reward_chosen*reward + 0.11 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 521/1000 --- L(Train): 0.0073587 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.799 reward + -0.377 value_reward_chosen*reward + 0.109 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 522/1000 --- L(Train): 0.0073576 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.8 reward + -0.377 value_reward_chosen*reward + 0.108 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 523/1000 --- L(Train): 0.0073559 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.802 reward + -0.377 value_reward_chosen*reward + 0.107 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 524/1000 --- L(Train): 0.0073548 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.803 reward + -0.377 value_reward_chosen*reward + 0.106 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 525/1000 --- L(Train): 0.0073538 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.804 reward + -0.377 value_reward_chosen*reward + 0.105 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 526/1000 --- L(Train): 0.0073525 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.805 reward + -0.377 value_reward_chosen*reward + 0.104 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 527/1000 --- L(Train): 0.0073507 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.806 reward + -0.377 value_reward_chosen*reward + 0.103 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 528/1000 --- L(Train): 0.0073498 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.807 reward + -0.377 value_reward_chosen*reward + 0.102 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 529/1000 --- L(Train): 0.0073490 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.808 reward + -0.377 value_reward_chosen*reward + 0.101 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 530/1000 --- L(Train): 0.0073481 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.809 reward + -0.377 value_reward_chosen*reward + 0.1 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 531/1000 --- L(Train): 0.0073462 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.81 reward + -0.377 value_reward_chosen*reward + 0.099 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 532/1000 --- L(Train): 0.0073454 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.811 reward + -0.377 value_reward_chosen*reward + 0.098 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 533/1000 --- L(Train): 0.0073445 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.812 reward + -0.377 value_reward_chosen*reward + 0.096 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 534/1000 --- L(Train): 0.0073431 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.813 reward + -0.377 value_reward_chosen*reward + 0.095 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 535/1000 --- L(Train): 0.0073425 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.814 reward + -0.377 value_reward_chosen*reward + 0.094 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 536/1000 --- L(Train): 0.0073416 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.815 reward + -0.377 value_reward_chosen*reward + 0.093 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 537/1000 --- L(Train): 0.0073411 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.816 reward + -0.377 value_reward_chosen*reward + 0.092 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 538/1000 --- L(Train): 0.0073399 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.817 reward + -0.377 value_reward_chosen*reward + 0.091 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 539/1000 --- L(Train): 0.0073386 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.818 reward + -0.377 value_reward_chosen*reward + 0.09 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 540/1000 --- L(Train): 0.0073469 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.82 reward + -0.377 value_reward_chosen*reward + 0.089 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 541/1000 --- L(Train): 0.0073394 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.821 reward + -0.377 value_reward_chosen*reward + 0.088 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 542/1000 --- L(Train): 0.0073364 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.822 reward + -0.377 value_reward_chosen*reward + 0.087 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 543/1000 --- L(Train): 0.0073377 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.823 reward + -0.377 value_reward_chosen*reward + 0.086 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 544/1000 --- L(Train): 0.0073392 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.824 reward + -0.377 value_reward_chosen*reward + 0.085 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 545/1000 --- L(Train): 0.0073362 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.825 reward + -0.377 value_reward_chosen*reward + 0.084 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 546/1000 --- L(Train): 0.0073324 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.826 reward + -0.377 value_reward_chosen*reward + 0.083 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 547/1000 --- L(Train): 0.0073301 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.827 reward + -0.377 value_reward_chosen*reward + 0.082 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 548/1000 --- L(Train): 0.0073299 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.828 reward + -0.377 value_reward_chosen*reward + 0.08 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 549/1000 --- L(Train): 0.0073303 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.829 reward + -0.377 value_reward_chosen*reward + 0.079 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 550/1000 --- L(Train): 0.0073288 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.83 reward + -0.377 value_reward_chosen*reward + 0.078 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 551/1000 --- L(Train): 0.0073259 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.831 reward + -0.377 value_reward_chosen*reward + 0.077 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 552/1000 --- L(Train): 0.0073238 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.832 reward + -0.377 value_reward_chosen*reward + 0.076 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 553/1000 --- L(Train): 0.0073232 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.834 reward + -0.377 value_reward_chosen*reward + 0.075 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 554/1000 --- L(Train): 0.0073230 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.835 reward + -0.377 value_reward_chosen*reward + 0.074 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 555/1000 --- L(Train): 0.0073226 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.836 reward + -0.377 value_reward_chosen*reward + 0.073 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 556/1000 --- L(Train): 0.0073214 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.837 reward + -0.377 value_reward_chosen*reward + 0.072 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 557/1000 --- L(Train): 0.0073197 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.838 reward + -0.377 value_reward_chosen*reward + 0.071 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 558/1000 --- L(Train): 0.0073184 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.839 reward + -0.377 value_reward_chosen*reward + 0.07 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 559/1000 --- L(Train): 0.0073178 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.84 reward + -0.377 value_reward_chosen*reward + 0.069 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 560/1000 --- L(Train): 0.0073178 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.841 reward + -0.377 value_reward_chosen*reward + 0.067 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 561/1000 --- L(Train): 0.0073167 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.842 reward + -0.377 value_reward_chosen*reward + 0.066 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 562/1000 --- L(Train): 0.0073149 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.843 reward + -0.377 value_reward_chosen*reward + 0.065 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 563/1000 --- L(Train): 0.0073136 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.844 reward + -0.377 value_reward_chosen*reward + 0.064 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 564/1000 --- L(Train): 0.0073130 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.846 reward + -0.377 value_reward_chosen*reward + 0.063 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 565/1000 --- L(Train): 0.0073126 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.847 reward + -0.377 value_reward_chosen*reward + 0.062 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 566/1000 --- L(Train): 0.0073112 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.848 reward + -0.377 value_reward_chosen*reward + 0.061 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 567/1000 --- L(Train): 0.0073095 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.849 reward + -0.377 value_reward_chosen*reward + 0.06 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 568/1000 --- L(Train): 0.0073085 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.85 reward + -0.377 value_reward_chosen*reward + 0.059 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 569/1000 --- L(Train): 0.0073077 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.851 reward + -0.377 value_reward_chosen*reward + 0.058 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 570/1000 --- L(Train): 0.0073066 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.852 reward + -0.377 value_reward_chosen*reward + 0.056 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 571/1000 --- L(Train): 0.0073052 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.853 reward + -0.377 value_reward_chosen*reward + 0.055 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 572/1000 --- L(Train): 0.0073044 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.854 reward + -0.377 value_reward_chosen*reward + 0.054 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 573/1000 --- L(Train): 0.0073038 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.855 reward + -0.377 value_reward_chosen*reward + 0.053 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 574/1000 --- L(Train): 0.0073028 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.857 reward + -0.377 value_reward_chosen*reward + 0.052 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 575/1000 --- L(Train): 0.0073018 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.858 reward + -0.377 value_reward_chosen*reward + 0.051 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 0\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 576/1000 --- L(Train): 0.0073008 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.859 reward + -0.377 value_reward_chosen*reward + 0.05 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 1\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 577/1000 --- L(Train): 0.0073001 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.86 reward + -0.377 value_reward_chosen*reward + 0.049 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 2\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 578/1000 --- L(Train): 0.0072992 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.861 reward + -0.377 value_reward_chosen*reward + 0.048 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 3\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 579/1000 --- L(Train): 0.0072981 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.862 reward + -0.377 value_reward_chosen*reward + 0.046 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 4\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 580/1000 --- L(Train): 0.0072972 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.863 reward + -0.377 value_reward_chosen*reward + 0.045 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 5\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 581/1000 --- L(Train): 0.0072964 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.864 reward + -0.377 value_reward_chosen*reward + 0.044 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 6\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 582/1000 --- L(Train): 0.0072956 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.865 reward + -0.377 value_reward_chosen*reward + 0.043 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 7\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 583/1000 --- L(Train): 0.0072945 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.867 reward + -0.377 value_reward_chosen*reward + 0.042 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 8\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 584/1000 --- L(Train): 0.0072936 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.868 reward + -0.377 value_reward_chosen*reward + 0.041 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 9\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 585/1000 --- L(Train): 0.0072925 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.869 reward + -0.377 value_reward_chosen*reward + 0.04 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 10\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 586/1000 --- L(Train): 0.0072917 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.87 reward + -0.377 value_reward_chosen*reward + 0.039 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 11\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 587/1000 --- L(Train): 0.0072909 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.871 reward + -0.377 value_reward_chosen*reward + 0.037 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 12\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 588/1000 --- L(Train): 0.0072899 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.872 reward + -0.377 value_reward_chosen*reward + 0.036 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 13\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 589/1000 --- L(Train): 0.0072889 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.873 reward + -0.377 value_reward_chosen*reward + 0.035 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 14\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 590/1000 --- L(Train): 0.0072886 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.874 reward + -0.377 value_reward_chosen*reward + 0.034 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 15\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 591/1000 --- L(Train): 0.0072869 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.876 reward + -0.377 value_reward_chosen*reward + 0.033 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 16\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 592/1000 --- L(Train): 0.0072865 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.877 reward + -0.377 value_reward_chosen*reward + 0.032 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 17\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 593/1000 --- L(Train): 0.0072860 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.878 reward + -0.377 value_reward_chosen*reward + 0.031 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 18\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 594/1000 --- L(Train): 0.0072850 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.879 reward + -0.377 value_reward_chosen*reward + 0.03 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 19\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 595/1000 --- L(Train): 0.0072839 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.88 reward + -0.377 value_reward_chosen*reward + 0.028 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 20\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 596/1000 --- L(Train): 0.0072827 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.881 reward + -0.377 value_reward_chosen*reward + 0.027 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 21\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 597/1000 --- L(Train): 0.0072820 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.882 reward + -0.377 value_reward_chosen*reward + 0.026 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 22\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 598/1000 --- L(Train): 0.0072817 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.884 reward + -0.377 value_reward_chosen*reward + 0.025 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 23\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 599/1000 --- L(Train): 0.0072808 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.885 reward + -0.377 value_reward_chosen*reward + 0.024 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 24\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 600/1000 --- L(Train): 0.0072797 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.886 reward + -0.377 value_reward_chosen*reward + 0.023 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 25\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 601/1000 --- L(Train): 0.0072789 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.887 reward + -0.377 value_reward_chosen*reward + 0.022 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 26\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 602/1000 --- L(Train): 0.0072780 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.888 reward + -0.377 value_reward_chosen*reward + 0.02 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 27\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 603/1000 --- L(Train): 0.0072770 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.889 reward + -0.377 value_reward_chosen*reward + 0.019 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 28\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 604/1000 --- L(Train): 0.0072763 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.89 reward + -0.377 value_reward_chosen*reward + 0.018 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 29\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 605/1000 --- L(Train): 0.0072754 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.892 reward + -0.377 value_reward_chosen*reward + 0.017 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 30\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 606/1000 --- L(Train): 0.0072743 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.893 reward + -0.377 value_reward_chosen*reward + 0.016 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 31\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 607/1000 --- L(Train): 0.0072735 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.894 reward + -0.377 value_reward_chosen*reward + 0.015 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 32\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 608/1000 --- L(Train): 0.0072727 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.895 reward + -0.377 value_reward_chosen*reward + 0.014 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 33\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 609/1000 --- L(Train): 0.0072723 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.896 reward + -0.377 value_reward_chosen*reward + 0.012 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 34\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 610/1000 --- L(Train): 0.0072717 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.897 reward + -0.377 value_reward_chosen*reward + 0.011 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 35\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 611/1000 --- L(Train): 0.0072704 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.898 reward + -0.377 value_reward_chosen*reward + 0.01 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 36\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 612/1000 --- L(Train): 0.0072697 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.9 reward + -0.377 value_reward_chosen*reward + 0.009 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 37\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 613/1000 --- L(Train): 0.0072694 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.901 reward + -0.377 value_reward_chosen*reward + 0.008 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 38\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 614/1000 --- L(Train): 0.0072681 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.902 reward + -0.377 value_reward_chosen*reward + 0.007 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 39\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 615/1000 --- L(Train): 0.0072669 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.903 reward + -0.377 value_reward_chosen*reward + 0.006 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 40\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 616/1000 --- L(Train): 0.0072666 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.904 reward + -0.377 value_reward_chosen*reward + 0.004 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 41\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 617/1000 --- L(Train): 0.0072660 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.905 reward + -0.377 value_reward_chosen*reward + 0.003 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 42\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 618/1000 --- L(Train): 0.0072651 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.907 reward + -0.377 value_reward_chosen*reward + 0.002 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 43\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 619/1000 --- L(Train): 0.0072644 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.908 reward + -0.377 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 44\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 620/1000 --- L(Train): 0.0072636 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.909 reward + -0.377 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 45\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 621/1000 --- L(Train): 0.0072629 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.91 reward + -0.377 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 46\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 622/1000 --- L(Train): 0.0072623 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.911 reward + -0.377 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 47\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 623/1000 --- L(Train): 0.0072612 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.912 reward + -0.378 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 48\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 624/1000 --- L(Train): 0.0072603 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.913 reward + -0.378 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 49\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 625/1000 --- L(Train): 0.0072592 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.914 reward + -0.379 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 50\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 626/1000 --- L(Train): 0.0072583 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.915 reward + -0.38 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 51\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 627/1000 --- L(Train): 0.0072576 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.916 reward + -0.381 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 52\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 628/1000 --- L(Train): 0.0072569 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.917 reward + -0.381 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 53\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 629/1000 --- L(Train): 0.0072559 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.918 reward + -0.381 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 54\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 630/1000 --- L(Train): 0.0072551 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.918 reward + -0.381 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 55\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 631/1000 --- L(Train): 0.0072544 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.898 value_reward_chosen[t] + 0.919 reward + -0.381 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 56\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 632/1000 --- L(Train): 0.0072537 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.381 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 57\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 633/1000 --- L(Train): 0.0072532 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.382 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 58\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 634/1000 --- L(Train): 0.0072523 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.921 reward + -0.382 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 59\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 635/1000 --- L(Train): 0.0072512 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.921 reward + -0.383 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 60\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 636/1000 --- L(Train): 0.0072508 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.922 reward + -0.383 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 61\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 637/1000 --- L(Train): 0.0072504 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.922 reward + -0.384 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 62\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 638/1000 --- L(Train): 0.0072496 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.922 reward + -0.384 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 63\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 639/1000 --- L(Train): 0.0072487 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 64\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 640/1000 --- L(Train): 0.0072483 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 65\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 641/1000 --- L(Train): 0.0072478 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 66\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 642/1000 --- L(Train): 0.0072470 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 67\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 643/1000 --- L(Train): 0.0072466 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 68\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 644/1000 --- L(Train): 0.0072460 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 69\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 645/1000 --- L(Train): 0.0072451 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 70\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 646/1000 --- L(Train): 0.0072443 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 71\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 647/1000 --- L(Train): 0.0072439 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 72\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 648/1000 --- L(Train): 0.0072432 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.386 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 73\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 649/1000 --- L(Train): 0.0072423 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.386 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 74\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 650/1000 --- L(Train): 0.0072414 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.386 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 75\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 651/1000 --- L(Train): 0.0072402 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.386 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 76\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 652/1000 --- L(Train): 0.0072508 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.386 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 77\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 653/1000 --- L(Train): 0.0072426 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.923 reward + -0.385 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 78\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 654/1000 --- L(Train): 0.0072392 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 79\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 655/1000 --- L(Train): 0.0072422 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 80\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 656/1000 --- L(Train): 0.0072437 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.002 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 81\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 657/1000 --- L(Train): 0.0072404 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.002 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 82\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 658/1000 --- L(Train): 0.0072364 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 83\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 659/1000 --- L(Train): 0.0072351 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.922 reward + -0.385 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 84\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 660/1000 --- L(Train): 0.0072378 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.921 reward + -0.385 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 85\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 661/1000 --- L(Train): 0.0072372 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.921 reward + -0.385 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 86\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 662/1000 --- L(Train): 0.0072365 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.385 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 87\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 663/1000 --- L(Train): 0.0072336 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 88\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 664/1000 --- L(Train): 0.0072319 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 89\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 665/1000 --- L(Train): 0.0072331 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.383 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 90\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 666/1000 --- L(Train): 0.0072337 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 91\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 667/1000 --- L(Train): 0.0072322 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.384 value_reward_chosen*reward + 0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 92\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 668/1000 --- L(Train): 0.0072307 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.384 value_reward_chosen*reward + -0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 93\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 669/1000 --- L(Train): 0.0072296 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.384 value_reward_chosen*reward + -0.0 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 94\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 670/1000 --- L(Train): 0.0072292 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.384 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 95\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 671/1000 --- L(Train): 0.0072286 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.384 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 96\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 672/1000 --- L(Train): 0.0072285 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward + 0.002 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 97\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 673/1000 --- L(Train): 0.0072278 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward + 0.002 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 98\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 674/1000 --- L(Train): 0.0072260 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward + 0.001 reward^2 \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, 99\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 675/1000 --- L(Train): 0.0072248 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 676/1000 --- L(Train): 0.0072250 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 677/1000 --- L(Train): 0.0072247 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 678/1000 --- L(Train): 0.0072240 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 679/1000 --- L(Train): 0.0072230 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 680/1000 --- L(Train): 0.0072221 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 681/1000 --- L(Train): 0.0072216 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 682/1000 --- L(Train): 0.0072211 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 683/1000 --- L(Train): 0.0072205 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 684/1000 --- L(Train): 0.0072197 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 685/1000 --- L(Train): 0.0072188 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 686/1000 --- L(Train): 0.0072179 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 687/1000 --- L(Train): 0.0072181 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 688/1000 --- L(Train): 0.0072168 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 689/1000 --- L(Train): 0.0072163 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 690/1000 --- L(Train): 0.0072161 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 691/1000 --- L(Train): 0.0072149 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 692/1000 --- L(Train): 0.0072144 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 693/1000 --- L(Train): 0.0072145 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 694/1000 --- L(Train): 0.0072141 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 695/1000 --- L(Train): 0.0072132 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 696/1000 --- L(Train): 0.0072125 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 697/1000 --- L(Train): 0.0072123 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 698/1000 --- L(Train): 0.0072117 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 699/1000 --- L(Train): 0.0072113 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 700/1000 --- L(Train): 0.0072117 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 701/1000 --- L(Train): 0.0072112 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 702/1000 --- L(Train): 0.0072101 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.92 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 703/1000 --- L(Train): 0.0072095 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 704/1000 --- L(Train): 0.0072094 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.383 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 705/1000 --- L(Train): 0.0072087 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 706/1000 --- L(Train): 0.0072078 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 707/1000 --- L(Train): 0.0072068 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 708/1000 --- L(Train): 0.0072072 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.899 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 709/1000 --- L(Train): 0.0072058 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 710/1000 --- L(Train): 0.0072056 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 711/1000 --- L(Train): 0.0072038 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 712/1000 --- L(Train): 0.0072034 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 713/1000 --- L(Train): 0.0072034 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 714/1000 --- L(Train): 0.0072024 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 715/1000 --- L(Train): 0.0072018 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 716/1000 --- L(Train): 0.0072017 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 717/1000 --- L(Train): 0.0072012 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 718/1000 --- L(Train): 0.0072005 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 719/1000 --- L(Train): 0.0072015 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 720/1000 --- L(Train): 0.0072004 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 721/1000 --- L(Train): 0.0071998 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 722/1000 --- L(Train): 0.0072009 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 723/1000 --- L(Train): 0.0071999 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 724/1000 --- L(Train): 0.0071995 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 725/1000 --- L(Train): 0.0071989 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 726/1000 --- L(Train): 0.0071982 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 727/1000 --- L(Train): 0.0071975 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 728/1000 --- L(Train): 0.0071970 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 729/1000 --- L(Train): 0.0071966 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 730/1000 --- L(Train): 0.0071963 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 731/1000 --- L(Train): 0.0071957 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 732/1000 --- L(Train): 0.0071949 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 733/1000 --- L(Train): 0.0071941 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 734/1000 --- L(Train): 0.0071937 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 735/1000 --- L(Train): 0.0071935 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 736/1000 --- L(Train): 0.0071930 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 737/1000 --- L(Train): 0.0071925 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 738/1000 --- L(Train): 0.0071919 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 739/1000 --- L(Train): 0.0071915 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 740/1000 --- L(Train): 0.0071910 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 741/1000 --- L(Train): 0.0071902 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 742/1000 --- L(Train): 0.0071897 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 743/1000 --- L(Train): 0.0071889 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 744/1000 --- L(Train): 0.0071884 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 745/1000 --- L(Train): 0.0071882 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 746/1000 --- L(Train): 0.0071873 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 747/1000 --- L(Train): 0.0071864 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 748/1000 --- L(Train): 0.0071864 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 749/1000 --- L(Train): 0.0071863 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 750/1000 --- L(Train): 0.0071858 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 751/1000 --- L(Train): 0.0071852 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 752/1000 --- L(Train): 0.0071852 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 753/1000 --- L(Train): 0.0071852 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 754/1000 --- L(Train): 0.0071851 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 755/1000 --- L(Train): 0.0071848 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 756/1000 --- L(Train): 0.0071840 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 757/1000 --- L(Train): 0.0071837 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 758/1000 --- L(Train): 0.0071834 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 759/1000 --- L(Train): 0.0071830 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 760/1000 --- L(Train): 0.0071830 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 761/1000 --- L(Train): 0.0071823 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 762/1000 --- L(Train): 0.0071819 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 763/1000 --- L(Train): 0.0071819 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 764/1000 --- L(Train): 0.0071812 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 765/1000 --- L(Train): 0.0071803 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 766/1000 --- L(Train): 0.0071799 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 767/1000 --- L(Train): 0.0071796 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 768/1000 --- L(Train): 0.0071788 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 769/1000 --- L(Train): 0.0071782 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 770/1000 --- L(Train): 0.0071777 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 771/1000 --- L(Train): 0.0071771 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 772/1000 --- L(Train): 0.0071768 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 773/1000 --- L(Train): 0.0071769 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 774/1000 --- L(Train): 0.0071765 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 775/1000 --- L(Train): 0.0071764 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 776/1000 --- L(Train): 0.0071764 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 777/1000 --- L(Train): 0.0071765 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 778/1000 --- L(Train): 0.0071765 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 779/1000 --- L(Train): 0.0071757 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 780/1000 --- L(Train): 0.0071754 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 781/1000 --- L(Train): 0.0071753 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 782/1000 --- L(Train): 0.0071750 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 783/1000 --- L(Train): 0.0071744 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 784/1000 --- L(Train): 0.0071745 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 785/1000 --- L(Train): 0.0071744 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 786/1000 --- L(Train): 0.0071737 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 787/1000 --- L(Train): 0.0071734 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 788/1000 --- L(Train): 0.0071734 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 789/1000 --- L(Train): 0.0071734 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 790/1000 --- L(Train): 0.0071730 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 791/1000 --- L(Train): 0.0071720 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 792/1000 --- L(Train): 0.0071716 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 793/1000 --- L(Train): 0.0071714 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 794/1000 --- L(Train): 0.0071709 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 795/1000 --- L(Train): 0.0071701 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 796/1000 --- L(Train): 0.0071699 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 797/1000 --- L(Train): 0.0071699 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 798/1000 --- L(Train): 0.0071698 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 799/1000 --- L(Train): 0.0071692 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 800/1000 --- L(Train): 0.0071687 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 801/1000 --- L(Train): 0.0071687 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 802/1000 --- L(Train): 0.0071685 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 803/1000 --- L(Train): 0.0071680 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 804/1000 --- L(Train): 0.0071677 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 805/1000 --- L(Train): 0.0071675 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 806/1000 --- L(Train): 0.0071672 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 807/1000 --- L(Train): 0.0071672 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 808/1000 --- L(Train): 0.0071673 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 809/1000 --- L(Train): 0.0071671 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 810/1000 --- L(Train): 0.0071678 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 811/1000 --- L(Train): 0.0071667 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 812/1000 --- L(Train): 0.0071667 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 813/1000 --- L(Train): 0.0071667 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 814/1000 --- L(Train): 0.0071663 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 815/1000 --- L(Train): 0.0071659 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 816/1000 --- L(Train): 0.0071659 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 817/1000 --- L(Train): 0.0071659 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 818/1000 --- L(Train): 0.0071655 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 819/1000 --- L(Train): 0.0071650 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 820/1000 --- L(Train): 0.0071648 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 821/1000 --- L(Train): 0.0071643 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 822/1000 --- L(Train): 0.0071639 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 823/1000 --- L(Train): 0.0071636 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 824/1000 --- L(Train): 0.0071633 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 825/1000 --- L(Train): 0.0071632 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 826/1000 --- L(Train): 0.0071631 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 827/1000 --- L(Train): 0.0071626 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 828/1000 --- L(Train): 0.0071618 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 829/1000 --- L(Train): 0.0071619 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 830/1000 --- L(Train): 0.0071616 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 831/1000 --- L(Train): 0.0071606 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 832/1000 --- L(Train): 0.0071609 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 833/1000 --- L(Train): 0.0071614 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 834/1000 --- L(Train): 0.0071613 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 835/1000 --- L(Train): 0.0071609 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 836/1000 --- L(Train): 0.0071607 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 837/1000 --- L(Train): 0.0071607 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 838/1000 --- L(Train): 0.0071610 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 839/1000 --- L(Train): 0.0071610 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 840/1000 --- L(Train): 0.0071606 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 841/1000 --- L(Train): 0.0071608 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 842/1000 --- L(Train): 0.0071605 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 843/1000 --- L(Train): 0.0071603 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 844/1000 --- L(Train): 0.0071607 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 845/1000 --- L(Train): 0.0071603 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 846/1000 --- L(Train): 0.0071623 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 847/1000 --- L(Train): 0.0071601 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 848/1000 --- L(Train): 0.0071600 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 849/1000 --- L(Train): 0.0071610 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 850/1000 --- L(Train): 0.0071604 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 851/1000 --- L(Train): 0.0071585 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 852/1000 --- L(Train): 0.0071583 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 853/1000 --- L(Train): 0.0071590 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 854/1000 --- L(Train): 0.0071586 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 855/1000 --- L(Train): 0.0071572 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 856/1000 --- L(Train): 0.0071569 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 857/1000 --- L(Train): 0.0071575 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 858/1000 --- L(Train): 0.0071575 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 859/1000 --- L(Train): 0.0071571 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 860/1000 --- L(Train): 0.0071567 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 861/1000 --- L(Train): 0.0071573 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 862/1000 --- L(Train): 0.0071577 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 863/1000 --- L(Train): 0.0071574 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 864/1000 --- L(Train): 0.0071570 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 865/1000 --- L(Train): 0.0071568 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 866/1000 --- L(Train): 0.0071568 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 867/1000 --- L(Train): 0.0071565 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 868/1000 --- L(Train): 0.0071565 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 869/1000 --- L(Train): 0.0071563 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 870/1000 --- L(Train): 0.0071561 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 871/1000 --- L(Train): 0.0071559 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 872/1000 --- L(Train): 0.0071560 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 873/1000 --- L(Train): 0.0071562 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 874/1000 --- L(Train): 0.0071561 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 875/1000 --- L(Train): 0.0071560 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 876/1000 --- L(Train): 0.0071555 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 877/1000 --- L(Train): 0.0071552 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 878/1000 --- L(Train): 0.0071551 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 879/1000 --- L(Train): 0.0071549 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 880/1000 --- L(Train): 0.0071548 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 881/1000 --- L(Train): 0.0071545 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 882/1000 --- L(Train): 0.0071541 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 883/1000 --- L(Train): 0.0071541 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 884/1000 --- L(Train): 0.0071540 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 885/1000 --- L(Train): 0.0071535 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 886/1000 --- L(Train): 0.0071532 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 887/1000 --- L(Train): 0.0071532 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 888/1000 --- L(Train): 0.0071530 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 889/1000 --- L(Train): 0.0071527 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 890/1000 --- L(Train): 0.0071526 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 891/1000 --- L(Train): 0.0071522 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 892/1000 --- L(Train): 0.0071526 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 893/1000 --- L(Train): 0.0071526 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 894/1000 --- L(Train): 0.0071523 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 895/1000 --- L(Train): 0.0071523 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 896/1000 --- L(Train): 0.0071525 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 897/1000 --- L(Train): 0.0071527 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 898/1000 --- L(Train): 0.0071528 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 899/1000 --- L(Train): 0.0071527 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 900/1000 --- L(Train): 0.0071520 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 901/1000 --- L(Train): 0.0071520 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 902/1000 --- L(Train): 0.0071523 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 903/1000 --- L(Train): 0.0071522 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 904/1000 --- L(Train): 0.0071521 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 905/1000 --- L(Train): 0.0071517 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 906/1000 --- L(Train): 0.0071517 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 907/1000 --- L(Train): 0.0071516 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 908/1000 --- L(Train): 0.0071512 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 909/1000 --- L(Train): 0.0071511 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 910/1000 --- L(Train): 0.0071512 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 911/1000 --- L(Train): 0.0071508 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 912/1000 --- L(Train): 0.0071502 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 913/1000 --- L(Train): 0.0071507 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 914/1000 --- L(Train): 0.0071503 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 915/1000 --- L(Train): 0.0071498 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 916/1000 --- L(Train): 0.0071501 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 917/1000 --- L(Train): 0.0071506 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 918/1000 --- L(Train): 0.0071503 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 919/1000 --- L(Train): 0.0071501 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 920/1000 --- L(Train): 0.0071502 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 921/1000 --- L(Train): 0.0071498 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 922/1000 --- L(Train): 0.0071497 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 923/1000 --- L(Train): 0.0071493 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 924/1000 --- L(Train): 0.0071494 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 925/1000 --- L(Train): 0.0071493 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 926/1000 --- L(Train): 0.0071485 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 927/1000 --- L(Train): 0.0071487 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 928/1000 --- L(Train): 0.0071491 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 929/1000 --- L(Train): 0.0071490 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 930/1000 --- L(Train): 0.0071488 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 931/1000 --- L(Train): 0.0071486 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 932/1000 --- L(Train): 0.0071485 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 933/1000 --- L(Train): 0.0071486 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 934/1000 --- L(Train): 0.0071487 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 935/1000 --- L(Train): 0.0071484 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 936/1000 --- L(Train): 0.0071479 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 937/1000 --- L(Train): 0.0071479 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 938/1000 --- L(Train): 0.0071480 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 939/1000 --- L(Train): 0.0071479 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 940/1000 --- L(Train): 0.0071477 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 941/1000 --- L(Train): 0.0071473 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 942/1000 --- L(Train): 0.0071473 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 943/1000 --- L(Train): 0.0071476 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 944/1000 --- L(Train): 0.0071476 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 945/1000 --- L(Train): 0.0071475 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 946/1000 --- L(Train): 0.0071475 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 947/1000 --- L(Train): 0.0071472 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 948/1000 --- L(Train): 0.0071470 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 949/1000 --- L(Train): 0.0071466 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 950/1000 --- L(Train): 0.0071465 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 951/1000 --- L(Train): 0.0071463 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 952/1000 --- L(Train): 0.0071465 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 953/1000 --- L(Train): 0.0071469 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 954/1000 --- L(Train): 0.0071464 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 955/1000 --- L(Train): 0.0071467 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 956/1000 --- L(Train): 0.0071467 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 957/1000 --- L(Train): 0.0071468 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 958/1000 --- L(Train): 0.0071468 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 959/1000 --- L(Train): 0.0071464 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 960/1000 --- L(Train): 0.0071461 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 961/1000 --- L(Train): 0.0071460 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 962/1000 --- L(Train): 0.0071458 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 963/1000 --- L(Train): 0.0071452 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 964/1000 --- L(Train): 0.0071455 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 965/1000 --- L(Train): 0.0071455 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 966/1000 --- L(Train): 0.0071450 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 967/1000 --- L(Train): 0.0071445 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 968/1000 --- L(Train): 0.0071443 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 969/1000 --- L(Train): 0.0071450 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 970/1000 --- L(Train): 0.0071450 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 971/1000 --- L(Train): 0.0071442 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 972/1000 --- L(Train): 0.0071437 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 973/1000 --- L(Train): 0.0071445 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 974/1000 --- L(Train): 0.0071445 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 975/1000 --- L(Train): 0.0071438 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 976/1000 --- L(Train): 0.0071438 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 977/1000 --- L(Train): 0.0071440 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 978/1000 --- L(Train): 0.0071445 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 979/1000 --- L(Train): 0.0071442 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 980/1000 --- L(Train): 0.0071440 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 981/1000 --- L(Train): 0.0071437 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 982/1000 --- L(Train): 0.0071438 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 983/1000 --- L(Train): 0.0071441 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 984/1000 --- L(Train): 0.0071439 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 985/1000 --- L(Train): 0.0071437 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 986/1000 --- L(Train): 0.0071436 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 987/1000 --- L(Train): 0.0071436 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 988/1000 --- L(Train): 0.0071436 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 989/1000 --- L(Train): 0.0071432 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 990/1000 --- L(Train): 0.0071427 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 991/1000 --- L(Train): 0.0071428 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 992/1000 --- L(Train): 0.0071428 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 993/1000 --- L(Train): 0.0071425 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 994/1000 --- L(Train): 0.0071427 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 995/1000 --- L(Train): 0.0071429 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 996/1000 --- L(Train): 0.0071425 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 997/1000 --- L(Train): 0.0071422 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 998/1000 --- L(Train): 0.0071419 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 999/1000 --- L(Train): 0.0071419 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 1000/1000 --- L(Train): 0.0071420 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 1001/1000 --- L(Train): 0.0071419 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 6):\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_chosen: -, 0, 0, -, 0, -\n",
      "value_reward_not_chosen: 0, 0, -\n",
      "value_reward_not_displayed: -, -, -\n",
      "================================================================================\n",
      "\n",
      "Training result:\n",
      "L(Train): 0.0000000 --- L(Val, RNN): 0.3487987 --- L(Val, SINDy): 0.3490609 --- LR: 1.0000000e-02\n",
      "\n",
      "RNN training finished.\n",
      "Training took 1420.50 seconds.\n",
      "Saving SPICE model to ../params/augustat2025/spice_augustat2025.pkl...\n",
      "================================================================================\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Example SPICE model (participant 0):\n",
      "--------------------------------------------------------------------------------\n",
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting training on {estimator.device}...\")\n",
    "print(\"=\" * 80)\n",
    "estimator.fit(dataset_train.xs, dataset_train.ys, dataset_test.xs, dataset_test.ys)\n",
    "# estimator.load_spice(args.model)\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Print example SPICE model for first participant\n",
    "print(\"\\nExample SPICE model (participant 0):\")\n",
    "print(\"-\" * 80)\n",
    "estimator.print_spice_model(participant_id=0)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a trained spice model; only necessary if not trained in current notebook session\n",
    "estimator.load_spice(path_spice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU for benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a general RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "from weinhardt2025.benchmarking.benchmarking_gru import training, setup_agent_gru\n",
    "\n",
    "path_gru = '../../weinhardt2025/params/augustat2025/gru_augustat2025.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU specifications:\n",
    "# 1. In order to function seamlessly with the rest of the framework the GRU has to output (logits, hidden_state)\n",
    "# 2. Because of the task type (more items to remember than displayed for action; n_items > n_actions):\n",
    "#   2.1 The GRU computes the action values for all items at each timestep (chosen, not chosen but displayed, and not displayed)\n",
    "#   2.2 To facilitate selection only among the shown options, the GRU has to map the values from the item space (all values) to the action space (only displayed ones)\n",
    "\n",
    "class GRU(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, n_items, additional_inputs: int = 0, hidden_size: int = 32, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gru_features = hidden_size\n",
    "        self.n_items = n_items\n",
    "        self.n_actions = n_actions\n",
    "        self.additional_inputs = additional_inputs\n",
    "\n",
    "        self.linear_in = torch.nn.Linear(in_features=n_actions+1+additional_inputs, out_features=hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.gru = torch.nn.GRU(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear_out = torch.nn.Linear(in_features=hidden_size, out_features=n_items)\n",
    "        \n",
    "    def forward(self, inputs, state=None):\n",
    "        \n",
    "        actions = inputs[..., :self.n_actions]\n",
    "        rewards = inputs[..., self.n_actions:2*self.n_actions].nan_to_num(0).sum(dim=-1, keepdims=True)\n",
    "        item_pairs = inputs[..., self.n_actions*2+2:self.n_actions*2+2+self.additional_inputs]  # +2 -> skip shown_at_i; keep only shown_at_i_next\n",
    "        inputs = torch.concat((actions, rewards, item_pairs), dim=-1)\n",
    "        \n",
    "        # Get item pairs\n",
    "        # item_pairs = inputs[..., 3*self.n_actions:3*self.n_actions+2]\n",
    "        \n",
    "        if state is not None and len(inputs.shape) == 3:\n",
    "            state = state.reshape(1, 1, self.gru_features)\n",
    "        \n",
    "        y = self.linear_in(inputs.nan_to_num(0))\n",
    "        y = self.dropout(y)\n",
    "        y, state = self.gru(y, state)\n",
    "        y = self.dropout(y)\n",
    "        y = self.linear_out(y)\n",
    "        \n",
    "        # map values from item space into action space to determine the next action based on the shown options\n",
    "        item1_values = torch.gather(y, 2, item_pairs[..., 0].unsqueeze(-1).nan_to_num(0).long())\n",
    "        item2_values = torch.gather(y, 2, item_pairs[..., 1].unsqueeze(-1).nan_to_num(0).long())\n",
    "        y = torch.cat([item1_values, item2_values], dim=-1)\n",
    "        \n",
    "        return y, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "gru = GRU(n_actions=n_actions, n_items=n_items, additional_inputs=2).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: L(Train): 0.7699153423309326; L(Test): 0.5198200345039368\n",
      "Epoch 2/1000: L(Train): 0.5324288606643677; L(Test): 0.4583016335964203\n",
      "Epoch 3/1000: L(Train): 0.4571947753429413; L(Test): 0.45632404088974\n",
      "Epoch 4/1000: L(Train): 0.47100141644477844; L(Test): 0.46150436997413635\n",
      "Epoch 5/1000: L(Train): 0.46861299872398376; L(Test): 0.4664887487888336\n",
      "Epoch 6/1000: L(Train): 0.45548897981643677; L(Test): 0.4665026366710663\n",
      "Epoch 7/1000: L(Train): 0.4839845597743988; L(Test): 0.4592019021511078\n",
      "Epoch 8/1000: L(Train): 0.458694726228714; L(Test): 0.4502069056034088\n",
      "Epoch 9/1000: L(Train): 0.44866496324539185; L(Test): 0.4445895850658417\n",
      "Epoch 10/1000: L(Train): 0.4370618164539337; L(Test): 0.44436201453208923\n",
      "Epoch 11/1000: L(Train): 0.4357244074344635; L(Test): 0.44738277792930603\n",
      "Epoch 12/1000: L(Train): 0.42047029733657837; L(Test): 0.4485151767730713\n",
      "Epoch 13/1000: L(Train): 0.4540117084980011; L(Test): 0.44732972979545593\n",
      "Epoch 14/1000: L(Train): 0.4637111723423004; L(Test): 0.4449225962162018\n",
      "Epoch 15/1000: L(Train): 0.4505753815174103; L(Test): 0.4427754878997803\n",
      "Epoch 16/1000: L(Train): 0.45556023716926575; L(Test): 0.4419431686401367\n",
      "Epoch 17/1000: L(Train): 0.4336847960948944; L(Test): 0.4425024092197418\n",
      "Epoch 18/1000: L(Train): 0.4297485053539276; L(Test): 0.4437781870365143\n",
      "Epoch 19/1000: L(Train): 0.42368900775909424; L(Test): 0.4448769688606262\n",
      "Epoch 20/1000: L(Train): 0.41920456290245056; L(Test): 0.44564884901046753\n",
      "Epoch 21/1000: L(Train): 0.45878538489341736; L(Test): 0.44512882828712463\n",
      "Epoch 22/1000: L(Train): 0.4645770788192749; L(Test): 0.4434972405433655\n",
      "Epoch 23/1000: L(Train): 0.45954638719558716; L(Test): 0.4417587220668793\n",
      "Epoch 24/1000: L(Train): 0.43253058195114136; L(Test): 0.44103655219078064\n",
      "Epoch 25/1000: L(Train): 0.43636611104011536; L(Test): 0.4413364827632904\n",
      "Epoch 26/1000: L(Train): 0.4427086412906647; L(Test): 0.4418957829475403\n",
      "Epoch 27/1000: L(Train): 0.4521491527557373; L(Test): 0.44229233264923096\n",
      "Epoch 28/1000: L(Train): 0.4546860158443451; L(Test): 0.4422842264175415\n",
      "Epoch 29/1000: L(Train): 0.45102745294570923; L(Test): 0.4418213963508606\n",
      "Epoch 30/1000: L(Train): 0.42877063155174255; L(Test): 0.4407849609851837\n",
      "Epoch 31/1000: L(Train): 0.44435352087020874; L(Test): 0.4402114152908325\n",
      "Epoch 32/1000: L(Train): 0.44229811429977417; L(Test): 0.44021522998809814\n",
      "Epoch 33/1000: L(Train): 0.4443688690662384; L(Test): 0.44059500098228455\n",
      "Epoch 34/1000: L(Train): 0.42982879281044006; L(Test): 0.44099608063697815\n",
      "Epoch 35/1000: L(Train): 0.43758225440979004; L(Test): 0.4413549304008484\n",
      "Epoch 36/1000: L(Train): 0.46142303943634033; L(Test): 0.44090571999549866\n",
      "Epoch 37/1000: L(Train): 0.4398321211338043; L(Test): 0.44012537598609924\n",
      "Epoch 38/1000: L(Train): 0.4525732398033142; L(Test): 0.4392811059951782\n",
      "Epoch 39/1000: L(Train): 0.443204790353775; L(Test): 0.43892329931259155\n",
      "Epoch 40/1000: L(Train): 0.4349750578403473; L(Test): 0.4390266239643097\n",
      "Epoch 41/1000: L(Train): 0.44971606135368347; L(Test): 0.4393208622932434\n",
      "Epoch 42/1000: L(Train): 0.43767234683036804; L(Test): 0.43937021493911743\n",
      "Epoch 43/1000: L(Train): 0.455072283744812; L(Test): 0.43927597999572754\n",
      "Epoch 44/1000: L(Train): 0.4382754862308502; L(Test): 0.43895670771598816\n",
      "Epoch 45/1000: L(Train): 0.45025986433029175; L(Test): 0.4384717643260956\n",
      "Epoch 46/1000: L(Train): 0.4542848765850067; L(Test): 0.4379819333553314\n",
      "Epoch 47/1000: L(Train): 0.43546703457832336; L(Test): 0.4376254677772522\n",
      "Epoch 48/1000: L(Train): 0.4377857744693756; L(Test): 0.4375114142894745\n",
      "Epoch 49/1000: L(Train): 0.4377608597278595; L(Test): 0.43746817111968994\n",
      "Epoch 50/1000: L(Train): 0.45118314027786255; L(Test): 0.4373616576194763\n",
      "Epoch 51/1000: L(Train): 0.44338640570640564; L(Test): 0.43724820017814636\n",
      "Epoch 52/1000: L(Train): 0.4607166647911072; L(Test): 0.437104195356369\n",
      "Epoch 53/1000: L(Train): 0.45460647344589233; L(Test): 0.4371148347854614\n",
      "Epoch 54/1000: L(Train): 0.4330359399318695; L(Test): 0.43670475482940674\n",
      "Epoch 55/1000: L(Train): 0.44539108872413635; L(Test): 0.4360448122024536\n",
      "Epoch 56/1000: L(Train): 0.4248845875263214; L(Test): 0.4354632794857025\n",
      "Epoch 57/1000: L(Train): 0.46107399463653564; L(Test): 0.4355401396751404\n",
      "Epoch 58/1000: L(Train): 0.43538883328437805; L(Test): 0.4355214834213257\n",
      "Epoch 59/1000: L(Train): 0.4373663663864136; L(Test): 0.43516141176223755\n",
      "Epoch 60/1000: L(Train): 0.4403907358646393; L(Test): 0.4344410300254822\n",
      "Epoch 61/1000: L(Train): 0.4175519049167633; L(Test): 0.43391555547714233\n",
      "Epoch 62/1000: L(Train): 0.4264306128025055; L(Test): 0.43359658122062683\n",
      "Epoch 63/1000: L(Train): 0.4369172751903534; L(Test): 0.4333881735801697\n",
      "Epoch 64/1000: L(Train): 0.43504151701927185; L(Test): 0.4330788850784302\n",
      "Epoch 65/1000: L(Train): 0.4226495027542114; L(Test): 0.4327385127544403\n",
      "Epoch 66/1000: L(Train): 0.41958609223365784; L(Test): 0.4325597584247589\n",
      "Epoch 67/1000: L(Train): 0.4245745539665222; L(Test): 0.4322805106639862\n",
      "Epoch 68/1000: L(Train): 0.43167248368263245; L(Test): 0.4311911165714264\n",
      "Epoch 69/1000: L(Train): 0.434360146522522; L(Test): 0.430212140083313\n",
      "Epoch 70/1000: L(Train): 0.4381237328052521; L(Test): 0.4304804503917694\n",
      "Epoch 71/1000: L(Train): 0.4317467510700226; L(Test): 0.4298873841762543\n",
      "Epoch 72/1000: L(Train): 0.4368704557418823; L(Test): 0.4288449287414551\n",
      "Epoch 73/1000: L(Train): 0.43680647015571594; L(Test): 0.4284532964229584\n",
      "Epoch 74/1000: L(Train): 0.4440971314907074; L(Test): 0.4274948537349701\n",
      "Epoch 75/1000: L(Train): 0.4282146990299225; L(Test): 0.42656210064888\n",
      "Epoch 76/1000: L(Train): 0.434195339679718; L(Test): 0.4259510934352875\n",
      "Epoch 77/1000: L(Train): 0.42922356724739075; L(Test): 0.4250853955745697\n",
      "Epoch 78/1000: L(Train): 0.44645455479621887; L(Test): 0.42455345392227173\n",
      "Epoch 79/1000: L(Train): 0.42829430103302; L(Test): 0.4236387014389038\n",
      "Epoch 80/1000: L(Train): 0.42491012811660767; L(Test): 0.4231657385826111\n",
      "Epoch 81/1000: L(Train): 0.4235062897205353; L(Test): 0.42164894938468933\n",
      "Epoch 82/1000: L(Train): 0.4253298044204712; L(Test): 0.4209284782409668\n",
      "Epoch 83/1000: L(Train): 0.42508435249328613; L(Test): 0.41963955760002136\n",
      "Epoch 84/1000: L(Train): 0.42568114399909973; L(Test): 0.41912615299224854\n",
      "Epoch 85/1000: L(Train): 0.4053274989128113; L(Test): 0.4190443754196167\n",
      "Epoch 86/1000: L(Train): 0.40686818957328796; L(Test): 0.4171193540096283\n",
      "Epoch 87/1000: L(Train): 0.41707682609558105; L(Test): 0.41645240783691406\n",
      "Epoch 88/1000: L(Train): 0.4171711504459381; L(Test): 0.4161626696586609\n",
      "Epoch 89/1000: L(Train): 0.40473687648773193; L(Test): 0.4160137474536896\n",
      "Epoch 90/1000: L(Train): 0.3956615924835205; L(Test): 0.4167749285697937\n",
      "Epoch 91/1000: L(Train): 0.4339120388031006; L(Test): 0.42522427439689636\n",
      "Epoch 92/1000: L(Train): 0.44606447219848633; L(Test): 0.4194440543651581\n",
      "Epoch 93/1000: L(Train): 0.42832088470458984; L(Test): 0.41948243975639343\n",
      "Epoch 94/1000: L(Train): 0.432677686214447; L(Test): 0.41472455859184265\n",
      "Epoch 95/1000: L(Train): 0.40587177872657776; L(Test): 0.4140448272228241\n",
      "Epoch 96/1000: L(Train): 0.4130439758300781; L(Test): 0.41317448019981384\n",
      "Epoch 97/1000: L(Train): 0.43746235966682434; L(Test): 0.4131442904472351\n",
      "Epoch 98/1000: L(Train): 0.42670106887817383; L(Test): 0.41076627373695374\n",
      "Epoch 99/1000: L(Train): 0.4083508551120758; L(Test): 0.41112083196640015\n",
      "Epoch 100/1000: L(Train): 0.41898852586746216; L(Test): 0.4099091589450836\n",
      "Epoch 101/1000: L(Train): 0.419485479593277; L(Test): 0.41023874282836914\n",
      "Epoch 102/1000: L(Train): 0.41897961497306824; L(Test): 0.40951359272003174\n",
      "Epoch 103/1000: L(Train): 0.40526464581489563; L(Test): 0.40818122029304504\n",
      "Epoch 104/1000: L(Train): 0.4141380488872528; L(Test): 0.4070444107055664\n",
      "Epoch 105/1000: L(Train): 0.4185623228549957; L(Test): 0.40665125846862793\n",
      "Epoch 106/1000: L(Train): 0.39790835976600647; L(Test): 0.4058913588523865\n",
      "Epoch 107/1000: L(Train): 0.41214635968208313; L(Test): 0.40667009353637695\n",
      "Epoch 108/1000: L(Train): 0.3988351821899414; L(Test): 0.4053477346897125\n",
      "Epoch 109/1000: L(Train): 0.40509578585624695; L(Test): 0.4027945101261139\n",
      "Epoch 110/1000: L(Train): 0.39245307445526123; L(Test): 0.40211161971092224\n",
      "Epoch 111/1000: L(Train): 0.4193437993526459; L(Test): 0.401015043258667\n",
      "Epoch 112/1000: L(Train): 0.40473729372024536; L(Test): 0.40097612142562866\n",
      "Epoch 113/1000: L(Train): 0.41038617491722107; L(Test): 0.39964866638183594\n",
      "Epoch 114/1000: L(Train): 0.40638476610183716; L(Test): 0.3987080752849579\n",
      "Epoch 115/1000: L(Train): 0.3824378550052643; L(Test): 0.399259090423584\n",
      "Epoch 116/1000: L(Train): 0.3873177170753479; L(Test): 0.39893609285354614\n",
      "Epoch 117/1000: L(Train): 0.4217272400856018; L(Test): 0.39721331000328064\n",
      "Epoch 118/1000: L(Train): 0.40994948148727417; L(Test): 0.3951188623905182\n",
      "Epoch 119/1000: L(Train): 0.39422279596328735; L(Test): 0.3976166844367981\n",
      "Epoch 120/1000: L(Train): 0.4090789258480072; L(Test): 0.3938536047935486\n",
      "Epoch 121/1000: L(Train): 0.39028072357177734; L(Test): 0.3935965299606323\n",
      "Epoch 122/1000: L(Train): 0.40893033146858215; L(Test): 0.39265552163124084\n",
      "Epoch 123/1000: L(Train): 0.40124136209487915; L(Test): 0.3930593430995941\n",
      "Epoch 124/1000: L(Train): 0.40660759806632996; L(Test): 0.39115357398986816\n",
      "Epoch 125/1000: L(Train): 0.394061803817749; L(Test): 0.3897538185119629\n",
      "Epoch 126/1000: L(Train): 0.37794071435928345; L(Test): 0.38931146264076233\n",
      "Epoch 127/1000: L(Train): 0.373717725276947; L(Test): 0.39034172892570496\n",
      "Epoch 128/1000: L(Train): 0.37586814165115356; L(Test): 0.3888495862483978\n",
      "Epoch 129/1000: L(Train): 0.40170207619667053; L(Test): 0.3868778944015503\n",
      "Epoch 130/1000: L(Train): 0.40040475130081177; L(Test): 0.3860307037830353\n",
      "Epoch 131/1000: L(Train): 0.38110437989234924; L(Test): 0.3851911723613739\n",
      "Epoch 132/1000: L(Train): 0.392494261264801; L(Test): 0.38486945629119873\n",
      "Epoch 133/1000: L(Train): 0.38664710521698; L(Test): 0.38351860642433167\n",
      "Epoch 134/1000: L(Train): 0.37375831604003906; L(Test): 0.38249871134757996\n",
      "Epoch 135/1000: L(Train): 0.3927736282348633; L(Test): 0.38106322288513184\n",
      "Epoch 136/1000: L(Train): 0.38595497608184814; L(Test): 0.379901647567749\n",
      "Epoch 137/1000: L(Train): 0.37828853726387024; L(Test): 0.3794496953487396\n",
      "Epoch 138/1000: L(Train): 0.37324175238609314; L(Test): 0.3787670135498047\n",
      "Epoch 139/1000: L(Train): 0.39430612325668335; L(Test): 0.37758949398994446\n",
      "Epoch 140/1000: L(Train): 0.38573193550109863; L(Test): 0.3764705955982208\n",
      "Epoch 141/1000: L(Train): 0.37713027000427246; L(Test): 0.37578195333480835\n",
      "Epoch 142/1000: L(Train): 0.3740783929824829; L(Test): 0.3752898573875427\n",
      "Epoch 143/1000: L(Train): 0.3605622947216034; L(Test): 0.3743523359298706\n",
      "Epoch 144/1000: L(Train): 0.38572269678115845; L(Test): 0.3723304867744446\n",
      "Epoch 145/1000: L(Train): 0.36315855383872986; L(Test): 0.3713208734989166\n",
      "Epoch 146/1000: L(Train): 0.36813056468963623; L(Test): 0.3707728981971741\n",
      "Epoch 147/1000: L(Train): 0.3715898096561432; L(Test): 0.37065064907073975\n",
      "Epoch 148/1000: L(Train): 0.3822808861732483; L(Test): 0.36939939856529236\n",
      "Epoch 149/1000: L(Train): 0.36131229996681213; L(Test): 0.36817410588264465\n",
      "Epoch 150/1000: L(Train): 0.36824730038642883; L(Test): 0.36726319789886475\n",
      "Epoch 151/1000: L(Train): 0.35951995849609375; L(Test): 0.36698710918426514\n",
      "Epoch 152/1000: L(Train): 0.3683020770549774; L(Test): 0.3665156364440918\n",
      "Epoch 153/1000: L(Train): 0.3547113537788391; L(Test): 0.36555615067481995\n",
      "Epoch 154/1000: L(Train): 0.3871248960494995; L(Test): 0.36391061544418335\n",
      "Epoch 155/1000: L(Train): 0.38131752610206604; L(Test): 0.3635072410106659\n",
      "Epoch 156/1000: L(Train): 0.37079599499702454; L(Test): 0.3634588122367859\n",
      "Epoch 157/1000: L(Train): 0.37158632278442383; L(Test): 0.36367762088775635\n",
      "Epoch 158/1000: L(Train): 0.38318297266960144; L(Test): 0.36127158999443054\n",
      "Epoch 159/1000: L(Train): 0.3559878468513489; L(Test): 0.36033105850219727\n",
      "Epoch 160/1000: L(Train): 0.3600774109363556; L(Test): 0.3601021468639374\n",
      "Epoch 161/1000: L(Train): 0.35581135749816895; L(Test): 0.3607335090637207\n",
      "Epoch 162/1000: L(Train): 0.36406636238098145; L(Test): 0.3584662973880768\n",
      "Epoch 163/1000: L(Train): 0.3609352111816406; L(Test): 0.3579776883125305\n",
      "Epoch 164/1000: L(Train): 0.36654266715049744; L(Test): 0.3574654459953308\n",
      "Epoch 165/1000: L(Train): 0.34964656829833984; L(Test): 0.35973745584487915\n",
      "Epoch 166/1000: L(Train): 0.37536031007766724; L(Test): 0.3569917380809784\n",
      "Epoch 167/1000: L(Train): 0.37752172350883484; L(Test): 0.3564096987247467\n",
      "Epoch 168/1000: L(Train): 0.3618582785129547; L(Test): 0.35490846633911133\n",
      "Epoch 169/1000: L(Train): 0.3546242415904999; L(Test): 0.3576647937297821\n",
      "Epoch 170/1000: L(Train): 0.370140016078949; L(Test): 0.35628801584243774\n",
      "Epoch 171/1000: L(Train): 0.3733067512512207; L(Test): 0.35461220145225525\n",
      "Epoch 172/1000: L(Train): 0.35833048820495605; L(Test): 0.3538091778755188\n",
      "Epoch 173/1000: L(Train): 0.33750689029693604; L(Test): 0.35394254326820374\n",
      "Epoch 174/1000: L(Train): 0.36992400884628296; L(Test): 0.35522228479385376\n",
      "Epoch 175/1000: L(Train): 0.36043694615364075; L(Test): 0.3528877794742584\n",
      "Epoch 176/1000: L(Train): 0.361752986907959; L(Test): 0.35224196314811707\n",
      "Epoch 177/1000: L(Train): 0.34393444657325745; L(Test): 0.3512513339519501\n",
      "Epoch 178/1000: L(Train): 0.34155532717704773; L(Test): 0.35246357321739197\n",
      "Epoch 179/1000: L(Train): 0.3493439555168152; L(Test): 0.35333070158958435\n",
      "Epoch 180/1000: L(Train): 0.35370856523513794; L(Test): 0.3508395254611969\n",
      "Epoch 181/1000: L(Train): 0.36438432335853577; L(Test): 0.35040751099586487\n",
      "Epoch 182/1000: L(Train): 0.3537029027938843; L(Test): 0.34963661432266235\n",
      "Epoch 183/1000: L(Train): 0.36131367087364197; L(Test): 0.35085880756378174\n",
      "Epoch 184/1000: L(Train): 0.36836808919906616; L(Test): 0.3495922088623047\n",
      "Epoch 185/1000: L(Train): 0.36168161034584045; L(Test): 0.348886638879776\n",
      "Epoch 186/1000: L(Train): 0.34611135721206665; L(Test): 0.3484209477901459\n",
      "Epoch 187/1000: L(Train): 0.3521817624568939; L(Test): 0.34860342741012573\n",
      "Epoch 188/1000: L(Train): 0.35114598274230957; L(Test): 0.3491154909133911\n",
      "Epoch 189/1000: L(Train): 0.3461596667766571; L(Test): 0.3482043147087097\n",
      "Epoch 190/1000: L(Train): 0.363863468170166; L(Test): 0.34707170724868774\n",
      "Epoch 191/1000: L(Train): 0.3448137640953064; L(Test): 0.34695225954055786\n",
      "Epoch 192/1000: L(Train): 0.3529341220855713; L(Test): 0.3469769060611725\n",
      "Epoch 193/1000: L(Train): 0.3477822542190552; L(Test): 0.3473936915397644\n",
      "Epoch 194/1000: L(Train): 0.3741132915019989; L(Test): 0.34648585319519043\n",
      "Epoch 195/1000: L(Train): 0.34159278869628906; L(Test): 0.34579384326934814\n",
      "Epoch 196/1000: L(Train): 0.356922447681427; L(Test): 0.34548333287239075\n",
      "Epoch 197/1000: L(Train): 0.34865009784698486; L(Test): 0.3456653654575348\n",
      "Epoch 198/1000: L(Train): 0.34603145718574524; L(Test): 0.34586086869239807\n",
      "Epoch 199/1000: L(Train): 0.34784045815467834; L(Test): 0.3452167212963104\n",
      "Epoch 200/1000: L(Train): 0.3352602422237396; L(Test): 0.34471991658210754\n",
      "Epoch 201/1000: L(Train): 0.3495069146156311; L(Test): 0.3444039523601532\n",
      "Epoch 202/1000: L(Train): 0.3341214954853058; L(Test): 0.34446242451667786\n",
      "Epoch 203/1000: L(Train): 0.35392382740974426; L(Test): 0.3444765508174896\n",
      "Epoch 204/1000: L(Train): 0.3500635623931885; L(Test): 0.344147652387619\n",
      "Epoch 205/1000: L(Train): 0.35149136185646057; L(Test): 0.3436800241470337\n",
      "Epoch 206/1000: L(Train): 0.38160598278045654; L(Test): 0.3431234657764435\n",
      "Epoch 207/1000: L(Train): 0.3322775065898895; L(Test): 0.3439622223377228\n",
      "Epoch 208/1000: L(Train): 0.3543153703212738; L(Test): 0.3444066643714905\n",
      "Epoch 209/1000: L(Train): 0.3516518175601959; L(Test): 0.342684268951416\n",
      "Epoch 210/1000: L(Train): 0.33473706245422363; L(Test): 0.34250929951667786\n",
      "Epoch 211/1000: L(Train): 0.3420237600803375; L(Test): 0.3422009348869324\n",
      "Epoch 212/1000: L(Train): 0.3331644535064697; L(Test): 0.3428904414176941\n",
      "Epoch 213/1000: L(Train): 0.3436712324619293; L(Test): 0.3433246612548828\n",
      "Epoch 214/1000: L(Train): 0.3468489646911621; L(Test): 0.3422055244445801\n",
      "Epoch 215/1000: L(Train): 0.3442626893520355; L(Test): 0.3411707580089569\n",
      "Epoch 216/1000: L(Train): 0.3579407334327698; L(Test): 0.340914249420166\n",
      "Epoch 217/1000: L(Train): 0.3558303713798523; L(Test): 0.34126362204551697\n",
      "Epoch 218/1000: L(Train): 0.35132521390914917; L(Test): 0.3409746289253235\n",
      "Epoch 219/1000: L(Train): 0.34057533740997314; L(Test): 0.3406461775302887\n",
      "Epoch 220/1000: L(Train): 0.32265371084213257; L(Test): 0.34030792117118835\n",
      "Epoch 221/1000: L(Train): 0.35134896636009216; L(Test): 0.33980000019073486\n",
      "Epoch 222/1000: L(Train): 0.3401067554950714; L(Test): 0.33996957540512085\n",
      "Epoch 223/1000: L(Train): 0.3281768262386322; L(Test): 0.3414132297039032\n",
      "Epoch 224/1000: L(Train): 0.36008134484291077; L(Test): 0.3394538462162018\n",
      "Epoch 225/1000: L(Train): 0.3624134361743927; L(Test): 0.3385079503059387\n",
      "Epoch 226/1000: L(Train): 0.35156089067459106; L(Test): 0.33898186683654785\n",
      "Epoch 227/1000: L(Train): 0.3579252362251282; L(Test): 0.3398008942604065\n",
      "Epoch 228/1000: L(Train): 0.3559798002243042; L(Test): 0.3381597697734833\n",
      "Epoch 229/1000: L(Train): 0.344662606716156; L(Test): 0.33766067028045654\n",
      "Epoch 230/1000: L(Train): 0.3356417715549469; L(Test): 0.33745479583740234\n",
      "Epoch 231/1000: L(Train): 0.3408518135547638; L(Test): 0.33798614144325256\n",
      "Epoch 232/1000: L(Train): 0.35530877113342285; L(Test): 0.3370550274848938\n",
      "Epoch 233/1000: L(Train): 0.3421841263771057; L(Test): 0.33658215403556824\n",
      "Epoch 234/1000: L(Train): 0.34384021162986755; L(Test): 0.3366784453392029\n",
      "Epoch 235/1000: L(Train): 0.3298051953315735; L(Test): 0.33834466338157654\n",
      "Epoch 236/1000: L(Train): 0.3431326150894165; L(Test): 0.336898535490036\n",
      "Epoch 237/1000: L(Train): 0.340984582901001; L(Test): 0.33551493287086487\n",
      "Epoch 238/1000: L(Train): 0.33066362142562866; L(Test): 0.3352743983268738\n",
      "Epoch 239/1000: L(Train): 0.3391062319278717; L(Test): 0.3361276388168335\n",
      "Epoch 240/1000: L(Train): 0.34502241015434265; L(Test): 0.3356229364871979\n",
      "Epoch 241/1000: L(Train): 0.3329775631427765; L(Test): 0.33456575870513916\n",
      "Epoch 242/1000: L(Train): 0.32634028792381287; L(Test): 0.3347088098526001\n",
      "Epoch 243/1000: L(Train): 0.33442801237106323; L(Test): 0.3350910246372223\n",
      "Epoch 244/1000: L(Train): 0.34134095907211304; L(Test): 0.33394551277160645\n",
      "Epoch 245/1000: L(Train): 0.34851548075675964; L(Test): 0.333718866109848\n",
      "Epoch 246/1000: L(Train): 0.33378127217292786; L(Test): 0.334483802318573\n",
      "Epoch 247/1000: L(Train): 0.34979596734046936; L(Test): 0.33337903022766113\n",
      "Epoch 248/1000: L(Train): 0.3292713165283203; L(Test): 0.33337560296058655\n",
      "Epoch 249/1000: L(Train): 0.3620195984840393; L(Test): 0.3329281210899353\n",
      "Epoch 250/1000: L(Train): 0.33099669218063354; L(Test): 0.3326073884963989\n",
      "Epoch 251/1000: L(Train): 0.34091082215309143; L(Test): 0.3320745825767517\n",
      "Epoch 252/1000: L(Train): 0.34857702255249023; L(Test): 0.3324081301689148\n",
      "Epoch 253/1000: L(Train): 0.3411572575569153; L(Test): 0.3317858874797821\n",
      "Epoch 254/1000: L(Train): 0.3336677551269531; L(Test): 0.3315991461277008\n",
      "Epoch 255/1000: L(Train): 0.33392998576164246; L(Test): 0.33179694414138794\n",
      "Epoch 256/1000: L(Train): 0.3454132378101349; L(Test): 0.33098989725112915\n",
      "Epoch 257/1000: L(Train): 0.33357104659080505; L(Test): 0.33032703399658203\n",
      "Epoch 258/1000: L(Train): 0.3267599940299988; L(Test): 0.3304082453250885\n",
      "Epoch 259/1000: L(Train): 0.33238881826400757; L(Test): 0.3299880623817444\n",
      "Epoch 260/1000: L(Train): 0.33141934871673584; L(Test): 0.32967308163642883\n",
      "Epoch 261/1000: L(Train): 0.3382127285003662; L(Test): 0.32990583777427673\n",
      "Epoch 262/1000: L(Train): 0.3288058638572693; L(Test): 0.3295324742794037\n",
      "Epoch 263/1000: L(Train): 0.3449869155883789; L(Test): 0.3286483585834503\n",
      "Epoch 264/1000: L(Train): 0.33364662528038025; L(Test): 0.3287600874900818\n",
      "Epoch 265/1000: L(Train): 0.33938995003700256; L(Test): 0.3292395770549774\n",
      "Epoch 266/1000: L(Train): 0.336791068315506; L(Test): 0.32848405838012695\n",
      "Epoch 267/1000: L(Train): 0.32936713099479675; L(Test): 0.32751548290252686\n",
      "Epoch 268/1000: L(Train): 0.3359162211418152; L(Test): 0.32737886905670166\n",
      "Epoch 269/1000: L(Train): 0.32978978753089905; L(Test): 0.3278430998325348\n",
      "Epoch 270/1000: L(Train): 0.33625340461730957; L(Test): 0.32687637209892273\n",
      "Epoch 271/1000: L(Train): 0.3218340575695038; L(Test): 0.3266335725784302\n",
      "Epoch 272/1000: L(Train): 0.3301897644996643; L(Test): 0.3264404833316803\n",
      "Epoch 273/1000: L(Train): 0.32750052213668823; L(Test): 0.3265923261642456\n",
      "Epoch 274/1000: L(Train): 0.3562229871749878; L(Test): 0.3256361484527588\n",
      "Epoch 275/1000: L(Train): 0.33749449253082275; L(Test): 0.3259117603302002\n",
      "Epoch 276/1000: L(Train): 0.34926146268844604; L(Test): 0.32544293999671936\n",
      "Epoch 277/1000: L(Train): 0.3412846624851227; L(Test): 0.3249560296535492\n",
      "Epoch 278/1000: L(Train): 0.35262975096702576; L(Test): 0.3246857523918152\n",
      "Epoch 279/1000: L(Train): 0.34699976444244385; L(Test): 0.3247077167034149\n",
      "Epoch 280/1000: L(Train): 0.32353469729423523; L(Test): 0.325261652469635\n",
      "Epoch 281/1000: L(Train): 0.3252122402191162; L(Test): 0.32390132546424866\n",
      "Epoch 282/1000: L(Train): 0.33422645926475525; L(Test): 0.323474645614624\n",
      "Epoch 283/1000: L(Train): 0.347168505191803; L(Test): 0.32409510016441345\n",
      "Epoch 284/1000: L(Train): 0.32520973682403564; L(Test): 0.3239382803440094\n",
      "Epoch 285/1000: L(Train): 0.3251412808895111; L(Test): 0.32287514209747314\n",
      "Epoch 286/1000: L(Train): 0.3180712163448334; L(Test): 0.3230697214603424\n",
      "Epoch 287/1000: L(Train): 0.32741567492485046; L(Test): 0.3243498206138611\n",
      "Epoch 288/1000: L(Train): 0.33604052662849426; L(Test): 0.3223264813423157\n",
      "Epoch 289/1000: L(Train): 0.3269573152065277; L(Test): 0.3219507336616516\n",
      "Epoch 290/1000: L(Train): 0.32449400424957275; L(Test): 0.32245486974716187\n",
      "Epoch 291/1000: L(Train): 0.3271107077598572; L(Test): 0.3223639726638794\n",
      "Epoch 292/1000: L(Train): 0.3258639872074127; L(Test): 0.3210817873477936\n",
      "Epoch 293/1000: L(Train): 0.3347334861755371; L(Test): 0.32101011276245117\n",
      "Epoch 294/1000: L(Train): 0.3295665979385376; L(Test): 0.32251420617103577\n",
      "Epoch 295/1000: L(Train): 0.3271975815296173; L(Test): 0.3213005065917969\n",
      "Epoch 296/1000: L(Train): 0.309965044260025; L(Test): 0.3206542730331421\n",
      "Epoch 297/1000: L(Train): 0.3291938006877899; L(Test): 0.320629358291626\n",
      "Epoch 298/1000: L(Train): 0.3366011679172516; L(Test): 0.32020196318626404\n",
      "Epoch 299/1000: L(Train): 0.32725590467453003; L(Test): 0.3199727535247803\n",
      "Epoch 300/1000: L(Train): 0.32615193724632263; L(Test): 0.3204217851161957\n",
      "Epoch 301/1000: L(Train): 0.32363051176071167; L(Test): 0.3202582895755768\n",
      "Epoch 302/1000: L(Train): 0.3320985436439514; L(Test): 0.31918177008628845\n",
      "Epoch 303/1000: L(Train): 0.3333965539932251; L(Test): 0.31900209188461304\n",
      "Epoch 304/1000: L(Train): 0.33003294467926025; L(Test): 0.31933099031448364\n",
      "Epoch 305/1000: L(Train): 0.3202030658721924; L(Test): 0.31886932253837585\n",
      "Epoch 306/1000: L(Train): 0.3120843470096588; L(Test): 0.31891292333602905\n",
      "Epoch 307/1000: L(Train): 0.3274805545806885; L(Test): 0.31897610425949097\n",
      "Epoch 308/1000: L(Train): 0.32528674602508545; L(Test): 0.3184773921966553\n",
      "Epoch 309/1000: L(Train): 0.3114579916000366; L(Test): 0.31851115822792053\n",
      "Epoch 310/1000: L(Train): 0.338971883058548; L(Test): 0.31769242882728577\n",
      "Epoch 311/1000: L(Train): 0.31474927067756653; L(Test): 0.31829363107681274\n",
      "Epoch 312/1000: L(Train): 0.303201287984848; L(Test): 0.3189035654067993\n",
      "Epoch 313/1000: L(Train): 0.3295883238315582; L(Test): 0.31714269518852234\n",
      "Epoch 314/1000: L(Train): 0.3158539831638336; L(Test): 0.3170584440231323\n",
      "Epoch 315/1000: L(Train): 0.30689048767089844; L(Test): 0.3181875944137573\n",
      "Epoch 316/1000: L(Train): 0.3285894989967346; L(Test): 0.3167478144168854\n",
      "Epoch 317/1000: L(Train): 0.3177022635936737; L(Test): 0.31629085540771484\n",
      "Epoch 318/1000: L(Train): 0.3249020576477051; L(Test): 0.316654771566391\n",
      "Epoch 319/1000: L(Train): 0.326395720243454; L(Test): 0.3166673481464386\n",
      "Epoch 320/1000: L(Train): 0.32443705201148987; L(Test): 0.31567272543907166\n",
      "Epoch 321/1000: L(Train): 0.3146796226501465; L(Test): 0.3157734274864197\n",
      "Epoch 322/1000: L(Train): 0.3267016112804413; L(Test): 0.3157937526702881\n",
      "Epoch 323/1000: L(Train): 0.3242868185043335; L(Test): 0.3154776394367218\n",
      "Epoch 324/1000: L(Train): 0.32595720887184143; L(Test): 0.3151201605796814\n",
      "Epoch 325/1000: L(Train): 0.30879655480384827; L(Test): 0.31525498628616333\n",
      "Epoch 326/1000: L(Train): 0.33147677779197693; L(Test): 0.31556376814842224\n",
      "Epoch 327/1000: L(Train): 0.3391267657279968; L(Test): 0.3148147165775299\n",
      "Epoch 328/1000: L(Train): 0.33549782633781433; L(Test): 0.31583091616630554\n",
      "Epoch 329/1000: L(Train): 0.3146499991416931; L(Test): 0.31525713205337524\n",
      "Epoch 330/1000: L(Train): 0.3386114835739136; L(Test): 0.3140714466571808\n",
      "Epoch 331/1000: L(Train): 0.3379073441028595; L(Test): 0.31369078159332275\n",
      "Epoch 332/1000: L(Train): 0.31725457310676575; L(Test): 0.3152596950531006\n",
      "Epoch 333/1000: L(Train): 0.32888391613960266; L(Test): 0.3138541877269745\n",
      "Epoch 334/1000: L(Train): 0.3131153881549835; L(Test): 0.31347814202308655\n",
      "Epoch 335/1000: L(Train): 0.31556999683380127; L(Test): 0.31429311633110046\n",
      "Epoch 336/1000: L(Train): 0.3150080144405365; L(Test): 0.3144729733467102\n",
      "Epoch 337/1000: L(Train): 0.3053063452243805; L(Test): 0.313062459230423\n",
      "Epoch 338/1000: L(Train): 0.32120653986930847; L(Test): 0.3129090368747711\n",
      "Epoch 339/1000: L(Train): 0.3291153311729431; L(Test): 0.3132452368736267\n",
      "Epoch 340/1000: L(Train): 0.32443463802337646; L(Test): 0.3127264678478241\n",
      "Epoch 341/1000: L(Train): 0.3206970989704132; L(Test): 0.3120180368423462\n",
      "Epoch 342/1000: L(Train): 0.33063703775405884; L(Test): 0.31197383999824524\n",
      "Epoch 343/1000: L(Train): 0.3085917830467224; L(Test): 0.3134881556034088\n",
      "Epoch 344/1000: L(Train): 0.3280286192893982; L(Test): 0.3123830556869507\n",
      "Epoch 345/1000: L(Train): 0.32509347796440125; L(Test): 0.3123193085193634\n",
      "Epoch 346/1000: L(Train): 0.3166749179363251; L(Test): 0.3121931850910187\n",
      "Epoch 347/1000: L(Train): 0.32940107583999634; L(Test): 0.3131323456764221\n",
      "Epoch 348/1000: L(Train): 0.3148885667324066; L(Test): 0.3110393285751343\n",
      "Epoch 349/1000: L(Train): 0.31406861543655396; L(Test): 0.3109138011932373\n",
      "Epoch 350/1000: L(Train): 0.3337385952472687; L(Test): 0.31200894713401794\n",
      "Epoch 351/1000: L(Train): 0.3251950442790985; L(Test): 0.3110937178134918\n",
      "Epoch 352/1000: L(Train): 0.312603235244751; L(Test): 0.3105291724205017\n",
      "Epoch 353/1000: L(Train): 0.31021568179130554; L(Test): 0.31038403511047363\n",
      "Epoch 354/1000: L(Train): 0.3115618824958801; L(Test): 0.31119975447654724\n",
      "Epoch 355/1000: L(Train): 0.3195606768131256; L(Test): 0.3104144334793091\n",
      "Epoch 356/1000: L(Train): 0.3067214787006378; L(Test): 0.30975645780563354\n",
      "Epoch 357/1000: L(Train): 0.3130687177181244; L(Test): 0.30958643555641174\n",
      "Epoch 358/1000: L(Train): 0.31205397844314575; L(Test): 0.30972668528556824\n",
      "Epoch 359/1000: L(Train): 0.30391231179237366; L(Test): 0.30989906191825867\n",
      "Epoch 360/1000: L(Train): 0.3063149154186249; L(Test): 0.3097257614135742\n",
      "Epoch 361/1000: L(Train): 0.30454233288764954; L(Test): 0.3093682825565338\n",
      "Epoch 362/1000: L(Train): 0.32127121090888977; L(Test): 0.3089114725589752\n",
      "Epoch 363/1000: L(Train): 0.3145977556705475; L(Test): 0.3083420395851135\n",
      "Epoch 364/1000: L(Train): 0.3030166029930115; L(Test): 0.3081454932689667\n",
      "Epoch 365/1000: L(Train): 0.3164052367210388; L(Test): 0.3085178732872009\n",
      "Epoch 366/1000: L(Train): 0.3078492283821106; L(Test): 0.3088129758834839\n",
      "Epoch 367/1000: L(Train): 0.30012691020965576; L(Test): 0.3089526891708374\n",
      "Epoch 368/1000: L(Train): 0.31332147121429443; L(Test): 0.30797040462493896\n",
      "Epoch 369/1000: L(Train): 0.3320259749889374; L(Test): 0.3074575364589691\n",
      "Epoch 370/1000: L(Train): 0.3063596189022064; L(Test): 0.30903124809265137\n",
      "Epoch 371/1000: L(Train): 0.32945239543914795; L(Test): 0.3072700500488281\n",
      "Epoch 372/1000: L(Train): 0.31862613558769226; L(Test): 0.3070778548717499\n",
      "Epoch 373/1000: L(Train): 0.31773829460144043; L(Test): 0.30791327357292175\n",
      "Epoch 374/1000: L(Train): 0.31457480788230896; L(Test): 0.30886703729629517\n",
      "Epoch 375/1000: L(Train): 0.31346458196640015; L(Test): 0.30739131569862366\n",
      "Epoch 376/1000: L(Train): 0.3173658847808838; L(Test): 0.30683866143226624\n",
      "Epoch 377/1000: L(Train): 0.32204657793045044; L(Test): 0.30645501613616943\n",
      "Epoch 378/1000: L(Train): 0.3240208029747009; L(Test): 0.30715155601501465\n",
      "Epoch 379/1000: L(Train): 0.3191626965999603; L(Test): 0.3060731887817383\n",
      "Epoch 380/1000: L(Train): 0.32740655541419983; L(Test): 0.3056521713733673\n",
      "Epoch 381/1000: L(Train): 0.32548850774765015; L(Test): 0.3057313859462738\n",
      "Epoch 382/1000: L(Train): 0.3288728594779968; L(Test): 0.30639979243278503\n",
      "Epoch 383/1000: L(Train): 0.3032897412776947; L(Test): 0.3063912093639374\n",
      "Epoch 384/1000: L(Train): 0.3247620463371277; L(Test): 0.3054230511188507\n",
      "Epoch 385/1000: L(Train): 0.3178572952747345; L(Test): 0.30485865473747253\n",
      "Epoch 386/1000: L(Train): 0.2998391389846802; L(Test): 0.3070600628852844\n",
      "Epoch 387/1000: L(Train): 0.30887311697006226; L(Test): 0.3050515353679657\n",
      "Epoch 388/1000: L(Train): 0.32409849762916565; L(Test): 0.3051368296146393\n",
      "Epoch 389/1000: L(Train): 0.31368544697761536; L(Test): 0.3042657971382141\n",
      "Epoch 390/1000: L(Train): 0.3009839951992035; L(Test): 0.3072624206542969\n",
      "Epoch 391/1000: L(Train): 0.323248028755188; L(Test): 0.3050415515899658\n",
      "Epoch 392/1000: L(Train): 0.32029128074645996; L(Test): 0.30445870757102966\n",
      "Epoch 393/1000: L(Train): 0.32900238037109375; L(Test): 0.3035755157470703\n",
      "Epoch 394/1000: L(Train): 0.29788732528686523; L(Test): 0.30490365624427795\n",
      "Epoch 395/1000: L(Train): 0.3011716306209564; L(Test): 0.303975909948349\n",
      "Epoch 396/1000: L(Train): 0.3140440881252289; L(Test): 0.30332380533218384\n",
      "Epoch 397/1000: L(Train): 0.31334009766578674; L(Test): 0.3031945824623108\n",
      "Epoch 398/1000: L(Train): 0.31267616152763367; L(Test): 0.3035058081150055\n",
      "Epoch 399/1000: L(Train): 0.314430832862854; L(Test): 0.3034496009349823\n",
      "Epoch 400/1000: L(Train): 0.31720221042633057; L(Test): 0.30249980092048645\n",
      "Epoch 401/1000: L(Train): 0.3140791058540344; L(Test): 0.30222436785697937\n",
      "Epoch 402/1000: L(Train): 0.3073340952396393; L(Test): 0.30281898379325867\n",
      "Epoch 403/1000: L(Train): 0.3066514730453491; L(Test): 0.30249735713005066\n",
      "Epoch 404/1000: L(Train): 0.2994561493396759; L(Test): 0.3019784986972809\n",
      "Epoch 405/1000: L(Train): 0.2888364791870117; L(Test): 0.30244705080986023\n",
      "Epoch 406/1000: L(Train): 0.3090961277484894; L(Test): 0.3030490577220917\n",
      "Epoch 407/1000: L(Train): 0.310753732919693; L(Test): 0.3016389012336731\n",
      "Epoch 408/1000: L(Train): 0.32165729999542236; L(Test): 0.3014450669288635\n",
      "Epoch 409/1000: L(Train): 0.3099537491798401; L(Test): 0.30171138048171997\n",
      "Epoch 410/1000: L(Train): 0.3182840049266815; L(Test): 0.301443487405777\n",
      "Epoch 411/1000: L(Train): 0.2925884425640106; L(Test): 0.3012707233428955\n",
      "Epoch 412/1000: L(Train): 0.3082878887653351; L(Test): 0.30120861530303955\n",
      "Epoch 413/1000: L(Train): 0.3098711669445038; L(Test): 0.30185121297836304\n",
      "Epoch 414/1000: L(Train): 0.31222566962242126; L(Test): 0.30142655968666077\n",
      "Epoch 415/1000: L(Train): 0.2895745635032654; L(Test): 0.3010658621788025\n",
      "Epoch 416/1000: L(Train): 0.31783977150917053; L(Test): 0.3003128170967102\n",
      "Epoch 417/1000: L(Train): 0.300242155790329; L(Test): 0.2999853789806366\n",
      "Epoch 418/1000: L(Train): 0.30119815468788147; L(Test): 0.3008764982223511\n",
      "Epoch 419/1000: L(Train): 0.33394843339920044; L(Test): 0.3006552457809448\n",
      "Epoch 420/1000: L(Train): 0.32553547620773315; L(Test): 0.2993515133857727\n",
      "Epoch 421/1000: L(Train): 0.31946825981140137; L(Test): 0.29909035563468933\n",
      "Epoch 422/1000: L(Train): 0.31738927960395813; L(Test): 0.2999733090400696\n",
      "Epoch 423/1000: L(Train): 0.31217047572135925; L(Test): 0.29960259795188904\n",
      "Epoch 424/1000: L(Train): 0.3115268349647522; L(Test): 0.2984303832054138\n",
      "Epoch 425/1000: L(Train): 0.31448155641555786; L(Test): 0.2982577383518219\n",
      "Epoch 426/1000: L(Train): 0.3056713044643402; L(Test): 0.3002380430698395\n",
      "Epoch 427/1000: L(Train): 0.3107872009277344; L(Test): 0.29949623346328735\n",
      "Epoch 428/1000: L(Train): 0.29384031891822815; L(Test): 0.29824966192245483\n",
      "Epoch 429/1000: L(Train): 0.29307451844215393; L(Test): 0.2977236211299896\n",
      "Epoch 430/1000: L(Train): 0.313589483499527; L(Test): 0.29882362484931946\n",
      "Epoch 431/1000: L(Train): 0.29862868785858154; L(Test): 0.29824259877204895\n",
      "Epoch 432/1000: L(Train): 0.32754138112068176; L(Test): 0.29794785380363464\n",
      "Epoch 433/1000: L(Train): 0.3113560080528259; L(Test): 0.29758113622665405\n",
      "Epoch 434/1000: L(Train): 0.28863582015037537; L(Test): 0.2996678948402405\n",
      "Epoch 435/1000: L(Train): 0.30645808577537537; L(Test): 0.2979150116443634\n",
      "Epoch 436/1000: L(Train): 0.320015549659729; L(Test): 0.29708462953567505\n",
      "Epoch 437/1000: L(Train): 0.30504584312438965; L(Test): 0.29686349630355835\n",
      "Epoch 438/1000: L(Train): 0.3153044879436493; L(Test): 0.29693323373794556\n",
      "Epoch 439/1000: L(Train): 0.32314303517341614; L(Test): 0.29735448956489563\n",
      "Epoch 440/1000: L(Train): 0.3054935336112976; L(Test): 0.29627329111099243\n",
      "Epoch 441/1000: L(Train): 0.32918915152549744; L(Test): 0.29601356387138367\n",
      "Epoch 442/1000: L(Train): 0.3155825138092041; L(Test): 0.29707401990890503\n",
      "Epoch 443/1000: L(Train): 0.3041447699069977; L(Test): 0.29685765504837036\n",
      "Epoch 444/1000: L(Train): 0.2960796654224396; L(Test): 0.29638978838920593\n",
      "Epoch 445/1000: L(Train): 0.30802682042121887; L(Test): 0.2958247661590576\n",
      "Epoch 446/1000: L(Train): 0.3168516755104065; L(Test): 0.29498615860939026\n",
      "Epoch 447/1000: L(Train): 0.32138538360595703; L(Test): 0.29537254571914673\n",
      "Epoch 448/1000: L(Train): 0.30816513299942017; L(Test): 0.2950544059276581\n",
      "Epoch 449/1000: L(Train): 0.2962545156478882; L(Test): 0.2946704626083374\n",
      "Epoch 450/1000: L(Train): 0.3178425431251526; L(Test): 0.29438936710357666\n",
      "Epoch 451/1000: L(Train): 0.3098709285259247; L(Test): 0.29451677203178406\n",
      "Epoch 452/1000: L(Train): 0.32529112696647644; L(Test): 0.29419660568237305\n",
      "Epoch 453/1000: L(Train): 0.32738566398620605; L(Test): 0.2938847541809082\n",
      "Epoch 454/1000: L(Train): 0.31864073872566223; L(Test): 0.29378101229667664\n",
      "Epoch 455/1000: L(Train): 0.31364715099334717; L(Test): 0.29438236355781555\n",
      "Epoch 456/1000: L(Train): 0.3197433054447174; L(Test): 0.29353243112564087\n",
      "Epoch 457/1000: L(Train): 0.3098048269748688; L(Test): 0.29313522577285767\n",
      "Epoch 458/1000: L(Train): 0.29707643389701843; L(Test): 0.29292669892311096\n",
      "Epoch 459/1000: L(Train): 0.3046312630176544; L(Test): 0.29362961649894714\n",
      "Epoch 460/1000: L(Train): 0.31311172246932983; L(Test): 0.2927418351173401\n",
      "Epoch 461/1000: L(Train): 0.2828311622142792; L(Test): 0.2922838628292084\n",
      "Epoch 462/1000: L(Train): 0.30587005615234375; L(Test): 0.2928301990032196\n",
      "Epoch 463/1000: L(Train): 0.3041449189186096; L(Test): 0.29355382919311523\n",
      "Epoch 464/1000: L(Train): 0.29725199937820435; L(Test): 0.29218700528144836\n",
      "Epoch 465/1000: L(Train): 0.3100243806838989; L(Test): 0.29196077585220337\n",
      "Epoch 466/1000: L(Train): 0.3222959339618683; L(Test): 0.2920079231262207\n",
      "Epoch 467/1000: L(Train): 0.3027689754962921; L(Test): 0.29210323095321655\n",
      "Epoch 468/1000: L(Train): 0.3105177581310272; L(Test): 0.29164761304855347\n",
      "Epoch 469/1000: L(Train): 0.2991344928741455; L(Test): 0.29206377267837524\n",
      "Epoch 470/1000: L(Train): 0.2944710850715637; L(Test): 0.29228976368904114\n",
      "Epoch 471/1000: L(Train): 0.29977738857269287; L(Test): 0.29137980937957764\n",
      "Epoch 472/1000: L(Train): 0.3091883957386017; L(Test): 0.2914567291736603\n",
      "Epoch 473/1000: L(Train): 0.30171605944633484; L(Test): 0.2909049987792969\n",
      "Epoch 474/1000: L(Train): 0.2995222806930542; L(Test): 0.2918784022331238\n",
      "Epoch 475/1000: L(Train): 0.3135388493537903; L(Test): 0.29082077741622925\n",
      "Epoch 476/1000: L(Train): 0.2939918041229248; L(Test): 0.29123249650001526\n",
      "Epoch 477/1000: L(Train): 0.3004514276981354; L(Test): 0.29109442234039307\n",
      "Epoch 478/1000: L(Train): 0.3038964867591858; L(Test): 0.2903698980808258\n",
      "Epoch 479/1000: L(Train): 0.3027489185333252; L(Test): 0.29063642024993896\n",
      "Epoch 480/1000: L(Train): 0.3051737844944; L(Test): 0.2902427017688751\n",
      "Epoch 481/1000: L(Train): 0.28882700204849243; L(Test): 0.2901431620121002\n",
      "Epoch 482/1000: L(Train): 0.32415658235549927; L(Test): 0.28936833143234253\n",
      "Epoch 483/1000: L(Train): 0.31181463599205017; L(Test): 0.28939640522003174\n",
      "Epoch 484/1000: L(Train): 0.29468831419944763; L(Test): 0.29023969173431396\n",
      "Epoch 485/1000: L(Train): 0.2907799482345581; L(Test): 0.2911381721496582\n",
      "Epoch 486/1000: L(Train): 0.30700215697288513; L(Test): 0.2892608642578125\n",
      "Epoch 487/1000: L(Train): 0.29959845542907715; L(Test): 0.2891954183578491\n",
      "Epoch 488/1000: L(Train): 0.30248281359672546; L(Test): 0.2898496091365814\n",
      "Epoch 489/1000: L(Train): 0.2941807508468628; L(Test): 0.2903038263320923\n",
      "Epoch 490/1000: L(Train): 0.3090001940727234; L(Test): 0.28903377056121826\n",
      "Epoch 491/1000: L(Train): 0.3132958710193634; L(Test): 0.28842291235923767\n",
      "Epoch 492/1000: L(Train): 0.28062763810157776; L(Test): 0.2893752455711365\n",
      "Epoch 493/1000: L(Train): 0.2944330871105194; L(Test): 0.288813978433609\n",
      "Epoch 494/1000: L(Train): 0.2888915240764618; L(Test): 0.28840371966362\n",
      "Epoch 495/1000: L(Train): 0.3006806969642639; L(Test): 0.2880552113056183\n",
      "Epoch 496/1000: L(Train): 0.29815033078193665; L(Test): 0.2888490855693817\n",
      "Epoch 497/1000: L(Train): 0.29633644223213196; L(Test): 0.2880406081676483\n",
      "Epoch 498/1000: L(Train): 0.3034129738807678; L(Test): 0.28760161995887756\n",
      "Epoch 499/1000: L(Train): 0.30849480628967285; L(Test): 0.28808096051216125\n",
      "Epoch 500/1000: L(Train): 0.2983391284942627; L(Test): 0.2874537706375122\n",
      "Epoch 501/1000: L(Train): 0.29287028312683105; L(Test): 0.28694507479667664\n",
      "Epoch 502/1000: L(Train): 0.28973641991615295; L(Test): 0.28677886724472046\n",
      "Epoch 503/1000: L(Train): 0.2966952323913574; L(Test): 0.28719067573547363\n",
      "Epoch 504/1000: L(Train): 0.3095523715019226; L(Test): 0.28629007935523987\n",
      "Epoch 505/1000: L(Train): 0.2938265800476074; L(Test): 0.2864694893360138\n",
      "Epoch 506/1000: L(Train): 0.2984090745449066; L(Test): 0.2866901755332947\n",
      "Epoch 507/1000: L(Train): 0.29159095883369446; L(Test): 0.2860715091228485\n",
      "Epoch 508/1000: L(Train): 0.28381219506263733; L(Test): 0.28572559356689453\n",
      "Epoch 509/1000: L(Train): 0.2867967188358307; L(Test): 0.2860497236251831\n",
      "Epoch 510/1000: L(Train): 0.30798274278640747; L(Test): 0.2857591509819031\n",
      "Epoch 511/1000: L(Train): 0.30400174856185913; L(Test): 0.28556734323501587\n",
      "Epoch 512/1000: L(Train): 0.29676011204719543; L(Test): 0.2849852442741394\n",
      "Epoch 513/1000: L(Train): 0.3016526997089386; L(Test): 0.2849988043308258\n",
      "Epoch 514/1000: L(Train): 0.29985126852989197; L(Test): 0.28636589646339417\n",
      "Epoch 515/1000: L(Train): 0.29576992988586426; L(Test): 0.2849155068397522\n",
      "Epoch 516/1000: L(Train): 0.280924916267395; L(Test): 0.28471025824546814\n",
      "Epoch 517/1000: L(Train): 0.2950252890586853; L(Test): 0.28658443689346313\n",
      "Epoch 518/1000: L(Train): 0.29789260029792786; L(Test): 0.2852422893047333\n",
      "Epoch 519/1000: L(Train): 0.30362144112586975; L(Test): 0.28539392352104187\n",
      "Epoch 520/1000: L(Train): 0.30646565556526184; L(Test): 0.28559815883636475\n",
      "Epoch 521/1000: L(Train): 0.3013207018375397; L(Test): 0.28695347905158997\n",
      "Epoch 522/1000: L(Train): 0.3012653887271881; L(Test): 0.28418442606925964\n",
      "Epoch 523/1000: L(Train): 0.30774205923080444; L(Test): 0.2849747836589813\n",
      "Epoch 524/1000: L(Train): 0.307865172624588; L(Test): 0.28580206632614136\n",
      "Epoch 525/1000: L(Train): 0.29778310656547546; L(Test): 0.2874848246574402\n",
      "Epoch 526/1000: L(Train): 0.3011942505836487; L(Test): 0.28415021300315857\n",
      "Epoch 527/1000: L(Train): 0.28604239225387573; L(Test): 0.28422972559928894\n",
      "Epoch 528/1000: L(Train): 0.2896130084991455; L(Test): 0.2854754328727722\n",
      "Epoch 529/1000: L(Train): 0.3069418668746948; L(Test): 0.2851090133190155\n",
      "Epoch 530/1000: L(Train): 0.29457584023475647; L(Test): 0.2838117778301239\n",
      "Epoch 531/1000: L(Train): 0.3122085630893707; L(Test): 0.28349459171295166\n",
      "Epoch 532/1000: L(Train): 0.2974489629268646; L(Test): 0.2852208614349365\n",
      "Epoch 533/1000: L(Train): 0.30341681838035583; L(Test): 0.2837492525577545\n",
      "Epoch 534/1000: L(Train): 0.29224464297294617; L(Test): 0.2830863893032074\n",
      "Epoch 535/1000: L(Train): 0.2937833368778229; L(Test): 0.2837834358215332\n",
      "Epoch 536/1000: L(Train): 0.28845861554145813; L(Test): 0.28419601917266846\n",
      "Epoch 537/1000: L(Train): 0.3044888973236084; L(Test): 0.2836034893989563\n",
      "Epoch 538/1000: L(Train): 0.29505735635757446; L(Test): 0.2831045985221863\n",
      "Epoch 539/1000: L(Train): 0.30302906036376953; L(Test): 0.28325945138931274\n",
      "Epoch 540/1000: L(Train): 0.27982574701309204; L(Test): 0.2833133339881897\n",
      "Epoch 541/1000: L(Train): 0.29367080330848694; L(Test): 0.28344687819480896\n",
      "Epoch 542/1000: L(Train): 0.28658199310302734; L(Test): 0.2842603921890259\n",
      "Epoch 543/1000: L(Train): 0.29882389307022095; L(Test): 0.2837481200695038\n",
      "Epoch 544/1000: L(Train): 0.3070794641971588; L(Test): 0.28351324796676636\n",
      "Epoch 545/1000: L(Train): 0.2975100576877594; L(Test): 0.28287819027900696\n",
      "Epoch 546/1000: L(Train): 0.30837544798851013; L(Test): 0.283197820186615\n",
      "Epoch 547/1000: L(Train): 0.29451870918273926; L(Test): 0.28295060992240906\n",
      "Epoch 548/1000: L(Train): 0.29198938608169556; L(Test): 0.28284934163093567\n",
      "Epoch 549/1000: L(Train): 0.30615395307540894; L(Test): 0.2827690541744232\n",
      "Epoch 550/1000: L(Train): 0.29533883929252625; L(Test): 0.2822380065917969\n",
      "Epoch 551/1000: L(Train): 0.28279823064804077; L(Test): 0.28274938464164734\n",
      "Epoch 552/1000: L(Train): 0.3068431615829468; L(Test): 0.28146985173225403\n",
      "Epoch 553/1000: L(Train): 0.2922600209712982; L(Test): 0.2815999984741211\n",
      "Epoch 554/1000: L(Train): 0.29164183139801025; L(Test): 0.2823919355869293\n",
      "Epoch 555/1000: L(Train): 0.2884676158428192; L(Test): 0.2829483449459076\n",
      "Epoch 556/1000: L(Train): 0.28690147399902344; L(Test): 0.28161314129829407\n",
      "Epoch 557/1000: L(Train): 0.31025177240371704; L(Test): 0.2810758650302887\n",
      "Epoch 558/1000: L(Train): 0.28167277574539185; L(Test): 0.2819320261478424\n",
      "Epoch 559/1000: L(Train): 0.30647486448287964; L(Test): 0.28135421872138977\n",
      "Epoch 560/1000: L(Train): 0.29220467805862427; L(Test): 0.2807479798793793\n",
      "Epoch 561/1000: L(Train): 0.27742671966552734; L(Test): 0.2804098427295685\n",
      "Epoch 562/1000: L(Train): 0.2773720324039459; L(Test): 0.28105589747428894\n",
      "Epoch 563/1000: L(Train): 0.2885831594467163; L(Test): 0.2812933623790741\n",
      "Epoch 564/1000: L(Train): 0.29183119535446167; L(Test): 0.27994877099990845\n",
      "Epoch 565/1000: L(Train): 0.28369325399398804; L(Test): 0.28001025319099426\n",
      "Epoch 566/1000: L(Train): 0.2943422198295593; L(Test): 0.28234580159187317\n",
      "Epoch 567/1000: L(Train): 0.2891101837158203; L(Test): 0.28044593334198\n",
      "Epoch 568/1000: L(Train): 0.2972797453403473; L(Test): 0.2809625267982483\n",
      "Epoch 569/1000: L(Train): 0.30848565697669983; L(Test): 0.2804443836212158\n",
      "Epoch 570/1000: L(Train): 0.30066847801208496; L(Test): 0.2831515371799469\n",
      "Epoch 571/1000: L(Train): 0.29348263144493103; L(Test): 0.28110066056251526\n",
      "Epoch 572/1000: L(Train): 0.27983975410461426; L(Test): 0.2812344431877136\n",
      "Epoch 573/1000: L(Train): 0.308767169713974; L(Test): 0.28161147236824036\n",
      "Epoch 574/1000: L(Train): 0.2944797873497009; L(Test): 0.2810651957988739\n",
      "Epoch 575/1000: L(Train): 0.297068327665329; L(Test): 0.2808471918106079\n",
      "Epoch 576/1000: L(Train): 0.2980603575706482; L(Test): 0.28013595938682556\n",
      "Epoch 577/1000: L(Train): 0.301173597574234; L(Test): 0.2801312506198883\n",
      "Epoch 578/1000: L(Train): 0.2926139235496521; L(Test): 0.28163135051727295\n",
      "Epoch 579/1000: L(Train): 0.29853925108909607; L(Test): 0.2791869342327118\n",
      "Epoch 580/1000: L(Train): 0.28190338611602783; L(Test): 0.2805073857307434\n",
      "Epoch 581/1000: L(Train): 0.3042161762714386; L(Test): 0.2805114984512329\n",
      "Epoch 582/1000: L(Train): 0.31647658348083496; L(Test): 0.27935174107551575\n",
      "Epoch 583/1000: L(Train): 0.2825563848018646; L(Test): 0.2792668342590332\n",
      "Epoch 584/1000: L(Train): 0.29404738545417786; L(Test): 0.2785462439060211\n",
      "Epoch 585/1000: L(Train): 0.2889168858528137; L(Test): 0.2795795202255249\n",
      "Epoch 586/1000: L(Train): 0.2922472357749939; L(Test): 0.2792014181613922\n",
      "Epoch 587/1000: L(Train): 0.2940830588340759; L(Test): 0.27881932258605957\n",
      "Epoch 588/1000: L(Train): 0.2983241677284241; L(Test): 0.2790428698062897\n",
      "Epoch 589/1000: L(Train): 0.2934190332889557; L(Test): 0.2790161371231079\n",
      "Epoch 590/1000: L(Train): 0.29301953315734863; L(Test): 0.27869245409965515\n",
      "Epoch 591/1000: L(Train): 0.27463749051094055; L(Test): 0.27748700976371765\n",
      "Epoch 592/1000: L(Train): 0.2936539947986603; L(Test): 0.27846503257751465\n",
      "Epoch 593/1000: L(Train): 0.2954549789428711; L(Test): 0.27925771474838257\n",
      "Epoch 594/1000: L(Train): 0.28025051951408386; L(Test): 0.27881452441215515\n",
      "Epoch 595/1000: L(Train): 0.30061185359954834; L(Test): 0.2797340154647827\n",
      "Epoch 596/1000: L(Train): 0.29646065831184387; L(Test): 0.2796359658241272\n",
      "Epoch 597/1000: L(Train): 0.2904544472694397; L(Test): 0.27957209944725037\n",
      "Epoch 598/1000: L(Train): 0.30044645071029663; L(Test): 0.27833235263824463\n",
      "Epoch 599/1000: L(Train): 0.3068699836730957; L(Test): 0.2773989737033844\n",
      "Epoch 600/1000: L(Train): 0.2927522659301758; L(Test): 0.2799098491668701\n",
      "Epoch 601/1000: L(Train): 0.3100658059120178; L(Test): 0.2779080867767334\n",
      "Epoch 602/1000: L(Train): 0.2940760552883148; L(Test): 0.2777520418167114\n",
      "Epoch 603/1000: L(Train): 0.28575950860977173; L(Test): 0.2773416340351105\n",
      "Epoch 604/1000: L(Train): 0.28630566596984863; L(Test): 0.27877122163772583\n",
      "Epoch 605/1000: L(Train): 0.2922170162200928; L(Test): 0.27744829654693604\n",
      "Epoch 606/1000: L(Train): 0.2897980511188507; L(Test): 0.27767181396484375\n",
      "Epoch 607/1000: L(Train): 0.2918801009654999; L(Test): 0.2790270149707794\n",
      "Epoch 608/1000: L(Train): 0.29461467266082764; L(Test): 0.2774827182292938\n",
      "Epoch 609/1000: L(Train): 0.3000023365020752; L(Test): 0.27696365118026733\n",
      "Epoch 610/1000: L(Train): 0.2910294830799103; L(Test): 0.2769164443016052\n",
      "Epoch 611/1000: L(Train): 0.28810954093933105; L(Test): 0.27910593152046204\n",
      "Epoch 612/1000: L(Train): 0.30042868852615356; L(Test): 0.2768523395061493\n",
      "Epoch 613/1000: L(Train): 0.29480263590812683; L(Test): 0.2763403058052063\n",
      "Epoch 614/1000: L(Train): 0.30078965425491333; L(Test): 0.27647462487220764\n",
      "Epoch 615/1000: L(Train): 0.28985708951950073; L(Test): 0.2774159908294678\n",
      "Epoch 616/1000: L(Train): 0.2922021448612213; L(Test): 0.2771015763282776\n",
      "Epoch 617/1000: L(Train): 0.29644736647605896; L(Test): 0.2764972448348999\n",
      "Epoch 618/1000: L(Train): 0.2905897796154022; L(Test): 0.2757219970226288\n",
      "Epoch 619/1000: L(Train): 0.28563931584358215; L(Test): 0.276721715927124\n",
      "Epoch 620/1000: L(Train): 0.29028138518333435; L(Test): 0.27564460039138794\n",
      "Epoch 621/1000: L(Train): 0.27749401330947876; L(Test): 0.2757580280303955\n",
      "Epoch 622/1000: L(Train): 0.28996655344963074; L(Test): 0.2763701379299164\n",
      "Epoch 623/1000: L(Train): 0.2994750440120697; L(Test): 0.27596789598464966\n",
      "Epoch 624/1000: L(Train): 0.2976955771446228; L(Test): 0.27549582719802856\n",
      "Epoch 625/1000: L(Train): 0.2811853885650635; L(Test): 0.275465726852417\n",
      "Epoch 626/1000: L(Train): 0.28448542952537537; L(Test): 0.27581480145454407\n",
      "Epoch 627/1000: L(Train): 0.28106147050857544; L(Test): 0.27572962641716003\n",
      "Epoch 628/1000: L(Train): 0.29544833302497864; L(Test): 0.27546289563179016\n",
      "Epoch 629/1000: L(Train): 0.29576238989830017; L(Test): 0.275174617767334\n",
      "Epoch 630/1000: L(Train): 0.26899680495262146; L(Test): 0.2753427028656006\n",
      "Epoch 631/1000: L(Train): 0.29800713062286377; L(Test): 0.27549508213996887\n",
      "Epoch 632/1000: L(Train): 0.29086360335350037; L(Test): 0.2750299870967865\n",
      "Epoch 633/1000: L(Train): 0.3112897276878357; L(Test): 0.27488642930984497\n",
      "Epoch 634/1000: L(Train): 0.28666970133781433; L(Test): 0.27466264367103577\n",
      "Epoch 635/1000: L(Train): 0.2828385829925537; L(Test): 0.27594709396362305\n",
      "Epoch 636/1000: L(Train): 0.28675615787506104; L(Test): 0.27487438917160034\n",
      "Epoch 637/1000: L(Train): 0.2938629984855652; L(Test): 0.275542676448822\n",
      "Epoch 638/1000: L(Train): 0.29880961775779724; L(Test): 0.2748907804489136\n",
      "Epoch 639/1000: L(Train): 0.29615938663482666; L(Test): 0.2750392556190491\n",
      "Epoch 640/1000: L(Train): 0.2818324565887451; L(Test): 0.27390092611312866\n",
      "Epoch 641/1000: L(Train): 0.27553027868270874; L(Test): 0.2735935151576996\n",
      "Epoch 642/1000: L(Train): 0.29876527190208435; L(Test): 0.2739555537700653\n",
      "Epoch 643/1000: L(Train): 0.27780336141586304; L(Test): 0.27491334080696106\n",
      "Epoch 644/1000: L(Train): 0.2913784384727478; L(Test): 0.27380192279815674\n",
      "Epoch 645/1000: L(Train): 0.27437612414360046; L(Test): 0.27372005581855774\n",
      "Epoch 646/1000: L(Train): 0.28166648745536804; L(Test): 0.2748660445213318\n",
      "Epoch 647/1000: L(Train): 0.3057204484939575; L(Test): 0.2737475633621216\n",
      "Epoch 648/1000: L(Train): 0.2859456539154053; L(Test): 0.273605614900589\n",
      "Epoch 649/1000: L(Train): 0.2951979637145996; L(Test): 0.2738591432571411\n",
      "Epoch 650/1000: L(Train): 0.28634217381477356; L(Test): 0.27428770065307617\n",
      "Epoch 651/1000: L(Train): 0.2796681821346283; L(Test): 0.2742508053779602\n",
      "Epoch 652/1000: L(Train): 0.29347386956214905; L(Test): 0.2740263044834137\n",
      "Epoch 653/1000: L(Train): 0.30836331844329834; L(Test): 0.27459678053855896\n",
      "Epoch 654/1000: L(Train): 0.2833127975463867; L(Test): 0.27665334939956665\n",
      "Epoch 655/1000: L(Train): 0.298564076423645; L(Test): 0.27353864908218384\n",
      "Epoch 656/1000: L(Train): 0.2916647493839264; L(Test): 0.2737919092178345\n",
      "Epoch 657/1000: L(Train): 0.29656487703323364; L(Test): 0.27552253007888794\n",
      "Epoch 658/1000: L(Train): 0.298658162355423; L(Test): 0.2765054404735565\n",
      "Epoch 659/1000: L(Train): 0.298076331615448; L(Test): 0.2729693055152893\n",
      "Epoch 660/1000: L(Train): 0.29115140438079834; L(Test): 0.273787260055542\n",
      "Epoch 661/1000: L(Train): 0.28943097591400146; L(Test): 0.2741207182407379\n",
      "Epoch 662/1000: L(Train): 0.3016888499259949; L(Test): 0.2748195230960846\n",
      "Epoch 663/1000: L(Train): 0.3011990487575531; L(Test): 0.2737378180027008\n",
      "Epoch 664/1000: L(Train): 0.29459717869758606; L(Test): 0.2732183039188385\n",
      "Epoch 665/1000: L(Train): 0.2831753194332123; L(Test): 0.2731815576553345\n",
      "Epoch 666/1000: L(Train): 0.3007905185222626; L(Test): 0.274859756231308\n",
      "Epoch 667/1000: L(Train): 0.2786826491355896; L(Test): 0.27426406741142273\n",
      "Epoch 668/1000: L(Train): 0.29180657863616943; L(Test): 0.27376672625541687\n",
      "Epoch 669/1000: L(Train): 0.27905791997909546; L(Test): 0.27329981327056885\n",
      "Epoch 670/1000: L(Train): 0.27709102630615234; L(Test): 0.2738417088985443\n",
      "Epoch 671/1000: L(Train): 0.3027963638305664; L(Test): 0.2723456621170044\n",
      "Epoch 672/1000: L(Train): 0.26869720220565796; L(Test): 0.2721749246120453\n",
      "Epoch 673/1000: L(Train): 0.2850314676761627; L(Test): 0.27281832695007324\n",
      "Epoch 674/1000: L(Train): 0.28143057227134705; L(Test): 0.2741425335407257\n",
      "Epoch 675/1000: L(Train): 0.28683069348335266; L(Test): 0.27270615100860596\n",
      "Epoch 676/1000: L(Train): 0.28887978196144104; L(Test): 0.27266842126846313\n",
      "Epoch 677/1000: L(Train): 0.29489877820014954; L(Test): 0.27227678894996643\n",
      "Epoch 678/1000: L(Train): 0.27279484272003174; L(Test): 0.2728066146373749\n",
      "Epoch 679/1000: L(Train): 0.2737930119037628; L(Test): 0.2723085284233093\n",
      "Epoch 680/1000: L(Train): 0.29758477210998535; L(Test): 0.2719121277332306\n",
      "Epoch 681/1000: L(Train): 0.2912555932998657; L(Test): 0.2723122537136078\n",
      "Epoch 682/1000: L(Train): 0.28921860456466675; L(Test): 0.27269262075424194\n",
      "Epoch 683/1000: L(Train): 0.2943406403064728; L(Test): 0.2719414234161377\n",
      "Epoch 684/1000: L(Train): 0.2856043875217438; L(Test): 0.2718077301979065\n",
      "Epoch 685/1000: L(Train): 0.2797306180000305; L(Test): 0.2722412049770355\n",
      "Epoch 686/1000: L(Train): 0.2839225232601166; L(Test): 0.2725556790828705\n",
      "Epoch 687/1000: L(Train): 0.2846037745475769; L(Test): 0.2720492482185364\n",
      "Epoch 688/1000: L(Train): 0.3063870370388031; L(Test): 0.2718339264392853\n",
      "Epoch 689/1000: L(Train): 0.2726891338825226; L(Test): 0.27204182744026184\n",
      "Epoch 690/1000: L(Train): 0.30497094988822937; L(Test): 0.2711748480796814\n",
      "Epoch 691/1000: L(Train): 0.2875165641307831; L(Test): 0.2713552415370941\n",
      "Epoch 692/1000: L(Train): 0.27990707755088806; L(Test): 0.27257969975471497\n",
      "Epoch 693/1000: L(Train): 0.2944963574409485; L(Test): 0.27210089564323425\n",
      "Epoch 694/1000: L(Train): 0.30386945605278015; L(Test): 0.27118444442749023\n",
      "Epoch 695/1000: L(Train): 0.27374011278152466; L(Test): 0.2713698446750641\n",
      "Epoch 696/1000: L(Train): 0.28086522221565247; L(Test): 0.2723018527030945\n",
      "Epoch 697/1000: L(Train): 0.28567832708358765; L(Test): 0.27255916595458984\n",
      "Epoch 698/1000: L(Train): 0.2798803746700287; L(Test): 0.271386981010437\n",
      "Epoch 699/1000: L(Train): 0.29472437500953674; L(Test): 0.2711256444454193\n",
      "Epoch 700/1000: L(Train): 0.29879167675971985; L(Test): 0.2715350389480591\n",
      "Epoch 701/1000: L(Train): 0.27271488308906555; L(Test): 0.27059319615364075\n",
      "Epoch 702/1000: L(Train): 0.28722429275512695; L(Test): 0.27114421129226685\n",
      "Epoch 703/1000: L(Train): 0.28229308128356934; L(Test): 0.27124443650245667\n",
      "Epoch 704/1000: L(Train): 0.27894601225852966; L(Test): 0.2718949019908905\n",
      "Epoch 705/1000: L(Train): 0.2962097227573395; L(Test): 0.2709898352622986\n",
      "Epoch 706/1000: L(Train): 0.2873614430427551; L(Test): 0.27133408188819885\n",
      "Epoch 707/1000: L(Train): 0.2929789423942566; L(Test): 0.2711488902568817\n",
      "Epoch 708/1000: L(Train): 0.30426138639450073; L(Test): 0.2718331813812256\n",
      "Epoch 709/1000: L(Train): 0.2982761263847351; L(Test): 0.27167388796806335\n",
      "Epoch 710/1000: L(Train): 0.29919707775115967; L(Test): 0.27100369334220886\n",
      "Epoch 711/1000: L(Train): 0.29544585943222046; L(Test): 0.27118489146232605\n",
      "Epoch 712/1000: L(Train): 0.286302775144577; L(Test): 0.2713060975074768\n",
      "Epoch 713/1000: L(Train): 0.28084683418273926; L(Test): 0.2721775472164154\n",
      "Epoch 714/1000: L(Train): 0.28308066725730896; L(Test): 0.27156051993370056\n",
      "Epoch 715/1000: L(Train): 0.2916654646396637; L(Test): 0.27172333002090454\n",
      "Epoch 716/1000: L(Train): 0.28647732734680176; L(Test): 0.2707989811897278\n",
      "Epoch 717/1000: L(Train): 0.2905389666557312; L(Test): 0.27031704783439636\n",
      "Epoch 718/1000: L(Train): 0.2909359931945801; L(Test): 0.27103400230407715\n",
      "Epoch 719/1000: L(Train): 0.2906205356121063; L(Test): 0.2707056403160095\n",
      "Epoch 720/1000: L(Train): 0.281277060508728; L(Test): 0.27031928300857544\n",
      "Epoch 721/1000: L(Train): 0.2872789204120636; L(Test): 0.2698945105075836\n",
      "Epoch 722/1000: L(Train): 0.2994748055934906; L(Test): 0.27002885937690735\n",
      "Epoch 723/1000: L(Train): 0.2867003083229065; L(Test): 0.270463764667511\n",
      "Epoch 724/1000: L(Train): 0.2897821366786957; L(Test): 0.26999157667160034\n",
      "Epoch 725/1000: L(Train): 0.28098437190055847; L(Test): 0.2694634795188904\n",
      "Epoch 726/1000: L(Train): 0.29255929589271545; L(Test): 0.2694782614707947\n",
      "Epoch 727/1000: L(Train): 0.295825332403183; L(Test): 0.2695460319519043\n",
      "Epoch 728/1000: L(Train): 0.2773705720901489; L(Test): 0.2698307931423187\n",
      "Epoch 729/1000: L(Train): 0.27235621213912964; L(Test): 0.2697863280773163\n",
      "Epoch 730/1000: L(Train): 0.2746192514896393; L(Test): 0.26952123641967773\n",
      "Epoch 731/1000: L(Train): 0.2965618968009949; L(Test): 0.26920777559280396\n",
      "Epoch 732/1000: L(Train): 0.27402007579803467; L(Test): 0.27001383900642395\n",
      "Epoch 733/1000: L(Train): 0.27395543456077576; L(Test): 0.26955553889274597\n",
      "Epoch 734/1000: L(Train): 0.29869216680526733; L(Test): 0.2696862518787384\n",
      "Epoch 735/1000: L(Train): 0.29099205136299133; L(Test): 0.26938867568969727\n",
      "Epoch 736/1000: L(Train): 0.2842908799648285; L(Test): 0.2703676223754883\n",
      "Epoch 737/1000: L(Train): 0.28112924098968506; L(Test): 0.27227264642715454\n",
      "Epoch 738/1000: L(Train): 0.27307063341140747; L(Test): 0.2707885503768921\n",
      "Epoch 739/1000: L(Train): 0.28465649485588074; L(Test): 0.2715413272380829\n",
      "Epoch 740/1000: L(Train): 0.2926103472709656; L(Test): 0.2695111632347107\n",
      "Epoch 741/1000: L(Train): 0.2885272204875946; L(Test): 0.27219897508621216\n",
      "Epoch 742/1000: L(Train): 0.2864099144935608; L(Test): 0.2713584899902344\n",
      "Epoch 743/1000: L(Train): 0.28797852993011475; L(Test): 0.2697712481021881\n",
      "Epoch 744/1000: L(Train): 0.2821867763996124; L(Test): 0.27111631631851196\n",
      "Epoch 745/1000: L(Train): 0.28020066022872925; L(Test): 0.27306097745895386\n",
      "Epoch 746/1000: L(Train): 0.2832375764846802; L(Test): 0.26987022161483765\n",
      "Epoch 747/1000: L(Train): 0.2813536822795868; L(Test): 0.2708775997161865\n",
      "Epoch 748/1000: L(Train): 0.2929384112358093; L(Test): 0.2698483169078827\n",
      "Epoch 749/1000: L(Train): 0.2765481173992157; L(Test): 0.2724727988243103\n",
      "Epoch 750/1000: L(Train): 0.3061133921146393; L(Test): 0.269140362739563\n",
      "Epoch 751/1000: L(Train): 0.2855954170227051; L(Test): 0.2696516513824463\n",
      "Epoch 752/1000: L(Train): 0.2892707884311676; L(Test): 0.27100077271461487\n",
      "Epoch 753/1000: L(Train): 0.273264080286026; L(Test): 0.2712554931640625\n",
      "Epoch 754/1000: L(Train): 0.29511168599128723; L(Test): 0.2695636749267578\n",
      "Epoch 755/1000: L(Train): 0.29697659611701965; L(Test): 0.2688918709754944\n",
      "Epoch 756/1000: L(Train): 0.29320046305656433; L(Test): 0.26939335465431213\n",
      "Epoch 757/1000: L(Train): 0.2802465856075287; L(Test): 0.2707444131374359\n",
      "Epoch 758/1000: L(Train): 0.275054007768631; L(Test): 0.2698270082473755\n",
      "Epoch 759/1000: L(Train): 0.2767660617828369; L(Test): 0.26940134167671204\n",
      "Epoch 760/1000: L(Train): 0.2747470736503601; L(Test): 0.26949432492256165\n",
      "Epoch 761/1000: L(Train): 0.28770411014556885; L(Test): 0.26961129903793335\n",
      "Epoch 762/1000: L(Train): 0.2906031608581543; L(Test): 0.26875779032707214\n",
      "Epoch 763/1000: L(Train): 0.2640339732170105; L(Test): 0.268540620803833\n",
      "Epoch 764/1000: L(Train): 0.2815065383911133; L(Test): 0.26797163486480713\n",
      "Epoch 765/1000: L(Train): 0.28904345631599426; L(Test): 0.26918065547943115\n",
      "Epoch 766/1000: L(Train): 0.2807384431362152; L(Test): 0.26932334899902344\n",
      "Epoch 767/1000: L(Train): 0.2746952176094055; L(Test): 0.26794445514678955\n",
      "Epoch 768/1000: L(Train): 0.2671870291233063; L(Test): 0.26764780282974243\n",
      "Epoch 769/1000: L(Train): 0.2769772410392761; L(Test): 0.2681565284729004\n",
      "Epoch 770/1000: L(Train): 0.28721219301223755; L(Test): 0.2681400179862976\n",
      "Epoch 771/1000: L(Train): 0.29655882716178894; L(Test): 0.26800915598869324\n",
      "Epoch 772/1000: L(Train): 0.2864355146884918; L(Test): 0.26785972714424133\n",
      "Epoch 773/1000: L(Train): 0.2826620638370514; L(Test): 0.2683405876159668\n",
      "Epoch 774/1000: L(Train): 0.2894122004508972; L(Test): 0.26788845658302307\n",
      "Epoch 775/1000: L(Train): 0.2854371666908264; L(Test): 0.2679721415042877\n",
      "Epoch 776/1000: L(Train): 0.2695252597332001; L(Test): 0.26752254366874695\n",
      "Epoch 777/1000: L(Train): 0.2789578437805176; L(Test): 0.26811209321022034\n",
      "Epoch 778/1000: L(Train): 0.27804091572761536; L(Test): 0.2673981189727783\n",
      "Epoch 779/1000: L(Train): 0.2768017053604126; L(Test): 0.26747003197669983\n",
      "Epoch 780/1000: L(Train): 0.28655147552490234; L(Test): 0.26749929785728455\n",
      "Epoch 781/1000: L(Train): 0.2702244520187378; L(Test): 0.26900091767311096\n",
      "Epoch 782/1000: L(Train): 0.2791053354740143; L(Test): 0.2668653428554535\n",
      "Epoch 783/1000: L(Train): 0.27881675958633423; L(Test): 0.2671549916267395\n",
      "Epoch 784/1000: L(Train): 0.2846115827560425; L(Test): 0.266380250453949\n",
      "Epoch 785/1000: L(Train): 0.2738227844238281; L(Test): 0.2682211697101593\n",
      "Epoch 786/1000: L(Train): 0.2882399260997772; L(Test): 0.2665713131427765\n",
      "Epoch 787/1000: L(Train): 0.2826395034790039; L(Test): 0.2669296860694885\n",
      "Epoch 788/1000: L(Train): 0.2792458236217499; L(Test): 0.26607221364974976\n",
      "Epoch 789/1000: L(Train): 0.2817343473434448; L(Test): 0.26689040660858154\n",
      "Epoch 790/1000: L(Train): 0.27442803978919983; L(Test): 0.26816442608833313\n",
      "Epoch 791/1000: L(Train): 0.2937719225883484; L(Test): 0.2664905786514282\n",
      "Epoch 792/1000: L(Train): 0.2721477746963501; L(Test): 0.26666420698165894\n",
      "Epoch 793/1000: L(Train): 0.29233890771865845; L(Test): 0.26692885160446167\n",
      "Epoch 794/1000: L(Train): 0.2904013991355896; L(Test): 0.26621130108833313\n",
      "Epoch 795/1000: L(Train): 0.26945656538009644; L(Test): 0.2666308879852295\n",
      "Epoch 796/1000: L(Train): 0.28957808017730713; L(Test): 0.266361266374588\n",
      "Epoch 797/1000: L(Train): 0.28712278604507446; L(Test): 0.2659156620502472\n",
      "Epoch 798/1000: L(Train): 0.2957053482532501; L(Test): 0.2657867670059204\n",
      "Epoch 799/1000: L(Train): 0.2827054560184479; L(Test): 0.266254723072052\n",
      "Epoch 800/1000: L(Train): 0.2834497094154358; L(Test): 0.2663082480430603\n",
      "Epoch 801/1000: L(Train): 0.2952495217323303; L(Test): 0.26563888788223267\n",
      "Epoch 802/1000: L(Train): 0.29195523262023926; L(Test): 0.2663770020008087\n",
      "Epoch 803/1000: L(Train): 0.27601760625839233; L(Test): 0.2665873169898987\n",
      "Epoch 804/1000: L(Train): 0.270813912153244; L(Test): 0.266288161277771\n",
      "Epoch 805/1000: L(Train): 0.2833794355392456; L(Test): 0.2667914927005768\n",
      "Epoch 806/1000: L(Train): 0.2822132706642151; L(Test): 0.26687902212142944\n",
      "Epoch 807/1000: L(Train): 0.28216657042503357; L(Test): 0.26615601778030396\n",
      "Epoch 808/1000: L(Train): 0.28020742535591125; L(Test): 0.26570895314216614\n",
      "Epoch 809/1000: L(Train): 0.27702152729034424; L(Test): 0.2658739686012268\n",
      "Epoch 810/1000: L(Train): 0.28907594084739685; L(Test): 0.2659185528755188\n",
      "Epoch 811/1000: L(Train): 0.29077109694480896; L(Test): 0.2658068835735321\n",
      "Epoch 812/1000: L(Train): 0.28790420293807983; L(Test): 0.26583558320999146\n",
      "Epoch 813/1000: L(Train): 0.2730668783187866; L(Test): 0.2657574713230133\n",
      "Epoch 814/1000: L(Train): 0.28632593154907227; L(Test): 0.2657112777233124\n",
      "Epoch 815/1000: L(Train): 0.28090980648994446; L(Test): 0.2650830149650574\n",
      "Epoch 816/1000: L(Train): 0.28521525859832764; L(Test): 0.26485252380371094\n",
      "Epoch 817/1000: L(Train): 0.27616554498672485; L(Test): 0.26577723026275635\n",
      "Epoch 818/1000: L(Train): 0.27541449666023254; L(Test): 0.2662639617919922\n",
      "Epoch 819/1000: L(Train): 0.2737351655960083; L(Test): 0.26571977138519287\n",
      "Epoch 820/1000: L(Train): 0.2670857608318329; L(Test): 0.2657247483730316\n",
      "Epoch 821/1000: L(Train): 0.2858598232269287; L(Test): 0.26575371623039246\n",
      "Epoch 822/1000: L(Train): 0.28443193435668945; L(Test): 0.26616770029067993\n",
      "Epoch 823/1000: L(Train): 0.2703092694282532; L(Test): 0.26491692662239075\n",
      "Epoch 824/1000: L(Train): 0.2933284342288971; L(Test): 0.2655744254589081\n",
      "Epoch 825/1000: L(Train): 0.2916579246520996; L(Test): 0.2661948800086975\n",
      "Epoch 826/1000: L(Train): 0.28454986214637756; L(Test): 0.2653907835483551\n",
      "Epoch 827/1000: L(Train): 0.2811374068260193; L(Test): 0.2643914222717285\n",
      "Epoch 828/1000: L(Train): 0.29188036918640137; L(Test): 0.265058696269989\n",
      "Epoch 829/1000: L(Train): 0.28461703658103943; L(Test): 0.2668571174144745\n",
      "Epoch 830/1000: L(Train): 0.2803560495376587; L(Test): 0.26713037490844727\n",
      "Epoch 831/1000: L(Train): 0.29048484563827515; L(Test): 0.2651931643486023\n",
      "Epoch 832/1000: L(Train): 0.2782359719276428; L(Test): 0.26535898447036743\n",
      "Epoch 833/1000: L(Train): 0.2755589187145233; L(Test): 0.26732394099235535\n",
      "Epoch 834/1000: L(Train): 0.2674678564071655; L(Test): 0.2654564082622528\n",
      "Epoch 835/1000: L(Train): 0.2766653895378113; L(Test): 0.2656360864639282\n",
      "Epoch 836/1000: L(Train): 0.2852850556373596; L(Test): 0.26630204916000366\n",
      "Epoch 837/1000: L(Train): 0.29792484641075134; L(Test): 0.26555967330932617\n",
      "Epoch 838/1000: L(Train): 0.27528852224349976; L(Test): 0.2658289074897766\n",
      "Epoch 839/1000: L(Train): 0.28453052043914795; L(Test): 0.2663820683956146\n",
      "Epoch 840/1000: L(Train): 0.27562883496284485; L(Test): 0.26711490750312805\n",
      "Epoch 841/1000: L(Train): 0.28525352478027344; L(Test): 0.26591160893440247\n",
      "Epoch 842/1000: L(Train): 0.2797524034976959; L(Test): 0.26584234833717346\n",
      "Epoch 843/1000: L(Train): 0.28487154841423035; L(Test): 0.26555773615837097\n",
      "Epoch 844/1000: L(Train): 0.2752460241317749; L(Test): 0.2652590274810791\n",
      "Epoch 845/1000: L(Train): 0.2934757173061371; L(Test): 0.2655406892299652\n",
      "Epoch 846/1000: L(Train): 0.2946552038192749; L(Test): 0.2653764486312866\n",
      "Epoch 847/1000: L(Train): 0.27939093112945557; L(Test): 0.26572057604789734\n",
      "Epoch 848/1000: L(Train): 0.2880185544490814; L(Test): 0.26601219177246094\n",
      "Epoch 849/1000: L(Train): 0.2830047011375427; L(Test): 0.2670849859714508\n",
      "Epoch 850/1000: L(Train): 0.27850404381752014; L(Test): 0.2663494050502777\n",
      "Epoch 851/1000: L(Train): 0.2780531942844391; L(Test): 0.26527076959609985\n",
      "Epoch 852/1000: L(Train): 0.27292683720588684; L(Test): 0.2650127112865448\n",
      "Epoch 853/1000: L(Train): 0.29935765266418457; L(Test): 0.2644803822040558\n",
      "Epoch 854/1000: L(Train): 0.2700890600681305; L(Test): 0.26466548442840576\n",
      "Epoch 855/1000: L(Train): 0.2838122844696045; L(Test): 0.26429545879364014\n",
      "Epoch 856/1000: L(Train): 0.28530681133270264; L(Test): 0.2658552825450897\n",
      "Epoch 857/1000: L(Train): 0.2730841636657715; L(Test): 0.26564180850982666\n",
      "Epoch 858/1000: L(Train): 0.2725796401500702; L(Test): 0.26474347710609436\n",
      "Epoch 859/1000: L(Train): 0.2804521918296814; L(Test): 0.26527276635169983\n",
      "Epoch 860/1000: L(Train): 0.2706409990787506; L(Test): 0.2650943398475647\n",
      "Epoch 861/1000: L(Train): 0.2859835922718048; L(Test): 0.26497313380241394\n",
      "Epoch 862/1000: L(Train): 0.27365532517433167; L(Test): 0.26533281803131104\n",
      "Epoch 863/1000: L(Train): 0.2869802713394165; L(Test): 0.26407960057258606\n",
      "Epoch 864/1000: L(Train): 0.2859939634799957; L(Test): 0.2646791636943817\n",
      "Epoch 865/1000: L(Train): 0.29127922654151917; L(Test): 0.26545250415802\n",
      "Epoch 866/1000: L(Train): 0.2967035472393036; L(Test): 0.2659028470516205\n",
      "Epoch 867/1000: L(Train): 0.26751038432121277; L(Test): 0.26591384410858154\n",
      "Epoch 868/1000: L(Train): 0.27131688594818115; L(Test): 0.26405027508735657\n",
      "Epoch 869/1000: L(Train): 0.28516706824302673; L(Test): 0.26538583636283875\n",
      "Epoch 870/1000: L(Train): 0.2813279330730438; L(Test): 0.2655455768108368\n",
      "Epoch 871/1000: L(Train): 0.28525638580322266; L(Test): 0.26416561007499695\n",
      "Epoch 872/1000: L(Train): 0.2800655663013458; L(Test): 0.26446038484573364\n",
      "Epoch 873/1000: L(Train): 0.2786044180393219; L(Test): 0.26509734988212585\n",
      "Epoch 874/1000: L(Train): 0.2773662209510803; L(Test): 0.26647308468818665\n",
      "Epoch 875/1000: L(Train): 0.28839462995529175; L(Test): 0.26485952734947205\n",
      "Epoch 876/1000: L(Train): 0.2759507894515991; L(Test): 0.26337823271751404\n",
      "Epoch 877/1000: L(Train): 0.28998520970344543; L(Test): 0.26374295353889465\n",
      "Epoch 878/1000: L(Train): 0.2697470188140869; L(Test): 0.2645377516746521\n",
      "Epoch 879/1000: L(Train): 0.28427553176879883; L(Test): 0.26439622044563293\n",
      "Epoch 880/1000: L(Train): 0.27929753065109253; L(Test): 0.26418039202690125\n",
      "Epoch 881/1000: L(Train): 0.2926468253135681; L(Test): 0.2646098732948303\n",
      "Epoch 882/1000: L(Train): 0.2771557867527008; L(Test): 0.26448488235473633\n",
      "Epoch 883/1000: L(Train): 0.2684479057788849; L(Test): 0.2645625174045563\n",
      "Epoch 884/1000: L(Train): 0.2789144217967987; L(Test): 0.26421648263931274\n",
      "Epoch 885/1000: L(Train): 0.27459952235221863; L(Test): 0.26493021845817566\n",
      "Epoch 886/1000: L(Train): 0.28206518292427063; L(Test): 0.2646045684814453\n",
      "Epoch 887/1000: L(Train): 0.28323379158973694; L(Test): 0.2656363248825073\n",
      "Epoch 888/1000: L(Train): 0.28144213557243347; L(Test): 0.26515522599220276\n",
      "Epoch 889/1000: L(Train): 0.2936421036720276; L(Test): 0.26458096504211426\n",
      "Epoch 890/1000: L(Train): 0.28300178050994873; L(Test): 0.2640375792980194\n",
      "Epoch 891/1000: L(Train): 0.28383949398994446; L(Test): 0.2634580731391907\n",
      "Epoch 892/1000: L(Train): 0.2895650267601013; L(Test): 0.2645692825317383\n",
      "Epoch 893/1000: L(Train): 0.2757318615913391; L(Test): 0.26360952854156494\n",
      "Epoch 894/1000: L(Train): 0.27794313430786133; L(Test): 0.26354697346687317\n",
      "Epoch 895/1000: L(Train): 0.2649434208869934; L(Test): 0.26413223147392273\n",
      "Epoch 896/1000: L(Train): 0.2721002697944641; L(Test): 0.26350539922714233\n",
      "Epoch 897/1000: L(Train): 0.2974720001220703; L(Test): 0.2639769911766052\n",
      "Epoch 898/1000: L(Train): 0.28833234310150146; L(Test): 0.2637827694416046\n",
      "Epoch 899/1000: L(Train): 0.26343828439712524; L(Test): 0.26399534940719604\n",
      "Epoch 900/1000: L(Train): 0.28514859080314636; L(Test): 0.26307690143585205\n",
      "Epoch 901/1000: L(Train): 0.2846546173095703; L(Test): 0.2625078856945038\n",
      "Epoch 902/1000: L(Train): 0.27731308341026306; L(Test): 0.2637281119823456\n",
      "Epoch 903/1000: L(Train): 0.2749571204185486; L(Test): 0.26627880334854126\n",
      "Epoch 904/1000: L(Train): 0.29365599155426025; L(Test): 0.2632865607738495\n",
      "Epoch 905/1000: L(Train): 0.276299387216568; L(Test): 0.2631836533546448\n",
      "Epoch 906/1000: L(Train): 0.2764612138271332; L(Test): 0.26417025923728943\n",
      "Epoch 907/1000: L(Train): 0.28251397609710693; L(Test): 0.26383063197135925\n",
      "Epoch 908/1000: L(Train): 0.29354071617126465; L(Test): 0.2631632685661316\n",
      "Epoch 909/1000: L(Train): 0.2847045361995697; L(Test): 0.2648696303367615\n",
      "Epoch 910/1000: L(Train): 0.29829004406929016; L(Test): 0.2642636299133301\n",
      "Epoch 911/1000: L(Train): 0.2681815028190613; L(Test): 0.2651735544204712\n",
      "Epoch 912/1000: L(Train): 0.27739763259887695; L(Test): 0.2628884017467499\n",
      "Epoch 913/1000: L(Train): 0.2856798768043518; L(Test): 0.26292937994003296\n",
      "Epoch 914/1000: L(Train): 0.2690544128417969; L(Test): 0.2630595564842224\n",
      "Epoch 915/1000: L(Train): 0.2708456218242645; L(Test): 0.2660331428050995\n",
      "Epoch 916/1000: L(Train): 0.2726424038410187; L(Test): 0.2635362148284912\n",
      "Epoch 917/1000: L(Train): 0.3007056415081024; L(Test): 0.2640892267227173\n",
      "Epoch 918/1000: L(Train): 0.2832445204257965; L(Test): 0.2642430365085602\n",
      "Epoch 919/1000: L(Train): 0.30300214886665344; L(Test): 0.26503226161003113\n",
      "Epoch 920/1000: L(Train): 0.290751576423645; L(Test): 0.26279929280281067\n",
      "Epoch 921/1000: L(Train): 0.2833777070045471; L(Test): 0.263967365026474\n",
      "Epoch 922/1000: L(Train): 0.27567175030708313; L(Test): 0.26262369751930237\n",
      "Epoch 923/1000: L(Train): 0.29679059982299805; L(Test): 0.26477518677711487\n",
      "Epoch 924/1000: L(Train): 0.26601237058639526; L(Test): 0.26525670289993286\n",
      "Epoch 925/1000: L(Train): 0.2813323438167572; L(Test): 0.26266732811927795\n",
      "Epoch 926/1000: L(Train): 0.280854731798172; L(Test): 0.2629532814025879\n",
      "Epoch 927/1000: L(Train): 0.2797218859195709; L(Test): 0.26323840022087097\n",
      "Epoch 928/1000: L(Train): 0.28375864028930664; L(Test): 0.2629282772541046\n",
      "Epoch 929/1000: L(Train): 0.27814605832099915; L(Test): 0.2632952630519867\n",
      "Epoch 930/1000: L(Train): 0.27181631326675415; L(Test): 0.2631726861000061\n",
      "Epoch 931/1000: L(Train): 0.280292809009552; L(Test): 0.26269620656967163\n",
      "Epoch 932/1000: L(Train): 0.26537492871284485; L(Test): 0.26309889554977417\n",
      "Epoch 933/1000: L(Train): 0.2893785536289215; L(Test): 0.26274967193603516\n",
      "Epoch 934/1000: L(Train): 0.274991512298584; L(Test): 0.26198580861091614\n",
      "Epoch 935/1000: L(Train): 0.29162847995758057; L(Test): 0.26175642013549805\n",
      "Epoch 936/1000: L(Train): 0.2843831479549408; L(Test): 0.26221996545791626\n",
      "Epoch 937/1000: L(Train): 0.2732750475406647; L(Test): 0.2617674171924591\n",
      "Epoch 938/1000: L(Train): 0.2840660512447357; L(Test): 0.2620967626571655\n",
      "Epoch 939/1000: L(Train): 0.28343555331230164; L(Test): 0.26153501868247986\n",
      "Epoch 940/1000: L(Train): 0.27272218465805054; L(Test): 0.26187875866889954\n",
      "Epoch 941/1000: L(Train): 0.2710780203342438; L(Test): 0.26183366775512695\n",
      "Epoch 942/1000: L(Train): 0.2976997494697571; L(Test): 0.2615455090999603\n",
      "Epoch 943/1000: L(Train): 0.25193774700164795; L(Test): 0.2623006999492645\n",
      "Epoch 944/1000: L(Train): 0.281578004360199; L(Test): 0.2620634436607361\n",
      "Epoch 945/1000: L(Train): 0.2844076156616211; L(Test): 0.26172274351119995\n",
      "Epoch 946/1000: L(Train): 0.28660547733306885; L(Test): 0.26123255491256714\n",
      "Epoch 947/1000: L(Train): 0.2896079421043396; L(Test): 0.2621304392814636\n",
      "Epoch 948/1000: L(Train): 0.30060094594955444; L(Test): 0.26155197620391846\n",
      "Epoch 949/1000: L(Train): 0.27581655979156494; L(Test): 0.2617175281047821\n",
      "Epoch 950/1000: L(Train): 0.2748120129108429; L(Test): 0.26187604665756226\n",
      "Epoch 951/1000: L(Train): 0.28812694549560547; L(Test): 0.2616053521633148\n",
      "Epoch 952/1000: L(Train): 0.2795001268386841; L(Test): 0.26121222972869873\n",
      "Epoch 953/1000: L(Train): 0.28776830434799194; L(Test): 0.2623150050640106\n",
      "Epoch 954/1000: L(Train): 0.289825975894928; L(Test): 0.26139697432518005\n",
      "Epoch 955/1000: L(Train): 0.2786385715007782; L(Test): 0.26154625415802\n",
      "Epoch 956/1000: L(Train): 0.28797081112861633; L(Test): 0.2631737291812897\n",
      "Epoch 957/1000: L(Train): 0.2952237129211426; L(Test): 0.26304903626441956\n",
      "Epoch 958/1000: L(Train): 0.2741314172744751; L(Test): 0.2615305483341217\n",
      "Epoch 959/1000: L(Train): 0.27167677879333496; L(Test): 0.26127713918685913\n",
      "Epoch 960/1000: L(Train): 0.2711058557033539; L(Test): 0.2617703378200531\n",
      "Epoch 961/1000: L(Train): 0.25298696756362915; L(Test): 0.26395487785339355\n",
      "Epoch 962/1000: L(Train): 0.28525450825691223; L(Test): 0.26232826709747314\n",
      "Epoch 963/1000: L(Train): 0.2694150507450104; L(Test): 0.2629210650920868\n",
      "Epoch 964/1000: L(Train): 0.29084694385528564; L(Test): 0.2625677287578583\n",
      "Epoch 965/1000: L(Train): 0.2783147692680359; L(Test): 0.2653253376483917\n",
      "Epoch 966/1000: L(Train): 0.28425008058547974; L(Test): 0.26368704438209534\n",
      "Epoch 967/1000: L(Train): 0.27836117148399353; L(Test): 0.2622062563896179\n",
      "Epoch 968/1000: L(Train): 0.26736924052238464; L(Test): 0.26267415285110474\n",
      "Epoch 969/1000: L(Train): 0.2753939628601074; L(Test): 0.2628617584705353\n",
      "Epoch 970/1000: L(Train): 0.27121612429618835; L(Test): 0.2615725100040436\n",
      "Epoch 971/1000: L(Train): 0.27470919489860535; L(Test): 0.2610599398612976\n",
      "Epoch 972/1000: L(Train): 0.26689791679382324; L(Test): 0.2616332173347473\n",
      "Epoch 973/1000: L(Train): 0.2899901270866394; L(Test): 0.2627452611923218\n",
      "Epoch 974/1000: L(Train): 0.2736690640449524; L(Test): 0.26303741335868835\n",
      "Epoch 975/1000: L(Train): 0.2780437767505646; L(Test): 0.2608903646469116\n",
      "Epoch 976/1000: L(Train): 0.27223098278045654; L(Test): 0.2608715891838074\n",
      "Epoch 977/1000: L(Train): 0.28855302929878235; L(Test): 0.26059314608573914\n",
      "Epoch 978/1000: L(Train): 0.2865932285785675; L(Test): 0.261650025844574\n",
      "Epoch 979/1000: L(Train): 0.27796095609664917; L(Test): 0.2621273994445801\n",
      "Epoch 980/1000: L(Train): 0.28113552927970886; L(Test): 0.26077064871788025\n",
      "Epoch 981/1000: L(Train): 0.2920232117176056; L(Test): 0.2610611319541931\n",
      "Epoch 982/1000: L(Train): 0.27944445610046387; L(Test): 0.2605499029159546\n",
      "Epoch 983/1000: L(Train): 0.26795950531959534; L(Test): 0.26229578256607056\n",
      "Epoch 984/1000: L(Train): 0.29031461477279663; L(Test): 0.26171714067459106\n",
      "Epoch 985/1000: L(Train): 0.2687079906463623; L(Test): 0.26094135642051697\n",
      "Epoch 986/1000: L(Train): 0.2764914631843567; L(Test): 0.26040637493133545\n",
      "Epoch 987/1000: L(Train): 0.2879875600337982; L(Test): 0.2611682713031769\n",
      "Epoch 988/1000: L(Train): 0.2777663767337799; L(Test): 0.25945112109184265\n",
      "Epoch 989/1000: L(Train): 0.28095653653144836; L(Test): 0.2599622309207916\n",
      "Epoch 990/1000: L(Train): 0.25570860505104065; L(Test): 0.2611689865589142\n",
      "Epoch 991/1000: L(Train): 0.2792568802833557; L(Test): 0.2606995701789856\n",
      "Epoch 992/1000: L(Train): 0.2818770408630371; L(Test): 0.2600906789302826\n",
      "Epoch 993/1000: L(Train): 0.27693137526512146; L(Test): 0.2592404782772064\n",
      "Epoch 994/1000: L(Train): 0.2731391489505768; L(Test): 0.25934308767318726\n",
      "Epoch 995/1000: L(Train): 0.28077203035354614; L(Test): 0.2603188157081604\n",
      "Epoch 996/1000: L(Train): 0.27269312739372253; L(Test): 0.2600477933883667\n",
      "Epoch 997/1000: L(Train): 0.28741705417633057; L(Test): 0.25893452763557434\n",
      "Epoch 998/1000: L(Train): 0.2706265449523926; L(Test): 0.2586789131164551\n",
      "Epoch 999/1000: L(Train): 0.27126362919807434; L(Test): 0.25913992524147034\n",
      "Epoch 1000/1000: L(Train): 0.28891125321388245; L(Test): 0.26021236181259155\n",
      "Trained GRU parameters saved to ../../weinhardt2025/params/augustat2025/gru_augustat2025.pkl\n"
     ]
    }
   ],
   "source": [
    "gru = training(\n",
    "    gru=gru,\n",
    "    optimizer=optimizer,\n",
    "    dataset_train=dataset_train,\n",
    "    dataset_test=dataset_test,\n",
    "    epochs=epochs,\n",
    "    )\n",
    "\n",
    "torch.save(gru.state_dict(), path_gru)\n",
    "print(\"Trained GRU parameters saved to \" + path_gru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_agent = setup_agent_gru(path_gru, gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SPICE against benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE model for participant 0\n",
      "value_reward_chosen[t+1] = 0.165 1 + 0.854 value_reward_chosen[t] + 0.927 reward + -0.427 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.779 1 + 0.041 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "\n",
      "SPICE model for participant 1\n",
      "value_reward_chosen[t+1] = 1.0 value_reward_chosen[t] + 0.931 reward + -0.491 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.801 1 + 0.011 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n",
      "\n",
      "SPICE model for participant 2\n",
      "value_reward_chosen[t+1] = -0.146 1 + 0.841 value_reward_chosen[t] + 1.056 reward + -0.489 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.741 1 + 0.045 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n"
     ]
    }
   ],
   "source": [
    "print(\"SPICE model for participant 0\")\n",
    "estimator.print_spice_model(3)\n",
    "print(\"\\nSPICE model for participant 1\")\n",
    "estimator.print_spice_model(4)\n",
    "print(\"\\nSPICE model for participant 2\")\n",
    "estimator.print_spice_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_reward_chosen[t+1] = 0.9 value_reward_chosen[t] + 0.921 reward + -0.384 value_reward_chosen*reward \n",
      "value_reward_not_chosen[t+1] = -0.791 1 + 0.027 value_reward_not_chosen[t] \n",
      "value_reward_not_displayed[t+1] = 1.0 value_reward_not_displayed[t] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spice import plot_session\n",
    "\n",
    "# plotting\n",
    "participant_id = 0\n",
    "\n",
    "estimator.print_spice_model(participant_id)\n",
    "\n",
    "agents = {\n",
    "    # add baseline agent here\n",
    "    'rnn': estimator.rnn_agent,\n",
    "    'spice': estimator.spice_agent,\n",
    "    # 'baseline': baseline_agent,\n",
    "    'gru': gru_agent,\n",
    "}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[participant_id])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
