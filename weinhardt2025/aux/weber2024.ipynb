{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7jlYw4NA0v",
        "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/whyhardt/SPICE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXIbg826NS5i",
        "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
      },
      "outputs": [],
      "source": [
        "# !pip install -e SPICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f0uVlABYznR5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spice import SpiceEstimator, SpiceDataset, SpiceConfig, BaseRNN, convert_dataset, split_data_along_sessiondim, plot_session\n",
        "\n",
        "# For custom RNN\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the data first with the `convert_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: torch.Size([1692, 1078, 10])\n",
            "Number of participants: 30\n",
            "Number of experiments (baseline vs. infusion): 2\n",
            "Number of actions in dataset: 2\n",
            "Number of additional inputs: 3\n"
          ]
        }
      ],
      "source": [
        "# Load your data\n",
        "dataset = convert_dataset(\n",
        "    file = '../data/weber2024/weber2024.csv',\n",
        "    df_participant_id='participant',\n",
        "    df_experiment_id='experiment',\n",
        "    df_choice='choice',\n",
        "    df_reward='reward',\n",
        "    df_block='block',\n",
        "    additional_inputs=['laserRotation', 'shieldRotation', 'totalReward'],\n",
        "    timeshift_additional_inputs=True,\n",
        "    )\n",
        "\n",
        "test_sessions = 8, 10, 12\n",
        "\n",
        "# restructure data to have only two actions (stay, move) instead of three (stay, move_clockwise, move_counter_clockwise)\n",
        "move = dataset.xs[..., 1:3].sum(dim=-1, keepdim=True)\n",
        "rewards_move = dataset.xs[..., 4:6].nan_to_num(0).sum(dim=-1, keepdim=True)\n",
        "move_ys = dataset.ys[..., 1:3].sum(dim=-1, keepdim=True)\n",
        "# create restructured dataset\n",
        "xs = torch.concat((dataset.xs[..., :1], move, dataset.xs[..., 3:4], rewards_move, dataset.xs[..., 6:]), dim=-1)\n",
        "ys = torch.concat((dataset.ys[..., :1], move_ys), dim=-1)\n",
        "dataset = SpiceDataset(xs, ys)\n",
        "\n",
        "# structure of dataset:\n",
        "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
        "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
        "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
        "\n",
        "# in order to set up the participant embedding we have to compute the number of unique participants in our data\n",
        "# to get the number of participants n_participants we do:\n",
        "n_actions = dataset.ys.shape[-1]\n",
        "n_participants = len(dataset.xs[..., -1].unique())\n",
        "n_experiments = len(dataset.xs[..., -2].unique())\n",
        "\n",
        "# split data into training and testing data\n",
        "dataset_train, dataset_test = split_data_along_sessiondim(dataset, list_test_sessions=test_sessions)\n",
        "\n",
        "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
        "print(f\"Number of participants: {n_participants}\")\n",
        "print(f\"Number of experiments (baseline vs. infusion): {n_experiments}\")\n",
        "print(f\"Number of actions in dataset: {n_actions}\")\n",
        "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPICE Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
        "\n",
        "The `SpiceConfig` takes as arguments \n",
        "1. `library_setup (dict)`: Defining the variable names of each module.\n",
        "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
        "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spice_config = SpiceConfig(\n",
        "    library_setup={\n",
        "        'value_distance': [\n",
        "            'distance[t]',\n",
        "            'distance[t-1]',\n",
        "            'distance[t-2]',\n",
        "            'distance[t-3]',\n",
        "            ],\n",
        "        'value_laser_volatility': [  # rather noise in task language\n",
        "            'laser_volatility[t]',      # captures noise (stochasticity) and mean jumps (volatility) \n",
        "            'laser_volatility[t-1]',\n",
        "            'laser_volatility[t-2]',\n",
        "            'laser_volatility[t-3]',\n",
        "            ],\n",
        "        'value_reward': [\n",
        "            'reward[t]',\n",
        "            'reward[t-1]',\n",
        "            'reward[t-2]',\n",
        "            'reward[t-3]',\n",
        "        ],\n",
        "        'value_stay': [\n",
        "            'total_reward',\n",
        "            'reward',\n",
        "            'distance',\n",
        "            'laser_volatility',\n",
        "            'move_switch',\n",
        "        ],\n",
        "        'value_move': [\n",
        "            'total_reward',\n",
        "            'reward',\n",
        "            'distance',\n",
        "            'laser_volatility',\n",
        "            'move_switch',\n",
        "        ],\n",
        "        # 'value_{move/stay}': [\n",
        "        #     'reward',\n",
        "        #     'distance',\n",
        "        #     'volatility',\n",
        "        #     'move_switch',\n",
        "        # ],\n",
        "        # 'urgency': [\n",
        "        #     'totalReward',\n",
        "        #     ],\n",
        "    },\n",
        "    \n",
        "    memory_state=[\n",
        "            'value_action',\n",
        "            'value_distance',\n",
        "            'value_laser_volatility',\n",
        "            'value_reward',\n",
        "            'distance[t-1]',\n",
        "            'distance[t-2]',\n",
        "            'distance[t-3]',\n",
        "            # 'laser_rotation[t-1]',\n",
        "            # 'laser_rotation[t-2]',\n",
        "            # 'laser_rotation[t-3]',\n",
        "            'laser_volatility[t-1]',\n",
        "            'laser_volatility[t-2]',\n",
        "            'laser_volatility[t-3]',\n",
        "            'reward[t-1]',\n",
        "            'reward[t-2]',\n",
        "            'reward[t-3]',\n",
        "    ],\n",
        "    \n",
        "    states_in_logit=['value_action'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
        "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
        "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
        "3. `n_participants (int)`: number of participants in your dataset.\n",
        "\n",
        "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
        "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
        "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
        "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0kOR2Qgz0FZ"
      },
      "outputs": [],
      "source": [
        "class SPICERNN(BaseRNN):\n",
        "    \n",
        "    def __init__(self, spice_config, **kwargs):\n",
        "        super().__init__(spice_config=spice_config, **kwargs)\n",
        "        \n",
        "        dropout = 0.1\n",
        "        \n",
        "        # participant embedding\n",
        "        self.participant_embedding = self.setup_embedding(num_embeddings=self.n_participants, embedding_size=self.embedding_size, dropout=dropout)\n",
        "        \n",
        "        # set up the submodules\n",
        "        # the inputs to the modules will be: (actual_inputs, participant_embedding, infusion_flag) #+ experiment conditions (sthoch, volatility)\n",
        "        self.setup_module(key_module='value_distance', input_size=4+self.embedding_size+1, dropout=dropout)\n",
        "        self.setup_module(key_module='value_laser_volatility', input_size=4+self.embedding_size+1, dropout=dropout)  # could describe diff between stable and volatile state (mean jump) (e.g. attention weights)\n",
        "        self.setup_module(key_module='value_reward', input_size=4+self.embedding_size+1, dropout=dropout)\n",
        "        self.setup_module(key_module='value_stay', input_size=5+self.embedding_size+1)\n",
        "        self.setup_module(key_module='value_move', input_size=5+self.embedding_size+1)\n",
        "        \n",
        "    def forward(self, inputs, prev_state, batch_first=False):\n",
        "        \n",
        "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
        "        \n",
        "        # transform rewards from action dependent (action: (1, 0) -> reward: (0.3, nan)) \n",
        "        # to same for all actions (action: (1, 0) -> reward: (0.3, 0.3))\n",
        "        rewards = spice_signals.rewards.nan_to_num(0).sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions)\n",
        "        \n",
        "        # the additional inputs are as defined in the dataset cell: (laser_rotation, shield_rotation, total_reward)\n",
        "        laser_rotation = spice_signals.additional_inputs[..., 0][:, :, None].repeat(1, 1, self.n_actions)\n",
        "        shield_rotation = spice_signals.additional_inputs[..., 1][:, :, None].repeat(1, 1, self.n_actions)\n",
        "        total_reward = spice_signals.additional_inputs[..., 2][:, :, None].repeat(1, 1, self.n_actions)\n",
        "        \n",
        "        # compute additional features (could also be done in dataset cell)\n",
        "        distance = (laser_rotation - shield_rotation).abs()\n",
        "        # shield move direction switch (switch between stay, move clockwise, move counter clockwise)\n",
        "        move_switch = torch.zeros_like(distance)\n",
        "        move_switch[1:-1] = shield_rotation.diff(dim=0).sign().diff(dim=0).abs()\n",
        "        # laser volatility\n",
        "        laser_volatility = torch.zeros_like(distance)\n",
        "        laser_volatility[1:] = laser_rotation.diff(dim=0)\n",
        "        \n",
        "        # time-invariant participant features\n",
        "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
        "        experiment_embeddings = spice_signals.experiment_ids.reshape(-1, 1)\n",
        "        \n",
        "        # masks for action selection\n",
        "        mask_action_stay = torch.zeros(spice_signals.actions.shape[1:])\n",
        "        mask_action_stay[:, 0] = 1\n",
        "        mask_action_move = torch.zeros(spice_signals.actions.shape[1:])\n",
        "        mask_action_move[:, 1] = 1\n",
        "        \n",
        "        for timestep in spice_signals.timesteps:\n",
        "            \n",
        "            self.call_module(\n",
        "                key_module='value_distance',\n",
        "                key_state='value_distance',\n",
        "                inputs=(\n",
        "                    distance[timestep],\n",
        "                    self.state['distance[t-1]'],\n",
        "                    self.state['distance[t-2]'],\n",
        "                    self.state['distance[t-3]'],\n",
        "                ),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "                experiment_index=spice_signals.experiment_ids,\n",
        "                experiment_embedding=experiment_embeddings,\n",
        "            )\n",
        "            \n",
        "            self.call_module(\n",
        "                key_module='value_laser_volatility',\n",
        "                key_state='value_laser_volatility',\n",
        "                inputs=(\n",
        "                    laser_volatility[timestep],\n",
        "                    self.state['laser_volatility[t-1]'],\n",
        "                    self.state['laser_volatility[t-2]'],\n",
        "                    self.state['laser_volatility[t-3]'],\n",
        "                ),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "                experiment_index=spice_signals.experiment_ids,\n",
        "                experiment_embedding=experiment_embeddings,\n",
        "            )\n",
        "            \n",
        "            self.call_module(\n",
        "                key_module='value_reward',\n",
        "                key_state='value_reward',\n",
        "                inputs=(\n",
        "                    rewards[timestep],\n",
        "                    self.state['reward[t-1]'],\n",
        "                    self.state['reward[t-2]'],\n",
        "                    self.state['reward[t-3]'],\n",
        "                ),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "                experiment_index=spice_signals.experiment_ids,\n",
        "                experiment_embedding=experiment_embeddings,\n",
        "            )\n",
        "            \n",
        "            self.call_module(\n",
        "                key_module='value_stay',\n",
        "                key_state='value_action',\n",
        "                action_mask=mask_action_stay,\n",
        "                inputs=(\n",
        "                    total_reward[timestep],\n",
        "                    self.state['value_reward'],\n",
        "                    self.state['value_distance'],\n",
        "                    self.state['value_laser_volatility'],\n",
        "                    move_switch[timestep],\n",
        "                ),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "                experiment_index=spice_signals.experiment_ids,\n",
        "                experiment_embedding=experiment_embeddings,\n",
        "            )\n",
        "            \n",
        "            self.call_module(\n",
        "                key_module='value_move',\n",
        "                key_state='value_action',\n",
        "                action_mask=mask_action_move,\n",
        "                inputs=(\n",
        "                    total_reward[timestep],\n",
        "                    self.state['value_reward'],\n",
        "                    self.state['value_distance'],\n",
        "                    self.state['value_laser_volatility'],\n",
        "                    move_switch[timestep],\n",
        "                ),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "                experiment_index=spice_signals.experiment_ids,\n",
        "                experiment_embedding=experiment_embeddings,\n",
        "            )\n",
        "            \n",
        "            # save action values as logits\n",
        "            spice_signals.logits[timestep] = self.state['value_action']\n",
        "            \n",
        "            # update working memory buffer\n",
        "            self.state['distance[t-1]'], self.state['distance[t-2]'], self.state['distance[t-3]'] = distance[timestep], self.state['distance[t-1]'], self.state['distance[t-2]']\n",
        "            self.state['reward[t-1]'], self.state['reward[t-2]'], self.state['reward[t-3]'] = rewards[timestep], self.state['reward[t-1]'], self.state['reward[t-2]']\n",
        "            self.state['laser_volatility[t-1]'], self.state['laser_volatility[t-2]'], self.state['laser_volatility[t-3]'] = laser_volatility[timestep], self.state['laser_volatility[t-1]'], self.state['laser_volatility[t-2]']\n",
        "            \n",
        "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
        "        \n",
        "        return spice_signals.logits, self.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup now the `SpiceEstimator` object and fit it to the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_spice = '../params/weber2024/spice_weber2024.pkl'\n",
        "\n",
        "estimator = SpiceEstimator(\n",
        "        # model paramaeters\n",
        "        rnn_class=SPICERNN,\n",
        "        spice_config=spice_config,\n",
        "        n_actions=n_actions,\n",
        "        n_participants=n_participants,\n",
        "        n_experiments=n_experiments,\n",
        "        \n",
        "        # rnn training parameters\n",
        "        epochs=2,\n",
        "        warmup_steps=200,\n",
        "        learning_rate=0.01,\n",
        "        \n",
        "        # sindy fitting parameters\n",
        "        sindy_weight=0.1,\n",
        "        sindy_pruning_threshold=0.05,\n",
        "        sindy_pruning_frequency=1,\n",
        "        sindy_pruning_terms=1,\n",
        "        sindy_pruning_patience=100,\n",
        "        sindy_epochs=1000,\n",
        "        sindy_l2_lambda=0.0001,\n",
        "        sindy_library_polynomial_degree=2,\n",
        "        sindy_ensemble_size=1,\n",
        "        \n",
        "        # additional generalization parameters\n",
        "        batch_size=1024,\n",
        "        bagging=True,\n",
        "        scheduler=True,\n",
        "        \n",
        "        verbose=True,\n",
        "        save_path_spice=path_spice,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3EnmDiUMWq6e",
        "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training on cpu...\n",
            "================================================================================\n",
            "\n",
            "Training the RNN...\n",
            "================================================================================\n",
            "Epoch 1/2 --- L(Train): 6.7472973 --- L(Val, RNN): 2.5669329 --- L(Val, SINDy): 7.9584112 --- Time: 43.52s; --- Convergence: 1.28e+00; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 119):\n",
            "value_distance[t+1] = 0.0 1 + 1.001 value_distance[t] + 0.0 distance[t] + 0.001 distance[t-1] + 0.001 distance[t-2] + 0.001 distance[t-3] + -0.0 value_distance^2 + 0.0 value_distance*distance[t] + 0.001 value_distance*distance[t-1] + 0.001 value_distance*distance[t-2] + -0.0 value_distance*distance[t-3] + 0.001 distance[t]^2 + 0.001 distance[t]*distance[t-1] + 0.001 distance[t]*distance[t-2] + 0.0 distance[t]*distance[t-3] + -0.0 distance[t-1]^2 + -0.002 distance[t-1]*distance[t-2] + 0.0 distance[t-1]*distance[t-3] + -0.001 distance[t-2]^2 + -0.002 distance[t-2]*distance[t-3] + -0.003 distance[t-3]^2 \n",
            "value_laser_volatility[t+1] = -0.009 1 + 1.01 value_laser_volatility[t] + 0.01 laser_volatility[t] + 0.009 laser_volatility[t-1] + -0.01 laser_volatility[t-2] + -0.009 laser_volatility[t-3] + -0.001 value_laser_volatility^2 + 0.011 value_laser_volatility*laser_volatility[t] + -0.01 value_laser_volatility*laser_volatility[t-1] + 0.011 value_laser_volatility*laser_volatility[t-2] + -0.011 value_laser_volatility*laser_volatility[t-3] + 0.01 laser_volatility[t]^2 + -0.011 laser_volatility[t]*laser_volatility[t-1] + -0.009 laser_volatility[t]*laser_volatility[t-2] + -0.008 laser_volatility[t]*laser_volatility[t-3] + -0.008 laser_volatility[t-1]^2 + 0.01 laser_volatility[t-1]*laser_volatility[t-2] + 0.01 laser_volatility[t-1]*laser_volatility[t-3] + -0.01 laser_volatility[t-2]^2 + 0.011 laser_volatility[t-2]*laser_volatility[t-3] + 0.009 laser_volatility[t-3]^2 \n",
            "value_reward[t+1] = 0.001 1 + 0.989 value_reward[t] + -0.0 reward[t] + 0.0 reward[t-1] + 0.001 reward[t-2] + 0.001 reward[t-3] + 0.0 value_reward^2 + -0.005 value_reward*reward[t] + -0.003 value_reward*reward[t-1] + -0.006 value_reward*reward[t-2] + -0.004 value_reward*reward[t-3] + -0.002 reward[t]^2 + -0.001 reward[t]*reward[t-1] + -0.0 reward[t]*reward[t-2] + 0.001 reward[t]*reward[t-3] + 0.001 reward[t-1]^2 + 0.0 reward[t-1]*reward[t-2] + -0.001 reward[t-1]*reward[t-3] + 0.001 reward[t-2]^2 + -0.001 reward[t-2]*reward[t-3] + -0.001 reward[t-3]^2 \n",
            "value_stay[t+1] = -0.005 1 + 0.997 value_stay[t] + -0.004 total_reward + -0.002 reward + 0.005 distance + 0.001 laser_volatility + -0.008 move_switch + -0.001 value_stay^2 + -0.001 value_stay*total_reward + -0.001 value_stay*reward + 0.002 value_stay*distance + 0.002 value_stay*laser_volatility + -0.004 value_stay*move_switch + -0.004 total_reward^2 + -0.002 total_reward*reward + 0.003 total_reward*distance + 0.001 total_reward*laser_volatility + -0.007 total_reward*move_switch + -0.001 reward^2 + 0.001 reward*distance + 0.0 reward*laser_volatility + -0.001 reward*move_switch + -0.001 distance^2 + 0.001 distance*laser_volatility + -0.002 distance*move_switch + -0.0 laser_volatility^2 + 0.001 laser_volatility*move_switch + -0.007 move_switch^2 \n",
            "value_move[t+1] = 0.01 1 + 1.011 value_move[t] + 0.011 total_reward + 0.009 reward + 0.008 distance + 0.009 laser_volatility + 0.01 move_switch + 0.009 value_move^2 + 0.01 value_move*total_reward + 0.011 value_move*reward + 0.009 value_move*distance + 0.009 value_move*laser_volatility + 0.009 value_move*move_switch + 0.009 total_reward^2 + 0.009 total_reward*reward + 0.011 total_reward*distance + 0.011 total_reward*laser_volatility + 0.01 total_reward*move_switch + 0.01 reward^2 + 0.008 reward*distance + 0.004 reward*laser_volatility + 0.007 reward*move_switch + 0.01 distance^2 + -0.007 distance*laser_volatility + 0.01 distance*move_switch + 0.01 laser_volatility^2 + 0.009 laser_volatility*move_switch + 0.01 move_switch^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_distance: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_laser_volatility: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_stay: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_move: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 2/2 --- L(Train): 8.1157093 --- L(Val, RNN): 1.2638351 --- L(Val, SINDy): 2.9616699 --- Time: 47.05s; --- Convergence: 1.29e+00; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 119):\n",
            "value_distance[t+1] = 0.0 1 + 1.0 value_distance[t] + 0.0 distance[t] + 0.001 distance[t-1] + 0.0 distance[t-2] + 0.001 distance[t-3] + -0.0 value_distance^2 + 0.0 value_distance*distance[t] + 0.001 value_distance*distance[t-1] + 0.001 value_distance*distance[t-2] + -0.0 value_distance*distance[t-3] + 0.001 distance[t]^2 + 0.001 distance[t]*distance[t-1] + 0.001 distance[t]*distance[t-2] + -0.0 distance[t]*distance[t-3] + -0.0 distance[t-1]^2 + -0.002 distance[t-1]*distance[t-2] + 0.0 distance[t-1]*distance[t-3] + -0.001 distance[t-2]^2 + -0.002 distance[t-2]*distance[t-3] + -0.003 distance[t-3]^2 \n",
            "value_laser_volatility[t+1] = -0.017 1 + 1.004 value_laser_volatility[t] + 0.007 laser_volatility[t] + 0.009 laser_volatility[t-1] + -0.02 laser_volatility[t-2] + -0.004 laser_volatility[t-3] + -0.007 value_laser_volatility^2 + 0.003 value_laser_volatility*laser_volatility[t] + -0.02 value_laser_volatility*laser_volatility[t-1] + 0.012 value_laser_volatility*laser_volatility[t-2] + -0.005 value_laser_volatility*laser_volatility[t-3] + 0.007 laser_volatility[t]^2 + -0.009 laser_volatility[t]*laser_volatility[t-1] + -0.002 laser_volatility[t]*laser_volatility[t-2] + -0.002 laser_volatility[t]*laser_volatility[t-3] + -0.012 laser_volatility[t-1]^2 + 0.003 laser_volatility[t-1]*laser_volatility[t-2] + 0.004 laser_volatility[t-1]*laser_volatility[t-3] + -0.011 laser_volatility[t-2]^2 + 0.004 laser_volatility[t-2]*laser_volatility[t-3] + 0.009 laser_volatility[t-3]^2 \n",
            "value_reward[t+1] = -0.005 1 + 0.98 value_reward[t] + -0.004 reward[t] + -0.003 reward[t-1] + -0.002 reward[t-2] + -0.002 reward[t-3] + 0.007 value_reward^2 + -0.011 value_reward*reward[t] + -0.009 value_reward*reward[t-1] + -0.011 value_reward*reward[t-2] + -0.01 value_reward*reward[t-3] + -0.002 reward[t]^2 + -0.001 reward[t]*reward[t-1] + -0.001 reward[t]*reward[t-2] + 0.0 reward[t]*reward[t-3] + 0.0 reward[t-1]^2 + -0.0 reward[t-1]*reward[t-2] + -0.001 reward[t-1]*reward[t-3] + 0.0 reward[t-2]^2 + -0.001 reward[t-2]*reward[t-3] + -0.001 reward[t-3]^2 \n",
            "value_stay[t+1] = 0.002 1 + 1.004 value_stay[t] + 0.003 total_reward + -0.009 reward + -0.0 distance + -0.004 laser_volatility + -0.003 move_switch + 0.005 value_stay^2 + 0.005 value_stay*total_reward + -0.007 value_stay*reward + -0.003 value_stay*distance + -0.001 value_stay*laser_volatility + 0.001 value_stay*move_switch + 0.003 total_reward^2 + -0.009 total_reward*reward + -0.003 total_reward*distance + -0.004 total_reward*laser_volatility + -0.002 total_reward*move_switch + 0.004 reward^2 + 0.005 reward*distance + 0.003 reward*laser_volatility + -0.007 reward*move_switch + 0.004 distance^2 + 0.003 distance*laser_volatility + -0.007 distance*move_switch + 0.005 laser_volatility^2 + -0.002 laser_volatility*move_switch + -0.002 move_switch^2 \n",
            "value_move[t+1] = 0.019 1 + 1.02 value_move[t] + 0.021 total_reward + 0.002 reward + 0.002 distance + 0.002 laser_volatility + 0.019 move_switch + 0.018 value_move^2 + 0.019 value_move*total_reward + 0.004 value_move*reward + 0.004 value_move*distance + 0.001 value_move*laser_volatility + 0.018 value_move*move_switch + 0.019 total_reward^2 + 0.001 total_reward*reward + 0.006 total_reward*distance + 0.004 total_reward*laser_volatility + 0.019 total_reward*move_switch + 0.018 reward^2 + 0.016 reward*distance + 0.011 reward*laser_volatility + -0.001 reward*move_switch + 0.019 distance^2 + -0.0 distance*laser_volatility + 0.005 distance*move_switch + 0.019 laser_volatility^2 + 0.002 laser_volatility*move_switch + 0.019 move_switch^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_distance: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_laser_volatility: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_stay: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_move: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "Maximum number of training epochs reached.\n",
            "Model did not converge yet.\n",
            "\n",
            "================================================================================\n",
            "Starting second stage SINDy fitting (threshold=0.05, single model)\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.0085669 --- L(Val, SINDy): 0.0000000 --- Time: 5.47s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 119):\n",
            "value_distance[t+1] = -0.008 1 + 1.01 value_distance[t] + 0.01 distance[t] + 0.01 distance[t-1] + 0.009 distance[t-2] + 0.009 distance[t-3] + 0.009 value_distance^2 + -0.01 value_distance*distance[t] + 0.009 value_distance*distance[t-1] + 0.01 value_distance*distance[t-2] + 0.01 value_distance*distance[t-3] + 0.01 distance[t]^2 + 0.011 distance[t]*distance[t-1] + 0.009 distance[t]*distance[t-2] + 0.01 distance[t]*distance[t-3] + 0.009 distance[t-1]^2 + 0.01 distance[t-1]*distance[t-2] + 0.01 distance[t-1]*distance[t-3] + 0.008 distance[t-2]^2 + 0.011 distance[t-2]*distance[t-3] + 0.01 distance[t-3]^2 \n",
            "value_laser_volatility[t+1] = -0.01 1 + 0.99 value_laser_volatility[t] + -0.009 laser_volatility[t] + 0.01 laser_volatility[t-1] + -0.009 laser_volatility[t-2] + -0.009 laser_volatility[t-3] + -0.008 value_laser_volatility^2 + -0.009 value_laser_volatility*laser_volatility[t] + -0.009 value_laser_volatility*laser_volatility[t-1] + 0.008 value_laser_volatility*laser_volatility[t-2] + -0.01 value_laser_volatility*laser_volatility[t-3] + -0.01 laser_volatility[t]^2 + 0.009 laser_volatility[t]*laser_volatility[t-1] + 0.009 laser_volatility[t]*laser_volatility[t-2] + 0.01 laser_volatility[t]*laser_volatility[t-3] + 0.009 laser_volatility[t-1]^2 + -0.007 laser_volatility[t-1]*laser_volatility[t-2] + 0.01 laser_volatility[t-1]*laser_volatility[t-3] + 0.009 laser_volatility[t-2]^2 + -0.009 laser_volatility[t-2]*laser_volatility[t-3] + 0.01 laser_volatility[t-3]^2 \n",
            "value_reward[t+1] = 0.01 1 + 1.009 value_reward[t] + 0.01 reward[t] + -0.01 reward[t-1] + -0.01 reward[t-2] + 0.009 reward[t-3] + -0.009 value_reward^2 + 0.009 value_reward*reward[t] + 0.01 value_reward*reward[t-1] + 0.009 value_reward*reward[t-2] + -0.009 value_reward*reward[t-3] + 0.01 reward[t]^2 + -0.01 reward[t]*reward[t-1] + -0.01 reward[t]*reward[t-2] + 0.009 reward[t]*reward[t-3] + 0.01 reward[t-1]^2 + -0.01 reward[t-1]*reward[t-2] + 0.009 reward[t-1]*reward[t-3] + -0.009 reward[t-2]^2 + -0.01 reward[t-2]*reward[t-3] + -0.009 reward[t-3]^2 \n",
            "value_stay[t+1] = 0.009 1 + 1.011 value_stay[t] + 0.011 total_reward + -0.01 reward + -0.01 distance + 0.01 laser_volatility + 0.009 move_switch + 0.008 value_stay^2 + 0.01 value_stay*total_reward + -0.011 value_stay*reward + -0.012 value_stay*distance + 0.01 value_stay*laser_volatility + 0.008 value_stay*move_switch + 0.01 total_reward^2 + -0.009 total_reward*reward + -0.009 total_reward*distance + 0.01 total_reward*laser_volatility + 0.009 total_reward*move_switch + 0.01 reward^2 + 0.013 reward*distance + -0.009 reward*laser_volatility + 0.009 reward*move_switch + 0.009 distance^2 + -0.01 distance*laser_volatility + -0.008 distance*move_switch + -0.009 laser_volatility^2 + -0.009 laser_volatility*move_switch + -0.01 move_switch^2 \n",
            "value_move[t+1] = -0.011 1 + 1.011 value_move[t] + -0.011 total_reward + 0.008 reward + 0.009 distance + -0.01 laser_volatility + 0.008 move_switch + -0.01 value_move^2 + 0.011 value_move*total_reward + -0.01 value_move*reward + -0.011 value_move*distance + 0.009 value_move*laser_volatility + 0.009 value_move*move_switch + -0.01 total_reward^2 + 0.01 total_reward*reward + 0.009 total_reward*distance + -0.011 total_reward*laser_volatility + 0.01 total_reward*move_switch + -0.011 reward^2 + -0.007 reward*distance + 0.009 reward*laser_volatility + 0.01 reward*move_switch + -0.008 distance^2 + 0.009 distance*laser_volatility + 0.009 distance*move_switch + -0.009 laser_volatility^2 + 0.009 laser_volatility*move_switch + 0.01 move_switch^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_distance: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_laser_volatility: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_stay: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_move: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.5200449 --- L(Val, SINDy): 0.0000000 --- Time: 5.36s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 119):\n",
            "value_distance[t+1] = -0.017 1 + 1.013 value_distance[t] + 0.003 distance[t] + 0.003 distance[t-1] + 0.003 distance[t-2] + 0.003 distance[t-3] + 0.008 value_distance^2 + -0.005 value_distance*distance[t] + 0.019 value_distance*distance[t-1] + 0.019 value_distance*distance[t-2] + 0.019 value_distance*distance[t-3] + 0.009 distance[t]^2 + 0.01 distance[t]*distance[t-1] + 0.004 distance[t]*distance[t-2] + 0.008 distance[t]*distance[t-3] + 0.009 distance[t-1]^2 + 0.006 distance[t-1]*distance[t-2] + 0.01 distance[t-1]*distance[t-3] + 0.004 distance[t-2]^2 + 0.006 distance[t-2]*distance[t-3] + 0.011 distance[t-3]^2 \n",
            "value_laser_volatility[t+1] = -0.002 1 + 0.997 value_laser_volatility[t] + -0.009 laser_volatility[t] + 0.009 laser_volatility[t-1] + -0.008 laser_volatility[t-2] + -0.009 laser_volatility[t-3] + -0.005 value_laser_volatility^2 + -0.009 value_laser_volatility*laser_volatility[t] + -0.009 value_laser_volatility*laser_volatility[t-1] + 0.007 value_laser_volatility*laser_volatility[t-2] + -0.009 value_laser_volatility*laser_volatility[t-3] + -0.009 laser_volatility[t]^2 + 0.008 laser_volatility[t]*laser_volatility[t-1] + 0.009 laser_volatility[t]*laser_volatility[t-2] + 0.009 laser_volatility[t]*laser_volatility[t-3] + 0.009 laser_volatility[t-1]^2 + -0.007 laser_volatility[t-1]*laser_volatility[t-2] + 0.009 laser_volatility[t-1]*laser_volatility[t-3] + 0.008 laser_volatility[t-2]^2 + -0.009 laser_volatility[t-2]*laser_volatility[t-3] + 0.009 laser_volatility[t-3]^2 \n",
            "value_reward[t+1] = 0.003 1 + 1.016 value_reward[t] + 0.004 reward[t] + -0.019 reward[t-1] + -0.019 reward[t-2] + 0.003 reward[t-3] + -0.018 value_reward^2 + 0.016 value_reward*reward[t] + 0.016 value_reward*reward[t-1] + 0.015 value_reward*reward[t-2] + -0.006 value_reward*reward[t-3] + 0.009 reward[t]^2 + -0.01 reward[t]*reward[t-1] + -0.01 reward[t]*reward[t-2] + 0.008 reward[t]*reward[t-3] + 0.009 reward[t-1]^2 + -0.01 reward[t-1]*reward[t-2] + 0.008 reward[t-1]*reward[t-3] + -0.01 reward[t-2]^2 + -0.01 reward[t-2]*reward[t-3] + -0.01 reward[t-3]^2 \n",
            "value_stay[t+1] = 0.002 1 + 1.004 value_stay[t] + 0.004 total_reward + -0.003 reward + -0.018 distance + 0.003 laser_volatility + 0.009 move_switch + 0.001 value_stay^2 + 0.003 value_stay*total_reward + -0.004 value_stay*reward + -0.02 value_stay*distance + 0.003 value_stay*laser_volatility + 0.008 value_stay*move_switch + 0.003 total_reward^2 + -0.002 total_reward*reward + -0.016 total_reward*distance + 0.003 total_reward*laser_volatility + 0.008 total_reward*move_switch + 0.003 reward^2 + 0.02 reward*distance + -0.003 reward*laser_volatility + 0.009 reward*move_switch + 0.002 distance^2 + -0.018 distance*laser_volatility + -0.008 distance*move_switch + -0.017 laser_volatility^2 + -0.009 laser_volatility*move_switch + -0.009 move_switch^2 \n",
            "value_move[t+1] = -0.005 1 + 1.005 value_move[t] + -0.005 total_reward + 0.002 reward + 0.018 distance + -0.004 laser_volatility + 0.008 move_switch + -0.004 value_move^2 + 0.005 value_move*total_reward + -0.004 value_move*reward + -0.02 value_move*distance + 0.003 value_move*laser_volatility + 0.009 value_move*move_switch + -0.004 total_reward^2 + 0.004 total_reward*reward + 0.018 total_reward*distance + -0.004 total_reward*laser_volatility + 0.009 total_reward*move_switch + -0.005 reward^2 + -0.016 reward*distance + 0.004 reward*laser_volatility + 0.009 reward*move_switch + -0.002 distance^2 + 0.018 distance*laser_volatility + 0.008 distance*move_switch + -0.004 laser_volatility^2 + 0.009 laser_volatility*move_switch + 0.009 move_switch^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_distance: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_laser_volatility: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_stay: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_move: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# estimator.load_spice(args.model)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/spice/estimator.py:204\u001b[39m, in \u001b[36mSpiceEstimator.fit\u001b[39m\u001b[34m(self, data, targets, data_test, target_test)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining the RNN...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    202\u001b[39m batch_size = data.shape[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m rnn_model, rnn_optimizer, _ = \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_threshold_frequency\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_threshold_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_threshold_terms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_threshold_terms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_threshold_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_cutoff_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_warmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbagging\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbagging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps_per_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_log\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_save_checkpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;28mself\u001b[39m.rnn_model = rnn_model\n\u001b[32m    229\u001b[39m \u001b[38;5;28mself\u001b[39m.rnn_optimizer = rnn_optimizer\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/spice/resources/spice_training.py:732\u001b[39m, in \u001b[36mfit_model\u001b[39m\u001b[34m(model, dataset_train, dataset_test, optimizer, convergence_threshold, sindy_weight, sindy_alpha, sindy_epochs, sindy_threshold, sindy_threshold_frequency, sindy_threshold_terms, sindy_threshold_patience, epochs, batch_size, bagging, scheduler, n_steps, verbose, keep_log, n_warmup_steps, path_save_checkpoints)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;66;03m# Second stage: Refit SINDy coefficients on trained RNN hidden states (always run when sindy_weight > 0)\u001b[39;00m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sindy_weight > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sindy_epochs > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     model = \u001b[43mfit_sindy_second_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msindy_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcutoff_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43msindy_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcutoff_n_terms\u001b[49m\u001b[43m=\u001b[49m\u001b[43msindy_threshold_terms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcutoff_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43msindy_threshold_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcutoff_warmup\u001b[49m\u001b[43m=\u001b[49m\u001b[43msindy_epochs\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43msindy_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    748\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mL(Train): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/spice/resources/spice_training.py:458\u001b[39m, in \u001b[36mfit_sindy_second_stage\u001b[39m\u001b[34m(model, dataset_train, dataset_test, learning_rate, epochs, cutoff_threshold, cutoff_frequency, cutoff_n_terms, cutoff_patience, cutoff_warmup, sindy_alpha, batch_size, verbose)\u001b[39m\n\u001b[32m    455\u001b[39m     batched_target_state_buffer[state] = target_state_buffer_train[state][idx:idx+batch_size][batched_nan_mask]\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# get sindy-based state updates from original rnn states\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m _, state_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[43m+\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatched_nan_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched_input_state_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m loss_batch = \u001b[32m0\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m model.spice_config.states_in_logit:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mSPICERNN.forward\u001b[39m\u001b[34m(self, inputs, prev_state, batch_first)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.call_module(\n\u001b[32m     84\u001b[39m     key_module=\u001b[33m'\u001b[39m\u001b[33mvalue_reward\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     85\u001b[39m     key_state=\u001b[33m'\u001b[39m\u001b[33mvalue_reward\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m     experiment_embedding=experiment_embeddings,\n\u001b[32m     96\u001b[39m )\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.call_module(\n\u001b[32m     99\u001b[39m     key_module=\u001b[33m'\u001b[39m\u001b[33mvalue_stay\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    100\u001b[39m     key_state=\u001b[33m'\u001b[39m\u001b[33mvalue_action\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m     experiment_embedding=experiment_embeddings,\n\u001b[32m    113\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_module\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue_move\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_state\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue_action\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_action_move\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_reward\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue_reward\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue_distance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue_laser_volatility\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmove_switch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspice_signals\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparticipant_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparticipant_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspice_signals\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperiment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# save action values as logits\u001b[39;00m\n\u001b[32m    133\u001b[39m spice_signals.logits[timestep] = \u001b[38;5;28mself\u001b[39m.state[\u001b[33m'\u001b[39m\u001b[33mvalue_action\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/spice/resources/rnn.py:317\u001b[39m, in \u001b[36mBaseRNN.call_module\u001b[39m\u001b[34m(self, key_module, key_state, action_mask, inputs, participant_embedding, participant_index, experiment_embedding, experiment_index, activation_rnn)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# use sindy coefficients\u001b[39;00m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m participant_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         next_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_sindy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m            \u001b[49m\u001b[43mh_current\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey_module\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparticipant_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mparticipant_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexperiment_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrols\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_polynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m            \u001b[49m\u001b[43mensemble_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    327\u001b[39m         next_value = torch.zeros_like(value)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/spice/resources/rnn.py:449\u001b[39m, in \u001b[36mBaseRNN.forward_sindy\u001b[39m\u001b[34m(self, h_current, key_module, participant_ids, experiment_ids, controls, polynomial_degree, ensemble_idx)\u001b[39m\n\u001b[32m    446\u001b[39m     mask = \u001b[38;5;28mself\u001b[39m.sindy_coefficients_presence[key_module][participant_ids, experiment_ids]  \u001b[38;5;66;03m# [batch, n_ensemble, n_library_terms]\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# Get coefficients and masks for all ensemble members\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     sindy_coeffs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msindy_coefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey_module\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparticipant_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, n_ensemble, n_library_terms]\u001b[39;00m\n\u001b[32m    450\u001b[39m     mask = \u001b[38;5;28mself\u001b[39m.sindy_coefficients_presence[key_module][participant_ids, experiment_ids, ensemble_idx].unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [batch, n_ensemble, n_library_terms]\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# custom dropout layer for sindy coefficients\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# if hasattr(self, 'dropout') and self.training:\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m#     dropout_mask = (torch.rand_like(sindy_coeffs) > self.dropout).float()\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;66;03m#     sindy_coeffs = sindy_coeffs * dropout_mask / (1 - self.dropout)\u001b[39;00m\n\u001b[32m    456\u001b[39m \n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Apply the mask to enforce sparsity during loss computation\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting training on {estimator.device}...\")\n",
        "print(\"=\" * 80)\n",
        "estimator.fit(dataset_train.xs, dataset_train.ys, dataset_test.xs, dataset_test.ys)\n",
        "# estimator.load_spice(args.model)\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Print example SPICE model for first participant\n",
        "print(\"\\nExample SPICE model (participant 0):\")\n",
        "print(\"-\" * 80)\n",
        "estimator.print_spice_model(participant_id=0)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimator.load_spice(path_spice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRU for benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('../..')\n",
        "from weinhardt2025.benchmarking.benchmarking_gru import GRU, training, setup_agent_gru\n",
        "\n",
        "path_gru = '../../weinhardt2025/params/weber2024/gru_weber2024.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000: L(Train): 0.5476048588752747; L(Test): 0.5421544313430786\n",
            "Epoch 2/1000: L(Train): 0.5668220520019531; L(Test): 0.5406689643859863\n",
            "Epoch 3/1000: L(Train): 0.5455498099327087; L(Test): 0.5279489159584045\n",
            "Epoch 4/1000: L(Train): 0.5340773463249207; L(Test): 0.5325810313224792\n",
            "Epoch 5/1000: L(Train): 0.5427414774894714; L(Test): 0.5351895689964294\n",
            "Epoch 6/1000: L(Train): 0.5421103835105896; L(Test): 0.5297689437866211\n",
            "Epoch 7/1000: L(Train): 0.5311235785484314; L(Test): 0.5268921852111816\n",
            "Epoch 8/1000: L(Train): 0.5373024940490723; L(Test): 0.5304366946220398\n",
            "Epoch 9/1000: L(Train): 0.5316314101219177; L(Test): 0.5325705409049988\n",
            "Epoch 10/1000: L(Train): 0.5388942360877991; L(Test): 0.5308281183242798\n",
            "Epoch 11/1000: L(Train): 0.5284050107002258; L(Test): 0.5276982188224792\n",
            "Epoch 12/1000: L(Train): 0.5302596092224121; L(Test): 0.5275452136993408\n",
            "Epoch 13/1000: L(Train): 0.5358317494392395; L(Test): 0.5296292901039124\n",
            "Epoch 14/1000: L(Train): 0.535137414932251; L(Test): 0.5293743014335632\n",
            "Epoch 15/1000: L(Train): 0.5336642265319824; L(Test): 0.5273953676223755\n",
            "Epoch 16/1000: L(Train): 0.5322416424751282; L(Test): 0.5268930196762085\n",
            "Epoch 17/1000: L(Train): 0.536391019821167; L(Test): 0.5289327502250671\n",
            "Epoch 18/1000: L(Train): 0.5336102247238159; L(Test): 0.5291916131973267\n",
            "Epoch 19/1000: L(Train): 0.5353087782859802; L(Test): 0.5276258587837219\n",
            "Epoch 20/1000: L(Train): 0.5341955423355103; L(Test): 0.5267666578292847\n",
            "Epoch 21/1000: L(Train): 0.5363306999206543; L(Test): 0.5274093151092529\n",
            "Epoch 22/1000: L(Train): 0.5320816040039062; L(Test): 0.5276841521263123\n",
            "Epoch 23/1000: L(Train): 0.5287736654281616; L(Test): 0.5273297429084778\n",
            "Epoch 24/1000: L(Train): 0.5375638008117676; L(Test): 0.5266496539115906\n",
            "Epoch 25/1000: L(Train): 0.5297395586967468; L(Test): 0.527762770652771\n",
            "Epoch 26/1000: L(Train): 0.5344976186752319; L(Test): 0.5277081727981567\n",
            "Epoch 27/1000: L(Train): 0.5288578867912292; L(Test): 0.5269073247909546\n",
            "Epoch 28/1000: L(Train): 0.531864583492279; L(Test): 0.5267943143844604\n",
            "Epoch 29/1000: L(Train): 0.5350738763809204; L(Test): 0.5272434949874878\n",
            "Epoch 30/1000: L(Train): 0.5337685942649841; L(Test): 0.5266815423965454\n",
            "Epoch 31/1000: L(Train): 0.5273920297622681; L(Test): 0.5269986391067505\n",
            "Epoch 32/1000: L(Train): 0.5282297730445862; L(Test): 0.5267183780670166\n",
            "Epoch 33/1000: L(Train): 0.5263488292694092; L(Test): 0.526759147644043\n",
            "Epoch 34/1000: L(Train): 0.5304113030433655; L(Test): 0.52692711353302\n",
            "Epoch 35/1000: L(Train): 0.5220962762832642; L(Test): 0.5265175700187683\n",
            "Epoch 36/1000: L(Train): 0.5352619886398315; L(Test): 0.5271502137184143\n",
            "Epoch 37/1000: L(Train): 0.5260279774665833; L(Test): 0.5272098779678345\n",
            "Epoch 38/1000: L(Train): 0.5277865529060364; L(Test): 0.5269032120704651\n",
            "Epoch 39/1000: L(Train): 0.5288283228874207; L(Test): 0.5269073843955994\n",
            "Epoch 40/1000: L(Train): 0.5294871926307678; L(Test): 0.5266314744949341\n",
            "Epoch 41/1000: L(Train): 0.5256011486053467; L(Test): 0.5262107849121094\n",
            "Epoch 42/1000: L(Train): 0.5284541249275208; L(Test): 0.5267521142959595\n",
            "Epoch 43/1000: L(Train): 0.5317152738571167; L(Test): 0.5269514322280884\n",
            "Epoch 44/1000: L(Train): 0.5288588404655457; L(Test): 0.5274487137794495\n",
            "Epoch 45/1000: L(Train): 0.5269270539283752; L(Test): 0.5270413160324097\n",
            "Epoch 46/1000: L(Train): 0.5286124348640442; L(Test): 0.5262688398361206\n",
            "Epoch 47/1000: L(Train): 0.5269189476966858; L(Test): 0.5260263681411743\n",
            "Epoch 48/1000: L(Train): 0.525189220905304; L(Test): 0.5264719724655151\n",
            "Epoch 49/1000: L(Train): 0.5308508276939392; L(Test): 0.5271176099777222\n",
            "Epoch 50/1000: L(Train): 0.5323719382286072; L(Test): 0.527447521686554\n",
            "Epoch 51/1000: L(Train): 0.5216678977012634; L(Test): 0.5262844562530518\n",
            "Epoch 52/1000: L(Train): 0.5323596596717834; L(Test): 0.5260937809944153\n",
            "Epoch 53/1000: L(Train): 0.5300071835517883; L(Test): 0.5260706543922424\n",
            "Epoch 54/1000: L(Train): 0.5277734398841858; L(Test): 0.5260401964187622\n",
            "Epoch 55/1000: L(Train): 0.5259241461753845; L(Test): 0.5266216993331909\n",
            "Epoch 56/1000: L(Train): 0.5323792099952698; L(Test): 0.5273506045341492\n",
            "Epoch 57/1000: L(Train): 0.5261224508285522; L(Test): 0.5273522138595581\n",
            "Epoch 58/1000: L(Train): 0.5279524922370911; L(Test): 0.5274789929389954\n",
            "Epoch 59/1000: L(Train): 0.5303770899772644; L(Test): 0.5272800326347351\n",
            "Epoch 60/1000: L(Train): 0.5267844796180725; L(Test): 0.5262987017631531\n",
            "Epoch 61/1000: L(Train): 0.5345789194107056; L(Test): 0.5263639688491821\n",
            "Epoch 62/1000: L(Train): 0.5220528244972229; L(Test): 0.5260183811187744\n",
            "Epoch 63/1000: L(Train): 0.5272873640060425; L(Test): 0.5258288979530334\n",
            "Epoch 64/1000: L(Train): 0.5318432450294495; L(Test): 0.5265460014343262\n",
            "Epoch 65/1000: L(Train): 0.5273057222366333; L(Test): 0.5258610248565674\n",
            "Epoch 66/1000: L(Train): 0.5278034210205078; L(Test): 0.5259353518486023\n",
            "Epoch 67/1000: L(Train): 0.5178815722465515; L(Test): 0.5258142948150635\n",
            "Epoch 68/1000: L(Train): 0.5272343158721924; L(Test): 0.5260171294212341\n",
            "Epoch 69/1000: L(Train): 0.5246759057044983; L(Test): 0.5282754302024841\n",
            "Epoch 70/1000: L(Train): 0.5287987589836121; L(Test): 0.5276419520378113\n",
            "Epoch 71/1000: L(Train): 0.5247374773025513; L(Test): 0.527057409286499\n",
            "Epoch 72/1000: L(Train): 0.5275587439537048; L(Test): 0.5259863138198853\n",
            "Epoch 73/1000: L(Train): 0.5276262164115906; L(Test): 0.5258122086524963\n",
            "Epoch 74/1000: L(Train): 0.5257828831672668; L(Test): 0.5265226364135742\n",
            "Epoch 75/1000: L(Train): 0.5282410383224487; L(Test): 0.5269513726234436\n",
            "Epoch 76/1000: L(Train): 0.5277382731437683; L(Test): 0.5278735160827637\n",
            "Epoch 77/1000: L(Train): 0.524956226348877; L(Test): 0.5264892578125\n",
            "Epoch 78/1000: L(Train): 0.5366889238357544; L(Test): 0.5263757705688477\n",
            "Epoch 79/1000: L(Train): 0.5261890888214111; L(Test): 0.5268210768699646\n",
            "Epoch 80/1000: L(Train): 0.5277787446975708; L(Test): 0.5265789031982422\n",
            "Epoch 81/1000: L(Train): 0.5204886794090271; L(Test): 0.5271105170249939\n",
            "Epoch 82/1000: L(Train): 0.5266578793525696; L(Test): 0.5264595746994019\n",
            "Epoch 83/1000: L(Train): 0.5272132754325867; L(Test): 0.5258065462112427\n",
            "Epoch 84/1000: L(Train): 0.5279977917671204; L(Test): 0.5265641212463379\n",
            "Epoch 85/1000: L(Train): 0.5225406289100647; L(Test): 0.5275306105613708\n",
            "Epoch 86/1000: L(Train): 0.5195888876914978; L(Test): 0.5293592214584351\n",
            "Epoch 87/1000: L(Train): 0.5301105976104736; L(Test): 0.5270618796348572\n",
            "Epoch 88/1000: L(Train): 0.5276613831520081; L(Test): 0.5261107087135315\n",
            "Epoch 89/1000: L(Train): 0.529015839099884; L(Test): 0.5257846117019653\n",
            "Epoch 90/1000: L(Train): 0.5286217331886292; L(Test): 0.5258104801177979\n",
            "Epoch 91/1000: L(Train): 0.5303218960762024; L(Test): 0.5272138118743896\n",
            "Epoch 92/1000: L(Train): 0.5242775082588196; L(Test): 0.5299791693687439\n",
            "Epoch 93/1000: L(Train): 0.529115617275238; L(Test): 0.5262550115585327\n",
            "Epoch 94/1000: L(Train): 0.522230863571167; L(Test): 0.525675892829895\n",
            "Epoch 95/1000: L(Train): 0.5258474349975586; L(Test): 0.525340735912323\n",
            "Epoch 96/1000: L(Train): 0.5223955512046814; L(Test): 0.5257635712623596\n",
            "Epoch 97/1000: L(Train): 0.5224010348320007; L(Test): 0.5271292328834534\n",
            "Epoch 98/1000: L(Train): 0.5251346826553345; L(Test): 0.5273922085762024\n",
            "Epoch 99/1000: L(Train): 0.5213767290115356; L(Test): 0.5261964797973633\n",
            "Epoch 100/1000: L(Train): 0.5250798463821411; L(Test): 0.5262408256530762\n",
            "Epoch 101/1000: L(Train): 0.528066873550415; L(Test): 0.5276561379432678\n",
            "Epoch 102/1000: L(Train): 0.527522623538971; L(Test): 0.5287132263183594\n",
            "Epoch 103/1000: L(Train): 0.529467761516571; L(Test): 0.5303670763969421\n",
            "Epoch 104/1000: L(Train): 0.52579665184021; L(Test): 0.5273838043212891\n",
            "Epoch 105/1000: L(Train): 0.5275027751922607; L(Test): 0.5262688994407654\n",
            "Epoch 106/1000: L(Train): 0.5223207473754883; L(Test): 0.5262669920921326\n",
            "Epoch 107/1000: L(Train): 0.5216975808143616; L(Test): 0.5261534452438354\n",
            "Epoch 108/1000: L(Train): 0.5277212858200073; L(Test): 0.52691251039505\n",
            "Epoch 109/1000: L(Train): 0.5335564613342285; L(Test): 0.5275859236717224\n",
            "Epoch 110/1000: L(Train): 0.5295953154563904; L(Test): 0.529079258441925\n",
            "Epoch 111/1000: L(Train): 0.5246791839599609; L(Test): 0.5316768288612366\n",
            "Epoch 112/1000: L(Train): 0.5228949785232544; L(Test): 0.5305361151695251\n",
            "Epoch 113/1000: L(Train): 0.5270903706550598; L(Test): 0.5302098989486694\n",
            "Epoch 114/1000: L(Train): 0.5219307541847229; L(Test): 0.5310850143432617\n",
            "Epoch 115/1000: L(Train): 0.5284313559532166; L(Test): 0.5281410217285156\n",
            "Epoch 116/1000: L(Train): 0.5296942591667175; L(Test): 0.5281562209129333\n",
            "Epoch 117/1000: L(Train): 0.5235130190849304; L(Test): 0.5287678241729736\n",
            "Epoch 118/1000: L(Train): 0.5223339200019836; L(Test): 0.5287893414497375\n",
            "Epoch 119/1000: L(Train): 0.5194278359413147; L(Test): 0.5283808708190918\n",
            "Epoch 120/1000: L(Train): 0.521241307258606; L(Test): 0.5289245843887329\n",
            "Epoch 121/1000: L(Train): 0.5260870456695557; L(Test): 0.533556342124939\n",
            "Epoch 122/1000: L(Train): 0.5275008082389832; L(Test): 0.5360504388809204\n",
            "Epoch 123/1000: L(Train): 0.5263015031814575; L(Test): 0.5359334349632263\n",
            "Epoch 124/1000: L(Train): 0.5218996405601501; L(Test): 0.5317839980125427\n",
            "Epoch 125/1000: L(Train): 0.5167683362960815; L(Test): 0.5293404459953308\n",
            "Epoch 126/1000: L(Train): 0.5231532454490662; L(Test): 0.5307947397232056\n",
            "Epoch 127/1000: L(Train): 0.5212447047233582; L(Test): 0.5336187481880188\n",
            "Epoch 128/1000: L(Train): 0.5239012241363525; L(Test): 0.5336519479751587\n",
            "Epoch 129/1000: L(Train): 0.5221799612045288; L(Test): 0.5310727953910828\n",
            "Epoch 130/1000: L(Train): 0.523192822933197; L(Test): 0.5294992923736572\n",
            "Epoch 131/1000: L(Train): 0.5284467935562134; L(Test): 0.5311851501464844\n",
            "Epoch 132/1000: L(Train): 0.5263153314590454; L(Test): 0.5335088968276978\n",
            "Epoch 133/1000: L(Train): 0.5293470621109009; L(Test): 0.5368374586105347\n",
            "Epoch 134/1000: L(Train): 0.5219862461090088; L(Test): 0.5383439064025879\n",
            "Epoch 135/1000: L(Train): 0.5215520262718201; L(Test): 0.5388562679290771\n",
            "Epoch 136/1000: L(Train): 0.5236837267875671; L(Test): 0.5397120714187622\n",
            "Epoch 137/1000: L(Train): 0.5256344676017761; L(Test): 0.536853015422821\n",
            "Epoch 138/1000: L(Train): 0.5256679654121399; L(Test): 0.5393552780151367\n",
            "Epoch 139/1000: L(Train): 0.5255271792411804; L(Test): 0.53801429271698\n",
            "Epoch 140/1000: L(Train): 0.5278360247612; L(Test): 0.5341697931289673\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m      5\u001b[39m optimizer = torch.optim.Adam(gru.parameters(), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m gru = \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgru\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m torch.save(gru.state_dict(), path_gru)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrained GRU parameters saved to \u001b[39m\u001b[33m\"\u001b[39m + path_gru)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/weinhardt2025/aux/../../weinhardt2025/benchmarking/benchmarking_gru.py:92\u001b[39m, in \u001b[36mtraining\u001b[39m\u001b[34m(gru, optimizer, dataset_train, dataset_test, epochs, batch_size, criterion, device)\u001b[39m\n\u001b[32m     90\u001b[39m nan_mask = ~xs[..., :n_actions].sum(dim=-\u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m).isnan().to(device)\n\u001b[32m     91\u001b[39m logits_flat = logits.reshape(-\u001b[32m1\u001b[39m, n_actions)[nan_mask]\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m labels_flat = torch.argmax(\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m[nan_mask], dim=-\u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m).long()\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     95\u001b[39m loss = criterion(logits_flat, labels_flat)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "\n",
        "gru = GRU(n_actions=n_actions, additional_inputs=3).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)\n",
        "\n",
        "gru = training(\n",
        "    gru=gru,\n",
        "    optimizer=optimizer,\n",
        "    dataset_train=dataset_train,\n",
        "    dataset_test=dataset_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=1024,\n",
        "    )\n",
        "\n",
        "torch.save(gru.state_dict(), path_gru)\n",
        "print(\"Trained GRU parameters saved to \" + path_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gru_agent = setup_agent_gru(path_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot SPICE against benchmark models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting\n",
        "participant_id = 7\n",
        "\n",
        "estimator.print_spice_model(participant_id)\n",
        "\n",
        "agents = {\n",
        "    # add baseline agent here\n",
        "    'rnn': estimator.rnn_agent,\n",
        "    'spice': estimator.spice_agent,\n",
        "    'gru': gru_agent,\n",
        "}\n",
        "\n",
        "fig, axs = plot_session(agents, dataset.xs[participant_id])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
